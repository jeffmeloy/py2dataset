[
    {
        "instruction": "Dependencies in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\n\"\"\"\nUtility functions for reading the input and saving the output of the py2dataset script.\nRequirements:\n[req01] The `read_file` function shall:\n        a. Accept a file path as an argument.\n        b. Read and return the contents of a JSON or YAML file as a dictionary.\n[req02] The `write_file` function shall:\n        a. Accept a dictionary and a file path as arguments.\n        b. Write the dictionary to a file in either JSON or YAML format.\n[req03] The `convert_json_to_html` function shall:\n        a. Convert JSON files within a given directory to HTML format.\n        b. Save each converted file with a .html extension.\n        c. Preserve spacing and tabs for the 'input' field.\n[req04] The `combine_json_files` function shall:\n        a. Accept a directory path as an argument.\n        b. Merge all JSON files in the directory.\n        c. Remove duplicates from the combined JSON files.\n        d. Write the combined data to 'instruct.json' files.\n        e. Convert the merged JSON files to HTML format.\n        f. Return the 'instruct_list' datasets.\n[req05] The `create_code_graph` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Generate code graphs based on the provided file details.\n        c. Save the graphs as PNG images in the specified output directory.\n[req06] The `save_python_data` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Save the details of the Python file as a YAML file.\n        c. Save the instruction data as JSON files.\n        d. Generate and save code graphs.\n\"\"\"\nimport json\nimport logging\nfrom html import escape\nfrom pathlib import Path\nfrom typing import Dict, List\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport yaml\n\n\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == \"json\":\n            return json.load(f)\n        if file_type == \"yaml\":\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}\n\n\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file.\n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open(\"w\") as f:\n        if file_type == \"json\":\n            json.dump(data, f, indent=4)\n        elif file_type == \"yaml\":\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n\n\ndef convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format and save .html file.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int = 4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(\" \", \"&nbsp;\").replace(\"\\t\", \"&nbsp;\" * tab_width)\n\n    for json_file in Path(directory).rglob(\"*.json\"):\n        try:\n            dataset = read_file(json_file)\n            if not dataset:\n                continue\n        except Exception:\n            continue\n\n        html_content = \"\"\"\n        <html>\n        <head>\n            <style>\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\n                th, td {\n                    border: 1px solid black;\n                    padding: 8px;\n                    text-align: left;\n                    white-space: pre-line;\n                    vertical-align: top;\n                    word-wrap: break-word;\n                }\n            </style>\n        </head>\n        <body>\n            <table>\n                <thead>\n                    <tr>\n        \"\"\"\n        column_count = len(dataset[0].keys())\n        column_width = round(100 / column_count, 2)\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += \"\"\"\n                    </tr>\n                </thead>\n                <tbody>\n        \"\"\"\n        html_rows = []\n        for entry in dataset:\n            row_parts = [\"<tr>\"]\n            for key in entry:\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace(\"\\n\", \"<br/>\")\n                row_parts.append(f\"<td>{value}</td>\")\n            row_parts.append(\"</tr>\")\n            html_rows.append(\"\".join(row_parts))\n        html_content += \"\".join(html_rows)\n\n        html_content += \"\"\"\n                </tbody>\n            </table>\n        </body>\n        </html>\n        \"\"\"\n        html_file_path = json_file.with_suffix(\".html\")\n        try:\n            with open(html_file_path, \"w\", encoding=\"utf-8\") as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f\"Failed saving: {html_file_path}\")\n\n\ndef combine_json_files(\n    directory: str, html: bool, questions: Dict\n) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and\n    then remove duplicates. Also generate a 'training.json' file.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n    logging.info(f\"Combining JSON files in {directory}\")\n\n    # Save instruct.json file\n    combined_data, code_filename = [], []\n    skip_files = {\"instruct.json\", \"shargpt.json\", \"document_code.json\"}\n    for json_file in Path(directory).rglob(\"*.json\"):\n        if json_file.name in skip_files:\n            continue\n        try:\n            file_data = read_file(json_file)\n            if file_data:\n                combined_data.extend(file_data)\n                cleaned_name = (\n                    json_file.relative_to(directory)\n                    .with_suffix(\"\")\n                    .as_posix()\n                    .replace(\".instruct\", \"\")\n                )\n                code_filename.append(cleaned_name)\n        except Exception as e:\n            logging.info(f\"Failed reading: {json_file}. Error: {e}\")\n    write_file(combined_data, Path(directory) / \"instruct.json\")\n\n    # Generate document_code.json file\n    purpose_question = [\n        item[\"text\"] for item in questions if item[\"id\"] == \"file_purpose\"\n    ][0]\n    purpose_question = purpose_question.split(\"{filename}\")[0]\n    purpose_data = [\n        item\n        for item in combined_data\n        if item[\"instruction\"].startswith(purpose_question)\n    ]\n    document_code = [\n        {\n            \"document\": item[\"output\"],\n            \"code\": item[\"input\"],\n        }\n        for item in purpose_data\n    ]\n    for i, item in enumerate(document_code):\n        item[\"code filename\"] = code_filename[i]\n    write_file(document_code, Path(directory) / \"document_code.json\")\n\n    # Generate sharegpt.json file\n    sharegpt = [\n        {\n            \"conversation\": [\n                {\"from\": \"system\", \"value\": f\"code documentation: {item['document']}\"},\n                {\n                    \"from\": \"human\",\n                    \"value\": \"Output the Python code described by the code documentation.\",\n                },\n                {\"from\": \"gpt\", \"value\": item[\"code\"]},\n            ],\n            \"nbytes\": \"0\",\n            \"source\": item[\"code filename\"],\n        }\n        for item in document_code\n    ]\n\n    # Compute the number of bytes for each conversation and update the nbytes field\n    for item in sharegpt:\n        nbytes = 0\n        for conv in item[\"conversation\"]:\n            nbytes += len(conv[\"value\"].encode(\"utf-8\"))\n        item[\"nbytes\"] = nbytes\n\n    write_file(sharegpt, Path(directory) / \"shargpt.json\")\n\n    if html:\n        logging.info(\"Converting JSON files to HTML\")\n        convert_json_to_html(directory)\n\n    return {\"instruct_list\": combined_data}\n\n\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    # create graph\n    graph_type = \"entire_code_graph\"\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details[\"file_info\"][graph_type][\"nodes\"])\n    for edge in file_details[\"file_info\"][graph_type][\"edges\"]:\n        source, target = edge[\"source\"], edge[\"target\"]\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(\n                source,\n                target,\n                **{\n                    k: v\n                    for k, v in edge.items()\n                    if k in [\"target_inputs\", \"target_returns\"]\n                },\n            )\n\n    # draw graph\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(\n        G,\n        pos,\n        with_labels=True,\n        font_weight=\"bold\",\n        font_size=8,\n        node_shape=\"s\",\n        node_size=500,\n        width=1,\n        arrowsize=12,\n    )\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if \"target_inputs\" in edge[2] and edge[2][\"target_inputs\"]:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if \"target_returns\" in edge[2] and edge[2][\"target_returns\"]:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[(edge[0], edge[1])] = \"\\n\".join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n\n    # save graph\n    output_file = output_subdir / f\"{base_name}.{graph_type}.png\"\n    plt.savefig(output_file)\n    plt.close()\n\n\ndef save_python_data(\n    file_details: dict, instruct_list: list, relative_path: Path, output_dir: str\n) -> None:\n    \"\"\"\n    Save the details of the Python file as a YAML file, the instruction data as JSON files,\n    and generate and save code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path of the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parent\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    base_name = relative_path.name\n    write_file(instruct_list, output_subdir / f\"{base_name}.instruct.json\")\n    write_file(file_details, output_subdir / f\"{base_name}.details.yaml\")\n\n    # Generating and saving the code graph\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except Exception as e:\n        logging.error(f\"Error creating graph for {base_name}: {e}\", exc_info=True)\n\n```",
        "output": "'matplotlib.pyplot', 'yaml', 'networkx', 'html', 'logging', 'typing', 'json', 'pathlib'\n\nPurpose and Significance:\n1) matplotlib.pyplot: This dependency from the Matplotlib library is utilized within the `create_code_graph` function to draw visual representations of Python file code structures as graphs. It creates diagrams depicting data flow relationships among different parts of a script using directed edges and nodes. The drawn graph gets saved as a PNG image for further usage in documentation or analysis purposes.\n\n2) yaml: YAML (Yet Another Markup Language) is employed in `read_file`, `write_file` functions along with handling JSON files. It serves as an alternative data serialization format to JSON, allowing more human-readable formatting with indentation and comments. In the code snippet above, 'yaml' helps to convert data to or read YAML structures by incorporating loader/dumper mechanisms efficiently through the 'Loader=yaml.SafeLoader'.\n\n3) networkx: It represents Python graph computing package needed primarily within the `create_code_graph` function for constructing code graphs. NetworkX library is used to create Directed Graphs (DiGraph object 'G') representing Python file dependencies. The library handles nodes and edges as well as edge attributes essential for visualizing code relations efficiently before converting into graph images as required by users of this utility module.\n\n4) html: It reflects HyperText Markup Language as the format employed while processing JSON data conversion (in `convert_json_to_html`) where every JSON file is transformed into an HTML table representation preserving whitespaces in input fields. This enables easier human readability and interpretation of JSON content for users.\n\n5) logging: The 'logging' module serves as a utility tool for capturing runtime messages (errors/warnings) generated during execution. In the provided code snippet, it records failures related to file reading or conversion operations like `Failed reading` or `Error creating graph`. It helps developers in debugging issues efficiently while running the script on larger datasets.\n\n6) typing: 'typing' module provides Python's static type hinting support throughout the functions with various data structures (e.g., Path, Dict, List). Specifically in this code, `Dict` represents key-value pair objects or function parameters with precise argument definitions to avoid potential misinterpretations. This feature boosts code clarity and improves compatibility among Python developers while interacting with `py2dataset\\save_output`.\n\n7) json: Json (JavaScript Object Notation) serves as the default format for reading and writing files within the utility module 'py2dataset\\save_output'. `json` library loads or dumps JSON content using readable formatting (e.g., indent=4 in `write_file`) to ensure better human understanding of data structures during debugging/analysis tasks.\n\n8) pathlib: This Python standard library offers a platform-independent way of working with file system paths. In the given code, 'pathlib' is used for handling relative and absolute paths (e.g., `Path(directory).rglob(\"*.json\")`), directory creation ('output_subdir.mkdir'), or manipulating filenames ('relative_to', 'as_posix'). It simplifies file operations across different operating systems making the script portable and robust."
    },
    {
        "instruction": "Call code graph in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\n\"\"\"\nUtility functions for reading the input and saving the output of the py2dataset script.\nRequirements:\n[req01] The `read_file` function shall:\n        a. Accept a file path as an argument.\n        b. Read and return the contents of a JSON or YAML file as a dictionary.\n[req02] The `write_file` function shall:\n        a. Accept a dictionary and a file path as arguments.\n        b. Write the dictionary to a file in either JSON or YAML format.\n[req03] The `convert_json_to_html` function shall:\n        a. Convert JSON files within a given directory to HTML format.\n        b. Save each converted file with a .html extension.\n        c. Preserve spacing and tabs for the 'input' field.\n[req04] The `combine_json_files` function shall:\n        a. Accept a directory path as an argument.\n        b. Merge all JSON files in the directory.\n        c. Remove duplicates from the combined JSON files.\n        d. Write the combined data to 'instruct.json' files.\n        e. Convert the merged JSON files to HTML format.\n        f. Return the 'instruct_list' datasets.\n[req05] The `create_code_graph` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Generate code graphs based on the provided file details.\n        c. Save the graphs as PNG images in the specified output directory.\n[req06] The `save_python_data` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Save the details of the Python file as a YAML file.\n        c. Save the instruction data as JSON files.\n        d. Generate and save code graphs.\n\"\"\"\nimport json\nimport logging\nfrom html import escape\nfrom pathlib import Path\nfrom typing import Dict, List\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport yaml\n\n\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == \"json\":\n            return json.load(f)\n        if file_type == \"yaml\":\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}\n\n\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file.\n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open(\"w\") as f:\n        if file_type == \"json\":\n            json.dump(data, f, indent=4)\n        elif file_type == \"yaml\":\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n\n\ndef convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format and save .html file.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int = 4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(\" \", \"&nbsp;\").replace(\"\\t\", \"&nbsp;\" * tab_width)\n\n    for json_file in Path(directory).rglob(\"*.json\"):\n        try:\n            dataset = read_file(json_file)\n            if not dataset:\n                continue\n        except Exception:\n            continue\n\n        html_content = \"\"\"\n        <html>\n        <head>\n            <style>\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\n                th, td {\n                    border: 1px solid black;\n                    padding: 8px;\n                    text-align: left;\n                    white-space: pre-line;\n                    vertical-align: top;\n                    word-wrap: break-word;\n                }\n            </style>\n        </head>\n        <body>\n            <table>\n                <thead>\n                    <tr>\n        \"\"\"\n        column_count = len(dataset[0].keys())\n        column_width = round(100 / column_count, 2)\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += \"\"\"\n                    </tr>\n                </thead>\n                <tbody>\n        \"\"\"\n        html_rows = []\n        for entry in dataset:\n            row_parts = [\"<tr>\"]\n            for key in entry:\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace(\"\\n\", \"<br/>\")\n                row_parts.append(f\"<td>{value}</td>\")\n            row_parts.append(\"</tr>\")\n            html_rows.append(\"\".join(row_parts))\n        html_content += \"\".join(html_rows)\n\n        html_content += \"\"\"\n                </tbody>\n            </table>\n        </body>\n        </html>\n        \"\"\"\n        html_file_path = json_file.with_suffix(\".html\")\n        try:\n            with open(html_file_path, \"w\", encoding=\"utf-8\") as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f\"Failed saving: {html_file_path}\")\n\n\ndef combine_json_files(\n    directory: str, html: bool, questions: Dict\n) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and\n    then remove duplicates. Also generate a 'training.json' file.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n    logging.info(f\"Combining JSON files in {directory}\")\n\n    # Save instruct.json file\n    combined_data, code_filename = [], []\n    skip_files = {\"instruct.json\", \"shargpt.json\", \"document_code.json\"}\n    for json_file in Path(directory).rglob(\"*.json\"):\n        if json_file.name in skip_files:\n            continue\n        try:\n            file_data = read_file(json_file)\n            if file_data:\n                combined_data.extend(file_data)\n                cleaned_name = (\n                    json_file.relative_to(directory)\n                    .with_suffix(\"\")\n                    .as_posix()\n                    .replace(\".instruct\", \"\")\n                )\n                code_filename.append(cleaned_name)\n        except Exception as e:\n            logging.info(f\"Failed reading: {json_file}. Error: {e}\")\n    write_file(combined_data, Path(directory) / \"instruct.json\")\n\n    # Generate document_code.json file\n    purpose_question = [\n        item[\"text\"] for item in questions if item[\"id\"] == \"file_purpose\"\n    ][0]\n    purpose_question = purpose_question.split(\"{filename}\")[0]\n    purpose_data = [\n        item\n        for item in combined_data\n        if item[\"instruction\"].startswith(purpose_question)\n    ]\n    document_code = [\n        {\n            \"document\": item[\"output\"],\n            \"code\": item[\"input\"],\n        }\n        for item in purpose_data\n    ]\n    for i, item in enumerate(document_code):\n        item[\"code filename\"] = code_filename[i]\n    write_file(document_code, Path(directory) / \"document_code.json\")\n\n    # Generate sharegpt.json file\n    sharegpt = [\n        {\n            \"conversation\": [\n                {\"from\": \"system\", \"value\": f\"code documentation: {item['document']}\"},\n                {\n                    \"from\": \"human\",\n                    \"value\": \"Output the Python code described by the code documentation.\",\n                },\n                {\"from\": \"gpt\", \"value\": item[\"code\"]},\n            ],\n            \"nbytes\": \"0\",\n            \"source\": item[\"code filename\"],\n        }\n        for item in document_code\n    ]\n\n    # Compute the number of bytes for each conversation and update the nbytes field\n    for item in sharegpt:\n        nbytes = 0\n        for conv in item[\"conversation\"]:\n            nbytes += len(conv[\"value\"].encode(\"utf-8\"))\n        item[\"nbytes\"] = nbytes\n\n    write_file(sharegpt, Path(directory) / \"shargpt.json\")\n\n    if html:\n        logging.info(\"Converting JSON files to HTML\")\n        convert_json_to_html(directory)\n\n    return {\"instruct_list\": combined_data}\n\n\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    # create graph\n    graph_type = \"entire_code_graph\"\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details[\"file_info\"][graph_type][\"nodes\"])\n    for edge in file_details[\"file_info\"][graph_type][\"edges\"]:\n        source, target = edge[\"source\"], edge[\"target\"]\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(\n                source,\n                target,\n                **{\n                    k: v\n                    for k, v in edge.items()\n                    if k in [\"target_inputs\", \"target_returns\"]\n                },\n            )\n\n    # draw graph\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(\n        G,\n        pos,\n        with_labels=True,\n        font_weight=\"bold\",\n        font_size=8,\n        node_shape=\"s\",\n        node_size=500,\n        width=1,\n        arrowsize=12,\n    )\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if \"target_inputs\" in edge[2] and edge[2][\"target_inputs\"]:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if \"target_returns\" in edge[2] and edge[2][\"target_returns\"]:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[(edge[0], edge[1])] = \"\\n\".join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n\n    # save graph\n    output_file = output_subdir / f\"{base_name}.{graph_type}.png\"\n    plt.savefig(output_file)\n    plt.close()\n\n\ndef save_python_data(\n    file_details: dict, instruct_list: list, relative_path: Path, output_dir: str\n) -> None:\n    \"\"\"\n    Save the details of the Python file as a YAML file, the instruction data as JSON files,\n    and generate and save code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path of the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parent\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    base_name = relative_path.name\n    write_file(instruct_list, output_subdir / f\"{base_name}.instruct.json\")\n    write_file(file_details, output_subdir / f\"{base_name}.details.yaml\")\n\n    # Generating and saving the code graph\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except Exception as e:\n        logging.error(f\"Error creating graph for {base_name}: {e}\", exc_info=True)\n\n```",
        "output": "{'nodes': ['read_file', 'write_file', 'convert_json_to_html', 'preserve_spacing', 'combine_json_files', 'create_code_graph', 'save_python_data', 'file_path.open', 'json.load', 'yaml.load', 'json.dump', 'yaml.dump', \"text.replace(' ', '&nbsp;').replace\", 'text.replace', 'Path(directory).rglob', 'Path', 'len', 'dataset[0].keys', 'round', 'escape', 'str', 'value.replace', 'row_parts.append', 'html_rows.append', \"''.join\", 'json_file.with_suffix', 'open', 'file.write', 'logging.info', 'combined_data.extend', \"json_file.relative_to(directory).with_suffix('').as_posix().replace\", \"json_file.relative_to(directory).with_suffix('').as_posix\", 'json_file.relative_to(directory).with_suffix', 'json_file.relative_to', 'code_filename.append', 'purpose_question.split', \"item['instruction'].startswith\", 'enumerate', \"conv['value'].encode\", 'nx.DiGraph', 'G.add_nodes_from', 'G.add_edge', 'edge.items', 'plt.figure', 'nx.spring_layout', 'nx.draw', 'G.edges', 'label.append', \"', '.join\", \"'\\\\n'.join\", 'nx.draw_networkx_edge_labels', 'plt.savefig', 'plt.close', 'output_subdir.mkdir', 'logging.error'], 'edges': [{'source': 'read_file', 'target': 'file_path.open', 'target_inputs': []}, {'source': 'read_file', 'target': 'json.load', 'target_inputs': ['f']}, {'source': 'read_file', 'target': 'yaml.load', 'target_inputs': ['f']}, {'source': 'write_file', 'target': 'file_path.open', 'target_inputs': [\"'w'\"]}, {'source': 'write_file', 'target': 'json.dump', 'target_inputs': ['data', 'f']}, {'source': 'write_file', 'target': 'yaml.dump', 'target_inputs': ['data', 'f']}, {'source': 'convert_json_to_html', 'target': \"text.replace(' ', '&nbsp;').replace\", 'target_inputs': [\"'\\\\t'\", \"'&nbsp;' * tab_width\"]}, {'source': 'convert_json_to_html', 'target': 'text.replace', 'target_inputs': [\"' '\", \"'&nbsp;'\"]}, {'source': 'convert_json_to_html', 'target': 'Path(directory).rglob', 'target_inputs': [\"'*.json'\"]}, {'source': 'convert_json_to_html', 'target': 'Path', 'target_inputs': ['directory']}, {'source': 'convert_json_to_html', 'target': 'read_file', 'target_inputs': ['json_file'], 'target_returns': ['yaml.load(f, Loader=yaml.SafeLoader)', '{}', 'json.load(f)']}, {'source': 'convert_json_to_html', 'target': 'len', 'target_inputs': ['dataset[0].keys()']}, {'source': 'convert_json_to_html', 'target': 'dataset[0].keys', 'target_inputs': []}, {'source': 'convert_json_to_html', 'target': 'round', 'target_inputs': ['100 / column_count', '2']}, {'source': 'convert_json_to_html', 'target': 'escape', 'target_inputs': ['str(entry[key])']}, {'source': 'convert_json_to_html', 'target': 'str', 'target_inputs': ['entry[key]']}, {'source': 'convert_json_to_html', 'target': 'preserve_spacing', 'target_inputs': ['value'], 'target_returns': [\"text.replace(' ', '&nbsp;').replace('\\\\t', '&nbsp;' * tab_width)\"]}, {'source': 'convert_json_to_html', 'target': 'value.replace', 'target_inputs': [\"'\\\\n'\", \"'<br/>'\"]}, {'source': 'convert_json_to_html', 'target': 'row_parts.append', 'target_inputs': [\"f'<td>{value}</td>'\", \"'</tr>'\"]}, {'source': 'convert_json_to_html', 'target': 'html_rows.append', 'target_inputs': [\"''.join(row_parts)\"]}, {'source': 'convert_json_to_html', 'target': \"''.join\", 'target_inputs': ['row_parts', 'html_rows']}, {'source': 'convert_json_to_html', 'target': 'json_file.with_suffix', 'target_inputs': [\"'.html'\"]}, {'source': 'convert_json_to_html', 'target': 'open', 'target_inputs': ['html_file_path', \"'w'\"]}, {'source': 'convert_json_to_html', 'target': 'file.write', 'target_inputs': ['html_content']}, {'source': 'convert_json_to_html', 'target': 'logging.info', 'target_inputs': [\"f'Failed saving: {html_file_path}'\"]}, {'source': 'preserve_spacing', 'target': \"text.replace(' ', '&nbsp;').replace\", 'target_inputs': [\"'\\\\t'\", \"'&nbsp;' * tab_width\"]}, {'source': 'preserve_spacing', 'target': 'text.replace', 'target_inputs': [\"' '\", \"'&nbsp;'\"]}, {'source': 'combine_json_files', 'target': 'logging.info', 'target_inputs': [\"f'Combining JSON files in {directory}'\", \"f'Failed reading: {json_file}. Error: {e}'\", \"'Converting JSON files to HTML'\"]}, {'source': 'combine_json_files', 'target': 'Path(directory).rglob', 'target_inputs': [\"'*.json'\"]}, {'source': 'combine_json_files', 'target': 'Path', 'target_inputs': ['directory', 'directory', 'directory', 'directory']}, {'source': 'combine_json_files', 'target': 'read_file', 'target_inputs': ['json_file'], 'target_returns': ['yaml.load(f, Loader=yaml.SafeLoader)', '{}', 'json.load(f)']}, {'source': 'combine_json_files', 'target': 'combined_data.extend', 'target_inputs': ['file_data']}, {'source': 'combine_json_files', 'target': \"json_file.relative_to(directory).with_suffix('').as_posix().replace\", 'target_inputs': [\"'.instruct'\", \"''\"]}, {'source': 'combine_json_files', 'target': \"json_file.relative_to(directory).with_suffix('').as_posix\", 'target_inputs': []}, {'source': 'combine_json_files', 'target': 'json_file.relative_to(directory).with_suffix', 'target_inputs': [\"''\"]}, {'source': 'combine_json_files', 'target': 'json_file.relative_to', 'target_inputs': ['directory']}, {'source': 'combine_json_files', 'target': 'code_filename.append', 'target_inputs': ['cleaned_name']}, {'source': 'combine_json_files', 'target': 'write_file', 'target_inputs': ['combined_data', \"Path(directory) / 'instruct.json'\", 'document_code', \"Path(directory) / 'document_code.json'\", 'sharegpt', \"Path(directory) / 'shargpt.json'\"], 'target_returns': []}, {'source': 'combine_json_files', 'target': 'purpose_question.split', 'target_inputs': [\"'{filename}'\"]}, {'source': 'combine_json_files', 'target': \"item['instruction'].startswith\", 'target_inputs': ['purpose_question']}, {'source': 'combine_json_files', 'target': 'enumerate', 'target_inputs': ['document_code']}, {'source': 'combine_json_files', 'target': 'len', 'target_inputs': [\"conv['value'].encode('utf-8')\"]}, {'source': 'combine_json_files', 'target': \"conv['value'].encode\", 'target_inputs': [\"'utf-8'\"]}, {'source': 'combine_json_files', 'target': 'convert_json_to_html', 'target_inputs': ['directory'], 'target_returns': [\"text.replace(' ', '&nbsp;').replace('\\\\t', '&nbsp;' * tab_width)\"]}, {'source': 'create_code_graph', 'target': 'nx.DiGraph', 'target_inputs': []}, {'source': 'create_code_graph', 'target': 'G.add_nodes_from', 'target_inputs': [\"file_details['file_info'][graph_type]['nodes']\"]}, {'source': 'create_code_graph', 'target': 'G.add_edge', 'target_inputs': ['source', 'target']}, {'source': 'create_code_graph', 'target': 'edge.items', 'target_inputs': []}, {'source': 'create_code_graph', 'target': 'plt.figure', 'target_inputs': []}, {'source': 'create_code_graph', 'target': 'nx.spring_layout', 'target_inputs': ['G']}, {'source': 'create_code_graph', 'target': 'nx.draw', 'target_inputs': ['G', 'pos']}, {'source': 'create_code_graph', 'target': 'G.edges', 'target_inputs': []}, {'source': 'create_code_graph', 'target': 'label.append', 'target_inputs': ['f\"Inputs: {\\', \\'.join(edge[2][\\'target_inputs\\'])}\"', 'f\"\\\\nReturns: {\\', \\'.join(edge[2][\\'target_returns\\'])}\"']}, {'source': 'create_code_graph', 'target': \"', '.join\", 'target_inputs': [\"edge[2]['target_inputs']\", \"edge[2]['target_returns']\"]}, {'source': 'create_code_graph', 'target': \"'\\\\n'.join\", 'target_inputs': ['label']}, {'source': 'create_code_graph', 'target': 'nx.draw_networkx_edge_labels', 'target_inputs': ['G', 'pos']}, {'source': 'create_code_graph', 'target': 'plt.savefig', 'target_inputs': ['output_file']}, {'source': 'create_code_graph', 'target': 'plt.close', 'target_inputs': []}, {'source': 'save_python_data', 'target': 'Path', 'target_inputs': ['output_dir']}, {'source': 'save_python_data', 'target': 'output_subdir.mkdir', 'target_inputs': []}, {'source': 'save_python_data', 'target': 'write_file', 'target_inputs': ['instruct_list', \"output_subdir / f'{base_name}.instruct.json'\", 'file_details', \"output_subdir / f'{base_name}.details.yaml'\"], 'target_returns': []}, {'source': 'save_python_data', 'target': 'create_code_graph', 'target_inputs': ['file_details', 'base_name', 'output_subdir'], 'target_returns': []}, {'source': 'save_python_data', 'target': 'logging.error', 'target_inputs': [\"f'Error creating graph for {base_name}: {e}'\"]}]}"
    },
    {
        "instruction": "Functions in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\n\"\"\"\nUtility functions for reading the input and saving the output of the py2dataset script.\nRequirements:\n[req01] The `read_file` function shall:\n        a. Accept a file path as an argument.\n        b. Read and return the contents of a JSON or YAML file as a dictionary.\n[req02] The `write_file` function shall:\n        a. Accept a dictionary and a file path as arguments.\n        b. Write the dictionary to a file in either JSON or YAML format.\n[req03] The `convert_json_to_html` function shall:\n        a. Convert JSON files within a given directory to HTML format.\n        b. Save each converted file with a .html extension.\n        c. Preserve spacing and tabs for the 'input' field.\n[req04] The `combine_json_files` function shall:\n        a. Accept a directory path as an argument.\n        b. Merge all JSON files in the directory.\n        c. Remove duplicates from the combined JSON files.\n        d. Write the combined data to 'instruct.json' files.\n        e. Convert the merged JSON files to HTML format.\n        f. Return the 'instruct_list' datasets.\n[req05] The `create_code_graph` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Generate code graphs based on the provided file details.\n        c. Save the graphs as PNG images in the specified output directory.\n[req06] The `save_python_data` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Save the details of the Python file as a YAML file.\n        c. Save the instruction data as JSON files.\n        d. Generate and save code graphs.\n\"\"\"\nimport json\nimport logging\nfrom html import escape\nfrom pathlib import Path\nfrom typing import Dict, List\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport yaml\n\n\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == \"json\":\n            return json.load(f)\n        if file_type == \"yaml\":\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}\n\n\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file.\n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open(\"w\") as f:\n        if file_type == \"json\":\n            json.dump(data, f, indent=4)\n        elif file_type == \"yaml\":\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n\n\ndef convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format and save .html file.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int = 4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(\" \", \"&nbsp;\").replace(\"\\t\", \"&nbsp;\" * tab_width)\n\n    for json_file in Path(directory).rglob(\"*.json\"):\n        try:\n            dataset = read_file(json_file)\n            if not dataset:\n                continue\n        except Exception:\n            continue\n\n        html_content = \"\"\"\n        <html>\n        <head>\n            <style>\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\n                th, td {\n                    border: 1px solid black;\n                    padding: 8px;\n                    text-align: left;\n                    white-space: pre-line;\n                    vertical-align: top;\n                    word-wrap: break-word;\n                }\n            </style>\n        </head>\n        <body>\n            <table>\n                <thead>\n                    <tr>\n        \"\"\"\n        column_count = len(dataset[0].keys())\n        column_width = round(100 / column_count, 2)\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += \"\"\"\n                    </tr>\n                </thead>\n                <tbody>\n        \"\"\"\n        html_rows = []\n        for entry in dataset:\n            row_parts = [\"<tr>\"]\n            for key in entry:\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace(\"\\n\", \"<br/>\")\n                row_parts.append(f\"<td>{value}</td>\")\n            row_parts.append(\"</tr>\")\n            html_rows.append(\"\".join(row_parts))\n        html_content += \"\".join(html_rows)\n\n        html_content += \"\"\"\n                </tbody>\n            </table>\n        </body>\n        </html>\n        \"\"\"\n        html_file_path = json_file.with_suffix(\".html\")\n        try:\n            with open(html_file_path, \"w\", encoding=\"utf-8\") as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f\"Failed saving: {html_file_path}\")\n\n\ndef combine_json_files(\n    directory: str, html: bool, questions: Dict\n) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and\n    then remove duplicates. Also generate a 'training.json' file.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n    logging.info(f\"Combining JSON files in {directory}\")\n\n    # Save instruct.json file\n    combined_data, code_filename = [], []\n    skip_files = {\"instruct.json\", \"shargpt.json\", \"document_code.json\"}\n    for json_file in Path(directory).rglob(\"*.json\"):\n        if json_file.name in skip_files:\n            continue\n        try:\n            file_data = read_file(json_file)\n            if file_data:\n                combined_data.extend(file_data)\n                cleaned_name = (\n                    json_file.relative_to(directory)\n                    .with_suffix(\"\")\n                    .as_posix()\n                    .replace(\".instruct\", \"\")\n                )\n                code_filename.append(cleaned_name)\n        except Exception as e:\n            logging.info(f\"Failed reading: {json_file}. Error: {e}\")\n    write_file(combined_data, Path(directory) / \"instruct.json\")\n\n    # Generate document_code.json file\n    purpose_question = [\n        item[\"text\"] for item in questions if item[\"id\"] == \"file_purpose\"\n    ][0]\n    purpose_question = purpose_question.split(\"{filename}\")[0]\n    purpose_data = [\n        item\n        for item in combined_data\n        if item[\"instruction\"].startswith(purpose_question)\n    ]\n    document_code = [\n        {\n            \"document\": item[\"output\"],\n            \"code\": item[\"input\"],\n        }\n        for item in purpose_data\n    ]\n    for i, item in enumerate(document_code):\n        item[\"code filename\"] = code_filename[i]\n    write_file(document_code, Path(directory) / \"document_code.json\")\n\n    # Generate sharegpt.json file\n    sharegpt = [\n        {\n            \"conversation\": [\n                {\"from\": \"system\", \"value\": f\"code documentation: {item['document']}\"},\n                {\n                    \"from\": \"human\",\n                    \"value\": \"Output the Python code described by the code documentation.\",\n                },\n                {\"from\": \"gpt\", \"value\": item[\"code\"]},\n            ],\n            \"nbytes\": \"0\",\n            \"source\": item[\"code filename\"],\n        }\n        for item in document_code\n    ]\n\n    # Compute the number of bytes for each conversation and update the nbytes field\n    for item in sharegpt:\n        nbytes = 0\n        for conv in item[\"conversation\"]:\n            nbytes += len(conv[\"value\"].encode(\"utf-8\"))\n        item[\"nbytes\"] = nbytes\n\n    write_file(sharegpt, Path(directory) / \"shargpt.json\")\n\n    if html:\n        logging.info(\"Converting JSON files to HTML\")\n        convert_json_to_html(directory)\n\n    return {\"instruct_list\": combined_data}\n\n\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    # create graph\n    graph_type = \"entire_code_graph\"\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details[\"file_info\"][graph_type][\"nodes\"])\n    for edge in file_details[\"file_info\"][graph_type][\"edges\"]:\n        source, target = edge[\"source\"], edge[\"target\"]\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(\n                source,\n                target,\n                **{\n                    k: v\n                    for k, v in edge.items()\n                    if k in [\"target_inputs\", \"target_returns\"]\n                },\n            )\n\n    # draw graph\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(\n        G,\n        pos,\n        with_labels=True,\n        font_weight=\"bold\",\n        font_size=8,\n        node_shape=\"s\",\n        node_size=500,\n        width=1,\n        arrowsize=12,\n    )\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if \"target_inputs\" in edge[2] and edge[2][\"target_inputs\"]:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if \"target_returns\" in edge[2] and edge[2][\"target_returns\"]:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[(edge[0], edge[1])] = \"\\n\".join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n\n    # save graph\n    output_file = output_subdir / f\"{base_name}.{graph_type}.png\"\n    plt.savefig(output_file)\n    plt.close()\n\n\ndef save_python_data(\n    file_details: dict, instruct_list: list, relative_path: Path, output_dir: str\n) -> None:\n    \"\"\"\n    Save the details of the Python file as a YAML file, the instruction data as JSON files,\n    and generate and save code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path of the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parent\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    base_name = relative_path.name\n    write_file(instruct_list, output_subdir / f\"{base_name}.instruct.json\")\n    write_file(file_details, output_subdir / f\"{base_name}.details.yaml\")\n\n    # Generating and saving the code graph\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except Exception as e:\n        logging.error(f\"Error creating graph for {base_name}: {e}\", exc_info=True)\n\n```",
        "output": "'read_file', 'write_file', 'convert_json_to_html', 'preserve_spacing', 'combine_json_files', 'create_code_graph', 'save_python_data'\n\nPurpose and Significance:\n1. read_file(file_path): This function plays a crucial role in loading JSON or YAML formatted data stored at given paths into memory as Python dictionaries. Its importance lies in handling multiple data source types, enabling flexibility when processing diverse files within the py2dataset script workflow.\n   - Purpose: Loads JSON/YAML files and returns their contents as a dictionary based on file extensions identified by suffixes ('json'/'yaml').\n   - Significance: Enables seamless data extraction from various input formats for further processing within the py2dataset ecosystem.\n\n2. write_file(data, file_path): Writes down dictionaries to disk in either JSON or YAML format, based on their extensions within specified file paths ('file_type'). These functionalities offer options to save processed data back into desired formats for storage purposes.\n   - Purpose: Serializes dictionary data into target files using appropriate libraries (json/'yaml') with custom indentation settings ('indent=4' in json case).\n   - Significance: Ensures compatibility of saved data with downstream processes or analysis tools requiring specific file types.\n\n3. convert_json_to_html(directory): Transforms JSON files within a given directory into HTML format while preserving whitespaces in input fields, facilitating better visualization of the contained information.\n   - Purpose: Enhances human readability by converting technical JSON data into a more understandable HTML table representation for users.\n   - Significance: Improves interpretability and accessibility of datasets to stakeholders beyond developers who deal with raw JSON data.\n\n4. preserve_spacing(text, tab_width=4): Helps maintain whitespaces as well as tabs within provided text while replacing spaces with HTML entities ('&nbsp;') and converting tabs into a given width sequence ('tab_width'). This preserves the original structure when generating HTML files.\n   - Purpose: To keep essential indentations consistent in output data without compromising presentation quality during conversion to HTML format.\n   - Significance: Ensures readability of converted HTML tables by retaining code formatting conventions used in JSON files.\n\n5. combine_json_files(directory, html=False, questions): Merges multiple JSON files into 'instruct.json', excluding some specific ones ('shargpt.json'/'document_code.json'), generates 'training.json' if needed and performs HTML conversion when 'html' is True. It returns a dictionary containing 'instruct_list'.\n   - Purpose: Consolidates JSON datasets into cohesive representations suitable for further analysis or training models by removing duplicates.\n   - Significance: Streamlines data organization and eliminates redundancy, improving performance while processing large amounts of information from multiple sources.\n\n6. create_code_graph(file_details, base_name, output_subdir): Draws Python file code structure as graphs using NetworkX library and saves them as PNG images in an output subdirectory ('output_subdir'). It visualizes relationships between code elements for better understanding of script functionality.\n   - Purpose: Offers graphical representation of code dependencies to aid developers in debugging or analyzing complex Python scripts.\n   - Significance: Facilitates comprehension of program flow and interdependencies among functions/modules within the script.\n\n7. save_python_data(file_details, instruct_list, relative_path, output_dir): Processes Python file information ('file_details'), saves it as YAML/'details.yaml' format along with instruction data ('instruct_list') as JSON files and creates code graphs from these details.\n   - Purpose: Combines parsed script metadata and its corresponding instruction set for archiving and graph generation.\n   - Significance: Archiving Python script data for documentation or reference purposes, supporting deeper analysis by offering both YAML/'details.yaml' and JSON ('instruct_list') outputs in designated directories.'''"
    },
    {
        "instruction": "1) Describe the overall Purpose and Processing Approach of Python file: `py2dataset\\save_output.py`; 2) Define API Signatures for each applicable Function and Class Method; 3) Derive Functional Requirements to specify the code's behavior and purpose; 4) Explain what each variable does in the code.",
        "input": "```python\n\"\"\"\nUtility functions for reading the input and saving the output of the py2dataset script.\nRequirements:\n[req01] The `read_file` function shall:\n        a. Accept a file path as an argument.\n        b. Read and return the contents of a JSON or YAML file as a dictionary.\n[req02] The `write_file` function shall:\n        a. Accept a dictionary and a file path as arguments.\n        b. Write the dictionary to a file in either JSON or YAML format.\n[req03] The `convert_json_to_html` function shall:\n        a. Convert JSON files within a given directory to HTML format.\n        b. Save each converted file with a .html extension.\n        c. Preserve spacing and tabs for the 'input' field.\n[req04] The `combine_json_files` function shall:\n        a. Accept a directory path as an argument.\n        b. Merge all JSON files in the directory.\n        c. Remove duplicates from the combined JSON files.\n        d. Write the combined data to 'instruct.json' files.\n        e. Convert the merged JSON files to HTML format.\n        f. Return the 'instruct_list' datasets.\n[req05] The `create_code_graph` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Generate code graphs based on the provided file details.\n        c. Save the graphs as PNG images in the specified output directory.\n[req06] The `save_python_data` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Save the details of the Python file as a YAML file.\n        c. Save the instruction data as JSON files.\n        d. Generate and save code graphs.\n\"\"\"\nimport json\nimport logging\nfrom html import escape\nfrom pathlib import Path\nfrom typing import Dict, List\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport yaml\n\n\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == \"json\":\n            return json.load(f)\n        if file_type == \"yaml\":\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}\n\n\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file.\n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open(\"w\") as f:\n        if file_type == \"json\":\n            json.dump(data, f, indent=4)\n        elif file_type == \"yaml\":\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n\n\ndef convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format and save .html file.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int = 4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(\" \", \"&nbsp;\").replace(\"\\t\", \"&nbsp;\" * tab_width)\n\n    for json_file in Path(directory).rglob(\"*.json\"):\n        try:\n            dataset = read_file(json_file)\n            if not dataset:\n                continue\n        except Exception:\n            continue\n\n        html_content = \"\"\"\n        <html>\n        <head>\n            <style>\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\n                th, td {\n                    border: 1px solid black;\n                    padding: 8px;\n                    text-align: left;\n                    white-space: pre-line;\n                    vertical-align: top;\n                    word-wrap: break-word;\n                }\n            </style>\n        </head>\n        <body>\n            <table>\n                <thead>\n                    <tr>\n        \"\"\"\n        column_count = len(dataset[0].keys())\n        column_width = round(100 / column_count, 2)\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += \"\"\"\n                    </tr>\n                </thead>\n                <tbody>\n        \"\"\"\n        html_rows = []\n        for entry in dataset:\n            row_parts = [\"<tr>\"]\n            for key in entry:\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace(\"\\n\", \"<br/>\")\n                row_parts.append(f\"<td>{value}</td>\")\n            row_parts.append(\"</tr>\")\n            html_rows.append(\"\".join(row_parts))\n        html_content += \"\".join(html_rows)\n\n        html_content += \"\"\"\n                </tbody>\n            </table>\n        </body>\n        </html>\n        \"\"\"\n        html_file_path = json_file.with_suffix(\".html\")\n        try:\n            with open(html_file_path, \"w\", encoding=\"utf-8\") as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f\"Failed saving: {html_file_path}\")\n\n\ndef combine_json_files(\n    directory: str, html: bool, questions: Dict\n) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and\n    then remove duplicates. Also generate a 'training.json' file.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n    logging.info(f\"Combining JSON files in {directory}\")\n\n    # Save instruct.json file\n    combined_data, code_filename = [], []\n    skip_files = {\"instruct.json\", \"shargpt.json\", \"document_code.json\"}\n    for json_file in Path(directory).rglob(\"*.json\"):\n        if json_file.name in skip_files:\n            continue\n        try:\n            file_data = read_file(json_file)\n            if file_data:\n                combined_data.extend(file_data)\n                cleaned_name = (\n                    json_file.relative_to(directory)\n                    .with_suffix(\"\")\n                    .as_posix()\n                    .replace(\".instruct\", \"\")\n                )\n                code_filename.append(cleaned_name)\n        except Exception as e:\n            logging.info(f\"Failed reading: {json_file}. Error: {e}\")\n    write_file(combined_data, Path(directory) / \"instruct.json\")\n\n    # Generate document_code.json file\n    purpose_question = [\n        item[\"text\"] for item in questions if item[\"id\"] == \"file_purpose\"\n    ][0]\n    purpose_question = purpose_question.split(\"{filename}\")[0]\n    purpose_data = [\n        item\n        for item in combined_data\n        if item[\"instruction\"].startswith(purpose_question)\n    ]\n    document_code = [\n        {\n            \"document\": item[\"output\"],\n            \"code\": item[\"input\"],\n        }\n        for item in purpose_data\n    ]\n    for i, item in enumerate(document_code):\n        item[\"code filename\"] = code_filename[i]\n    write_file(document_code, Path(directory) / \"document_code.json\")\n\n    # Generate sharegpt.json file\n    sharegpt = [\n        {\n            \"conversation\": [\n                {\"from\": \"system\", \"value\": f\"code documentation: {item['document']}\"},\n                {\n                    \"from\": \"human\",\n                    \"value\": \"Output the Python code described by the code documentation.\",\n                },\n                {\"from\": \"gpt\", \"value\": item[\"code\"]},\n            ],\n            \"nbytes\": \"0\",\n            \"source\": item[\"code filename\"],\n        }\n        for item in document_code\n    ]\n\n    # Compute the number of bytes for each conversation and update the nbytes field\n    for item in sharegpt:\n        nbytes = 0\n        for conv in item[\"conversation\"]:\n            nbytes += len(conv[\"value\"].encode(\"utf-8\"))\n        item[\"nbytes\"] = nbytes\n\n    write_file(sharegpt, Path(directory) / \"shargpt.json\")\n\n    if html:\n        logging.info(\"Converting JSON files to HTML\")\n        convert_json_to_html(directory)\n\n    return {\"instruct_list\": combined_data}\n\n\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    # create graph\n    graph_type = \"entire_code_graph\"\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details[\"file_info\"][graph_type][\"nodes\"])\n    for edge in file_details[\"file_info\"][graph_type][\"edges\"]:\n        source, target = edge[\"source\"], edge[\"target\"]\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(\n                source,\n                target,\n                **{\n                    k: v\n                    for k, v in edge.items()\n                    if k in [\"target_inputs\", \"target_returns\"]\n                },\n            )\n\n    # draw graph\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(\n        G,\n        pos,\n        with_labels=True,\n        font_weight=\"bold\",\n        font_size=8,\n        node_shape=\"s\",\n        node_size=500,\n        width=1,\n        arrowsize=12,\n    )\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if \"target_inputs\" in edge[2] and edge[2][\"target_inputs\"]:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if \"target_returns\" in edge[2] and edge[2][\"target_returns\"]:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[(edge[0], edge[1])] = \"\\n\".join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n\n    # save graph\n    output_file = output_subdir / f\"{base_name}.{graph_type}.png\"\n    plt.savefig(output_file)\n    plt.close()\n\n\ndef save_python_data(\n    file_details: dict, instruct_list: list, relative_path: Path, output_dir: str\n) -> None:\n    \"\"\"\n    Save the details of the Python file as a YAML file, the instruction data as JSON files,\n    and generate and save code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path of the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parent\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    base_name = relative_path.name\n    write_file(instruct_list, output_subdir / f\"{base_name}.instruct.json\")\n    write_file(file_details, output_subdir / f\"{base_name}.details.yaml\")\n\n    # Generating and saving the code graph\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except Exception as e:\n        logging.error(f\"Error creating graph for {base_name}: {e}\", exc_info=True)\n\n```",
        "output": "Overview: The 'py2dataset\\save_output.py' script offers utility functions for handling input/output operations within the py2dataset framework. It includes reading JSON or YAML files, writing data to these formats back, converting JSON files into HTML format, combining multiple JSON files into instruct.json and document_code.json while removing duplicates, generating code graphs based on Python file details as images, and saving Python file information in YAML format along with instruction data as JSON files.\n\n1) Purpose and Processing Approach of py2dataset\\save_output.py: This script serves as a supporting module for managing various input/output operations within the py2dataset toolchain by offering different functionalities related to handling data storage and presentation formats. These operations enable comprehensive analysis, merging datasets from multiple sources into cohesive representations suitable for diverse downstream tasks like training models or documentation generation.\n\n2) API Signatures with Function Descriptions:\na) read_file(file_path: This function reads JSON or YAML files given their paths and returns the contents as dictionaries. It identifies file types using suffixes ('json'/'yaml') and utilizes appropriate Python libraries (json/yaml) to load data accordingly.\nb) write_file(data, file_path): Accepts a dictionary 'data' and a path 'file_path', writes it into JSON or YAML format based on the suffix of 'file_path'. It uses json/'yaml' libraries for serialization with indentation settings (json/'SafeDumper').\nc) convert_json_to_html(directory): Iterates through all JSON files in a given directory ('directory'), converts them into HTML format by preserving spaces and tabs within input fields. It creates an HTML table representation of each file's content using string manipulation techniques.\nd) preserve_spacing(text, tab_width=4): A utility function that maintains whitespaces in text while replacing spaces with '&nbsp;' entities and tabs with specified width ('tab_width') sequences.\ne) combine_json_files(directory, html=False, questions): Combines JSON files from a directory into instruct.json excluding 'shargpt.json' and 'document_code.json', identifies documents describing the purpose from instructions based on query key (\"file_purpose\"). Also produces 'training.json', and when called with html set True performs conversion of generated JSON data to HTML format. Returns a dictionary containing 'instruct_list'.\nf) create_code_graph(file_details, base_name, output_subdir): Creates directed graphs representing Python file code structure using NetworkX library, saves them as PNG images in an output subdirectory ('output_subdir'). It requires file details ('file_details') extracted from a Python script and generates graphs for specific graph types.\ng) save_python_data(file_details, instruct_list, relative_path, output_dir): Takes extracted information of a Python file, its instructions dataset ('instruct_list'), a path within the output directory ('relative_path') and saves these data in YAML/'details.yaml' format while generating code graphs using 'create_code_graph'.\n\n3) Functional Requirements:\na) read_file(file_path): Reads JSON or YAML files, ensuring proper handling of file types through suffix identification ('file_type').\nb) write_file(data, file_path): Writes dictionary data into JSON/YAML format maintaining indentation for readability.\nc) convert_json_to_html(directory): Converts JSON files within a directory to HTML preserving whitespaces in input fields.\nd) combine_json_files(directory, html=False, questions): Merges JSON files into instruct.json and document_code.json after removing duplicates, generates training.json if needed, and converts merged data to HTML format when 'html' is True.\ne) create_code_graph(file_details, base_name, output_subdir): Generates code graphs from Python file details ('file_details'), saving them as PNG images in a subdirectory of 'output_dir'. It leverages networkX to create the graph representation.\nf) save_python_data(file_details, instruct_list, relative_path, output_dir): Processes a Python script's information ('file_details') along with instruction data ('instruct_list'), saves it in YAML/JSON format within 'output_dir', generates code graphs when called.\ng) preserve_spacing(text, tab_width=4): Preserves whitespaces by replacing spaces with HTML entities and tabs with specified width sequences ('tab_width').\n\n4) Variable Descriptions:\na) file_path (read_file): Path object representing the input JSON/YAML file location.\nb) file_type (read_file): File suffix indicating its format ('json'/'yaml').\nc) f with read_file: Opened file stream used for reading data into memory.\nd) row_parts (convert_json_to_html): List of strings to construct HTML table rows incrementally.\ne) html_content (convert_json_to_html): String containing the growing HTML representation with headers and rows data.\nf) column_count (convert_json_to_html): Number of columns in a JSON file table.\ng) dataset (convert_json_to_html): Contents of a JSON file read by 'read_file'.\nh) json_file (convert_json_to_html): Path object representing a JSON file within the directory.\ni) html_rows (convert_json_to_html): List holding rows constructed from table data for writing to HTML files.\nj) cleaned_name (combine_json_files): File name excluding extensions needed in 'instruct'/'document_code'.json files creation.\nk) code_filename (combine_json_files): Relative file names from merged JSON files required for 'document_code.json'.\nl) skip_files (combine_json_files): Set of file names to be skipped during processing ('instruct.json', 'shargpt.json').\nm) purpose_question (combine_json_files): String extracted from instruction dataset describing file purpose.\nn) purpose_data (combine_json_files): List containing entries matching the purpose query ('file_purpose') in combined data.\no) document_code (combine_json_files): List of dictionaries representing Python code documentation with 'code filename'.\np) nbytes (combine_json_files): Summation of encoded string lengths for each conversation edge weight in 'sharegpt' JSON file.\nq) output_subdir (create_code_graph): Path object referring to subdirectory storing output files/images generated by this script.\nr) pos (create_code_graph): Coordinates node positioning using spring layout for drawing graph edges ('nx.spring_layout').\ns) G (create_code_graph): NetworkX DiGraph object representing Python code structure.\nt) edge_labels (create_code_graph): Dictionary mapping edge tuples to multi-line labels in HTML format.\nu) base_name (save_python_data): Base name for output files generated from a Python script ('relative_path').\nv) output_file (create_code_graph): Path object representing the graph image file path with '.png' extension.''''"
    },
    {
        "instruction": "Inputs to `convert_json_to_html` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format and save .html file.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int=4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n    for json_file in Path(directory).rglob('*.json'):\n        try:\n            dataset = read_file(json_file)\n            if not dataset:\n                continue\n        except Exception:\n            continue\n        html_content = '\\n        <html>\\n        <head>\\n            <style>\\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\\n                th, td {\\n                    border: 1px solid black;\\n                    padding: 8px;\\n                    text-align: left;\\n                    white-space: pre-line;\\n                    vertical-align: top;\\n                    word-wrap: break-word;\\n                }\\n            </style>\\n        </head>\\n        <body>\\n            <table>\\n                <thead>\\n                    <tr>\\n        '\n        column_count = len(dataset[0].keys())\n        column_width = round(100 / column_count, 2)\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += '\\n                    </tr>\\n                </thead>\\n                <tbody>\\n        '\n        html_rows = []\n        for entry in dataset:\n            row_parts = ['<tr>']\n            for key in entry:\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                row_parts.append(f'<td>{value}</td>')\n            row_parts.append('</tr>')\n            html_rows.append(''.join(row_parts))\n        html_content += ''.join(html_rows)\n        html_content += '\\n                </tbody>\\n            </table>\\n        </body>\\n        </html>\\n        '\n        html_file_path = json_file.with_suffix('.html')\n        try:\n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f'Failed saving: {html_file_path}')\n```",
        "output": "'directory'\n\nPurpose and Significance:\nThe 'directory' input parameter given to the `convert_json_to_html` function holds significant value as it specifies the pathway leading to a collection of JSON files that need conversion into HTML format. This directory acts as the source location where the script searches for files to process and convert their contents into readable web page representations with preserved whitespaces maintaining tabulations within input fields. Within `convert_json_to_html`, this input guides file iteration and helps construct the output HTML files accordingly. It allows manipulating data in an organized manner, facilitating human interpretation of complex JSON content stored across multiple files related to code documentation or analysis purposes.''"
    },
    {
        "instruction": "Docstring of `convert_json_to_html` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format and save .html file.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int=4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n    for json_file in Path(directory).rglob('*.json'):\n        try:\n            dataset = read_file(json_file)\n            if not dataset:\n                continue\n        except Exception:\n            continue\n        html_content = '\\n        <html>\\n        <head>\\n            <style>\\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\\n                th, td {\\n                    border: 1px solid black;\\n                    padding: 8px;\\n                    text-align: left;\\n                    white-space: pre-line;\\n                    vertical-align: top;\\n                    word-wrap: break-word;\\n                }\\n            </style>\\n        </head>\\n        <body>\\n            <table>\\n                <thead>\\n                    <tr>\\n        '\n        column_count = len(dataset[0].keys())\n        column_width = round(100 / column_count, 2)\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += '\\n                    </tr>\\n                </thead>\\n                <tbody>\\n        '\n        html_rows = []\n        for entry in dataset:\n            row_parts = ['<tr>']\n            for key in entry:\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                row_parts.append(f'<td>{value}</td>')\n            row_parts.append('</tr>')\n            html_rows.append(''.join(row_parts))\n        html_content += ''.join(html_rows)\n        html_content += '\\n                </tbody>\\n            </table>\\n        </body>\\n        </html>\\n        '\n        html_file_path = json_file.with_suffix('.html')\n        try:\n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f'Failed saving: {html_file_path}')\n```",
        "output": "Convert JSON files within a given directory to HTML format and save .html file.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None"
    },
    {
        "instruction": "Calls made in `convert_json_to_html` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format and save .html file.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int=4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n    for json_file in Path(directory).rglob('*.json'):\n        try:\n            dataset = read_file(json_file)\n            if not dataset:\n                continue\n        except Exception:\n            continue\n        html_content = '\\n        <html>\\n        <head>\\n            <style>\\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\\n                th, td {\\n                    border: 1px solid black;\\n                    padding: 8px;\\n                    text-align: left;\\n                    white-space: pre-line;\\n                    vertical-align: top;\\n                    word-wrap: break-word;\\n                }\\n            </style>\\n        </head>\\n        <body>\\n            <table>\\n                <thead>\\n                    <tr>\\n        '\n        column_count = len(dataset[0].keys())\n        column_width = round(100 / column_count, 2)\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += '\\n                    </tr>\\n                </thead>\\n                <tbody>\\n        '\n        html_rows = []\n        for entry in dataset:\n            row_parts = ['<tr>']\n            for key in entry:\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                row_parts.append(f'<td>{value}</td>')\n            row_parts.append('</tr>')\n            html_rows.append(''.join(row_parts))\n        html_content += ''.join(html_rows)\n        html_content += '\\n                </tbody>\\n            </table>\\n        </body>\\n        </html>\\n        '\n        html_file_path = json_file.with_suffix('.html')\n        try:\n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f'Failed saving: {html_file_path}')\n```",
        "output": "'text.replace(', '&nbsp;').replace', 'text.replace', 'Path(directory).rglob', 'Path', 'read_file', 'len', 'dataset[0].keys', 'round', 'escape', 'str', 'preserve_spacing', 'value.replace', 'row_parts.append', 'html_rows.append', '.join', 'json_file.with_suffix', 'open', 'file.write', 'logging.info'\n\nPurpose and Significance:\nIn `convert_json_to_html`, several crucial operations are performed to convert JSON files within a given directory into HTML format while preserving whitespaces in input fields. The primary purpose of these calls is to generate an HTML representation of each JSON file's content with proper table structure and formatting. Here's a breakdown of their roles:\n\n1. `text.replace(', '&nbsp;')` & `value.replace(' ', '&nbsp;')`: These replace specific characters in strings to ensure accurate HTML rendering. The former changes commas into non-breaking spaces ('&nbsp;'), while the latter substitutes regular spaces with non-breakable spaces for maintaining whitespaces within values read from JSON files during HTML table creation.\n2. `Path(directory)`: This creates a Path object representing the given directory path where JSON files are located, enabling access to relevant files using relative paths in subsequent operations.\n3. `json_file.with_suffix('.html')`: Modifies the JSON file path by appending '.html' suffix to generate output HTML filenames for saved converted data.\n4. `read_file(json_file)`: Reads a JSON file using its Path object argument and returns its contents as a dictionary, which will be processed further in this function.\n5. `len(combined_data)`, `len(entry['input'].encode('utf-8')`: Both these calculate string lengths relevant to graph manipulation and other processing needs. However, the first retrieves length from 'combined_data', a list containing multiple JSON file data sets read previously. The second returns UTF-8 byte count of an input field ('entry['input']).\n6. `escape(str(entry['output']))`: Encodes special characters in strings to be safely used within HTML contexts without disrupting their meaning or appearance.\n7. `row_parts`, `html_rows`, & '.join`: Used collectively, they compose individual sections required in developing complete rows or sets of HTML tables consisting of parsed JSON objects for every read JSON file later appended in string output HTML representation ('html_content'). Row-level modifications take place by populating row_parts with `append`, whereas multiple row arrays (`html_rows`) collect row_part combinations per each dataset before generating an extensive html_rows string using '.join'.\n8. `open(html_file_path, 'w', 'encoding=\"utf-8\")` & `file.write(html_content)`: Create a new file with the given path ('html_file_path') in write mode ('w') and save the constructed HTML content ('html_content'). This ensures proper storage of converted JSON data into HTML format.\n9. `logging.info('Failed saving: {html_file_path}')`: Records information about any issues faced while saving HTML files to help debugging during runtime. It logs a warning message indicating failed file writing with respective paths ('html_file_path').''"
    },
    {
        "instruction": "Variables in `convert_json_to_html` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format and save .html file.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int=4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n    for json_file in Path(directory).rglob('*.json'):\n        try:\n            dataset = read_file(json_file)\n            if not dataset:\n                continue\n        except Exception:\n            continue\n        html_content = '\\n        <html>\\n        <head>\\n            <style>\\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\\n                th, td {\\n                    border: 1px solid black;\\n                    padding: 8px;\\n                    text-align: left;\\n                    white-space: pre-line;\\n                    vertical-align: top;\\n                    word-wrap: break-word;\\n                }\\n            </style>\\n        </head>\\n        <body>\\n            <table>\\n                <thead>\\n                    <tr>\\n        '\n        column_count = len(dataset[0].keys())\n        column_width = round(100 / column_count, 2)\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += '\\n                    </tr>\\n                </thead>\\n                <tbody>\\n        '\n        html_rows = []\n        for entry in dataset:\n            row_parts = ['<tr>']\n            for key in entry:\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                row_parts.append(f'<td>{value}</td>')\n            row_parts.append('</tr>')\n            html_rows.append(''.join(row_parts))\n        html_content += ''.join(html_rows)\n        html_content += '\\n                </tbody>\\n            </table>\\n        </body>\\n        </html>\\n        '\n        html_file_path = json_file.with_suffix('.html')\n        try:\n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f'Failed saving: {html_file_path}')\n```",
        "output": "'row_parts', 'value', 'directory', 'html_rows', 'html_file_path', 'html_content', 'dataset', 'column_count', 'column_width'\n\nPurpose and Significance:\nThe variables in `convert_json_to_html` function play crucial roles during the conversion of JSON files into HTML format within a given directory. Their purposes are as follows:\n\n1. `row_parts`: This list is utilized to construct HTML table rows incrementally while generating the final HTML representation. It holds strings representing each cell content in tabular format for every entry in a JSON file. As the function iterates through dataset elements, it appends appropriate tags around values to create well-structured table rows.\n\n2. `value`: Within loops, `value` temporarily stores escaped string versions of JSON entries' values after applying Python's `escape()` function for safe HTML rendering. It ensures proper displaying of characters in the output HTML file without causing security issues.\n\n3. `directory` (not explicitly mentioned but implied): This global variable refers to the input directory path where JSON files are located, as specified in the `combine_json_files()` function call within `convert_json_to_html`. It serves as a context for finding individual JSON files to convert into HTML format.\n\n4. `html_rows`: A list that accumulates complete rows of converted tabular data for each JSON file entry after escaping special characters and formatting required HTML elements around them. Later, this list gets joined into one string with newlines (\"\\n\") separating rows before saving as an HTML file.\n\n5. `html_file_path`: A dynamically generated Path object representing the output filename with a '.html' extension for each JSON input file during conversion. This object serves when saving constructed HTML files with processed contents back in their original folder.\n\n6. `html_content`: Serves as a StringBuilder holding header, opening <HTML> tags followed by tables headers, initial row (column names) and all the HTML rows concatenated till that moment. As function execution proceeds, this variable gathers necessary parts for generating the complete HTML file representation of each JSON file content.\n\n7. `dataset`: This variable stores contents read from a JSON file using `read_file()`. It contains parsed data ready to be processed into HTML format by `convert_json_to_html()`.\n\n8. `column_count`: Calculated by counting the number of keys in the first JSON entry's dictionary, representing columns present in each table row. This value is used later for setting width percentages in CSS styles applied to table cells ensuring proper alignment and layout.\n\n9. `column_width`: A calculated percentage value derived from dividing 100 by `column_count`. It determines the width assigned to individual table columns for optimized rendering and presentation when writing HTML rows into final content (`html_content`). This guarantees an equally distributed column distribution.\nThese variables are pivotal in converting JSON data into an accessible human-readable HTML representation preserving spaces and tabs from input files while generating tabular structure accordingly.''"
    },
    {
        "instruction": "Returns from `convert_json_to_html` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format and save .html file.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int=4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n    for json_file in Path(directory).rglob('*.json'):\n        try:\n            dataset = read_file(json_file)\n            if not dataset:\n                continue\n        except Exception:\n            continue\n        html_content = '\\n        <html>\\n        <head>\\n            <style>\\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\\n                th, td {\\n                    border: 1px solid black;\\n                    padding: 8px;\\n                    text-align: left;\\n                    white-space: pre-line;\\n                    vertical-align: top;\\n                    word-wrap: break-word;\\n                }\\n            </style>\\n        </head>\\n        <body>\\n            <table>\\n                <thead>\\n                    <tr>\\n        '\n        column_count = len(dataset[0].keys())\n        column_width = round(100 / column_count, 2)\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += '\\n                    </tr>\\n                </thead>\\n                <tbody>\\n        '\n        html_rows = []\n        for entry in dataset:\n            row_parts = ['<tr>']\n            for key in entry:\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                row_parts.append(f'<td>{value}</td>')\n            row_parts.append('</tr>')\n            html_rows.append(''.join(row_parts))\n        html_content += ''.join(html_rows)\n        html_content += '\\n                </tbody>\\n            </table>\\n        </body>\\n        </html>\\n        '\n        html_file_path = json_file.with_suffix('.html')\n        try:\n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f'Failed saving: {html_file_path}')\n```",
        "output": "'text.replace(', '&nbsp;').replace('\\\\t', '&nbsp;' * tab_width)'\n\nPurpose and Significance:\nThe 'convert_json_to_html' function in the `py2dataset.save_output.py` module operates specifically on converting given JSON files stored in a provided directory to HTML format with retained white-space appearance. These processed files bear a .html extension and exhibit data in table form for improved readability. It handles multiple input files by iterating through all '.json' entities within the specified directory path, extracting their contents using `read_file`. Then, it builds an HTML structure containing headers and rows with preserved spaces and tabs from each JSON file content. The function ensures proper alignment of columns by calculating column widths proportionally based on the number of keys in each dataset entry. Finally, these tables are written to disk under modified '.html' filenames replacing the original '.json' extension. In summary, this method provides an alternative view of structured JSON data while preserving input whitespaces crucial for documentation purposes. It facilitates human readability without losing essential information.'''"
    },
    {
        "instruction": "Inputs to `combine_json_files` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef combine_json_files(directory: str, html: bool, questions: Dict) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and\n    then remove duplicates. Also generate a 'training.json' file.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n    logging.info(f'Combining JSON files in {directory}')\n    combined_data, code_filename = ([], [])\n    skip_files = {'instruct.json', 'shargpt.json', 'document_code.json'}\n    for json_file in Path(directory).rglob('*.json'):\n        if json_file.name in skip_files:\n            continue\n        try:\n            file_data = read_file(json_file)\n            if file_data:\n                combined_data.extend(file_data)\n                cleaned_name = json_file.relative_to(directory).with_suffix('').as_posix().replace('.instruct', '')\n                code_filename.append(cleaned_name)\n        except Exception as e:\n            logging.info(f'Failed reading: {json_file}. Error: {e}')\n    write_file(combined_data, Path(directory) / 'instruct.json')\n    purpose_question = [item['text'] for item in questions if item['id'] == 'file_purpose'][0]\n    purpose_question = purpose_question.split('{filename}')[0]\n    purpose_data = [item for item in combined_data if item['instruction'].startswith(purpose_question)]\n    document_code = [{'document': item['output'], 'code': item['input']} for item in purpose_data]\n    for i, item in enumerate(document_code):\n        item['code filename'] = code_filename[i]\n    write_file(document_code, Path(directory) / 'document_code.json')\n    sharegpt = [{'conversation': [{'from': 'system', 'value': f\"code documentation: {item['document']}\"}, {'from': 'human', 'value': 'Output the Python code described by the code documentation.'}, {'from': 'gpt', 'value': item['code']}], 'nbytes': '0', 'source': item['code filename']} for item in document_code]\n    for item in sharegpt:\n        nbytes = 0\n        for conv in item['conversation']:\n            nbytes += len(conv['value'].encode('utf-8'))\n        item['nbytes'] = nbytes\n    write_file(sharegpt, Path(directory) / 'shargpt.json')\n    if html:\n        logging.info('Converting JSON files to HTML')\n        convert_json_to_html(directory)\n    return {'instruct_list': combined_data}\n```",
        "output": "'directory', 'html', 'questions'\n\nPurpose and Significance:\nThe `combine_json_files` function plays a crucial role within the py2dataset framework by consolidating multiple JSON files into structured datasets for further processing. It primarily operates on a specified directory containing JSON data and handles three primary inputs - 'directory', 'html', and 'questions'. These inputs contribute to various functionalities as follows:\n\n1. `directory` (str): This argument represents the path pointing towards the directory where all JSON files reside that need to be combined into coherent datasets. It initiates the process of collecting data from each JSON file within this location and performing subsequent operations like removing duplicates, merging contents into 'instruct.json', generating 'training.json' if required, and converting merged JSON files into HTML format when instructed ('html' parameter is True).\n\n2. `html` (bool): A flag determining whether to generate HTML representations of combined datasets after the data processing step. If set as True, it triggers the conversion process following merging JSON files into 'instruct_list'. This helps visualize information more clearly or prepare training sets for machine learning models depending on the user's need.\n\n3. `questions` (List[Dict]): Although not explicitly used within `combine_json_files`, it relates to filtering relevant entries from combined data ('purpose_question') - an instruction subset with \"file_purpose\" key identifying files describing the code's intent (\"{filename}\"). This list contains dictionaries representing individual instructions extracted during Python script analysis. It helps identify documents related to a particular purpose while generating 'training.json'. In essence, it assists in refining the output datasets based on user queries or specific requirements.'''"
    },
    {
        "instruction": "Docstring of `combine_json_files` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef combine_json_files(directory: str, html: bool, questions: Dict) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and\n    then remove duplicates. Also generate a 'training.json' file.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n    logging.info(f'Combining JSON files in {directory}')\n    combined_data, code_filename = ([], [])\n    skip_files = {'instruct.json', 'shargpt.json', 'document_code.json'}\n    for json_file in Path(directory).rglob('*.json'):\n        if json_file.name in skip_files:\n            continue\n        try:\n            file_data = read_file(json_file)\n            if file_data:\n                combined_data.extend(file_data)\n                cleaned_name = json_file.relative_to(directory).with_suffix('').as_posix().replace('.instruct', '')\n                code_filename.append(cleaned_name)\n        except Exception as e:\n            logging.info(f'Failed reading: {json_file}. Error: {e}')\n    write_file(combined_data, Path(directory) / 'instruct.json')\n    purpose_question = [item['text'] for item in questions if item['id'] == 'file_purpose'][0]\n    purpose_question = purpose_question.split('{filename}')[0]\n    purpose_data = [item for item in combined_data if item['instruction'].startswith(purpose_question)]\n    document_code = [{'document': item['output'], 'code': item['input']} for item in purpose_data]\n    for i, item in enumerate(document_code):\n        item['code filename'] = code_filename[i]\n    write_file(document_code, Path(directory) / 'document_code.json')\n    sharegpt = [{'conversation': [{'from': 'system', 'value': f\"code documentation: {item['document']}\"}, {'from': 'human', 'value': 'Output the Python code described by the code documentation.'}, {'from': 'gpt', 'value': item['code']}], 'nbytes': '0', 'source': item['code filename']} for item in document_code]\n    for item in sharegpt:\n        nbytes = 0\n        for conv in item['conversation']:\n            nbytes += len(conv['value'].encode('utf-8'))\n        item['nbytes'] = nbytes\n    write_file(sharegpt, Path(directory) / 'shargpt.json')\n    if html:\n        logging.info('Converting JSON files to HTML')\n        convert_json_to_html(directory)\n    return {'instruct_list': combined_data}\n```",
        "output": "Combine all JSON files in the output directory into 'instruct.json', and\n    then remove duplicates. Also generate a 'training.json' file.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets."
    },
    {
        "instruction": "Calls made in `combine_json_files` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef combine_json_files(directory: str, html: bool, questions: Dict) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and\n    then remove duplicates. Also generate a 'training.json' file.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n    logging.info(f'Combining JSON files in {directory}')\n    combined_data, code_filename = ([], [])\n    skip_files = {'instruct.json', 'shargpt.json', 'document_code.json'}\n    for json_file in Path(directory).rglob('*.json'):\n        if json_file.name in skip_files:\n            continue\n        try:\n            file_data = read_file(json_file)\n            if file_data:\n                combined_data.extend(file_data)\n                cleaned_name = json_file.relative_to(directory).with_suffix('').as_posix().replace('.instruct', '')\n                code_filename.append(cleaned_name)\n        except Exception as e:\n            logging.info(f'Failed reading: {json_file}. Error: {e}')\n    write_file(combined_data, Path(directory) / 'instruct.json')\n    purpose_question = [item['text'] for item in questions if item['id'] == 'file_purpose'][0]\n    purpose_question = purpose_question.split('{filename}')[0]\n    purpose_data = [item for item in combined_data if item['instruction'].startswith(purpose_question)]\n    document_code = [{'document': item['output'], 'code': item['input']} for item in purpose_data]\n    for i, item in enumerate(document_code):\n        item['code filename'] = code_filename[i]\n    write_file(document_code, Path(directory) / 'document_code.json')\n    sharegpt = [{'conversation': [{'from': 'system', 'value': f\"code documentation: {item['document']}\"}, {'from': 'human', 'value': 'Output the Python code described by the code documentation.'}, {'from': 'gpt', 'value': item['code']}], 'nbytes': '0', 'source': item['code filename']} for item in document_code]\n    for item in sharegpt:\n        nbytes = 0\n        for conv in item['conversation']:\n            nbytes += len(conv['value'].encode('utf-8'))\n        item['nbytes'] = nbytes\n    write_file(sharegpt, Path(directory) / 'shargpt.json')\n    if html:\n        logging.info('Converting JSON files to HTML')\n        convert_json_to_html(directory)\n    return {'instruct_list': combined_data}\n```",
        "output": "'logging.info', 'Path(directory).rglob', 'Path', 'read_file', 'combined_data.extend', 'json_file.relative_to(directory).with_suffix('').as_posix().replace', 'json_file.relative_to(directory).with_suffix('').as_posix', 'json_file.relative_to(directory).with_suffix', 'json_file.relative_to', 'code_filename.append', 'write_file', 'purpose_question.split', 'item['instruction'].startswith', 'enumerate', 'len', 'conv['value'].encode', 'convert_json_to_html'\n\nPurpose and Significance:\nThe `combine_json_files` function plays a crucial role within the py2dataset script by consolidating data extracted from numerous JSON files in a specific directory. It manages different operations involving merge, cleansing duplicates, producing structured datasets and even handling conversion of some merged datasets to HTML format upon demand (flagged by `html` argument). Breaking down each call within this function:\n\n1. `logging.info(\"Combining JSON files in {directory}\")`: This logs an informative message indicating the process of combining JSON files present within given `directory`. It supports developers' visibility on current execution step, adding useful details like directories used to combine JSON datasets. Python's `logging` module aids tracking workflows while processing tasks in background operations.\n2. `Path(directory).rglob('*.json')`: Using Pathlib's `Path()`, it scans the input directory ('directory') for all files with '.json' extension, collecting paths to JSON files within that directory. The `rglob()` method returns an iterator over pathlike objects matching a specified pattern ('*.json').\n3. `read_file(json_file)`: This call reads each JSON file from the iterator into memory as dictionaries, where `json_file` represents an individual path found by `Path(directory).rglob('*.json')`. `read_file()` handles JSON or YAML formats according to file suffixes ('json'/'yaml').\n4. `combined_data.extend(file_data)`: After reading each JSON file, the collected data is appended into a master list `combined_data`, preparing it for subsequent merging operations and duplicate removal steps.\n5. `json_file.relative_to(directory).with_suffix('').as_posix()` & `json_file.relative_to(directory).with_suffix('').as_posix()`: These lines transform file paths from their original structure into simplified names suitable for renaming the 'document_code' output files ('instruct','shargpt', etc.). By dropping extensions and keeping directory details intact, these methods prepare strings needed in saving finalized JSON datasets.\n6. `write_file(combined_data, Path(directory) / \"instruct.json\")`: Merges the processed combined_data list into 'instruct.json' residing in provided directory by using `write_file()`. It stores aggregated information without duplicate entries in the target file for future uses such as training model data generation or documentation purposes.\n7. `purpose_question = [item[\"text\"] for item in questions if item[\"id\"] == \"file_purpose\"`[0]`: This extracts a specific query string ('file_purpose') from the instruction dataset ('questions'), identifying the purpose description for each Python script processed by py2dataset.\n8. `enumerate(combined_data)`: Iterates over combined JSON data with indexing support, enabling access to both keys and values simultaneously in a tuple format (index, item). This helps filtering instructions related to file purposes later on.\n9. `len(conv['value'].encode('utf-8')`: Encodes instruction strings into bytes using UTF-8 encoding to compute 'nbytes' field values in the 'sharegpt' JSON object accurately representing edge weights of graph edges (data transfer size).\n10. `convert_json_to_html(directory)`: If `html` flag is set to True during function call, this triggers HTML conversion for combined JSON datasets following `convert_json_to_html()`. It preserves whitespaces in input fields while generating HTML representations of JSON files within the directory.\n\nIn summary, `combine_json_files()` consolidates JSON data from a given path, cleans duplicates, generates other relevant JSON files as necessary and transforms outputs (based on flag) with crucial method invocations reflecting operations on read datasets or conversion processes.''"
    },
    {
        "instruction": "Variables in `combine_json_files` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef combine_json_files(directory: str, html: bool, questions: Dict) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and\n    then remove duplicates. Also generate a 'training.json' file.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n    logging.info(f'Combining JSON files in {directory}')\n    combined_data, code_filename = ([], [])\n    skip_files = {'instruct.json', 'shargpt.json', 'document_code.json'}\n    for json_file in Path(directory).rglob('*.json'):\n        if json_file.name in skip_files:\n            continue\n        try:\n            file_data = read_file(json_file)\n            if file_data:\n                combined_data.extend(file_data)\n                cleaned_name = json_file.relative_to(directory).with_suffix('').as_posix().replace('.instruct', '')\n                code_filename.append(cleaned_name)\n        except Exception as e:\n            logging.info(f'Failed reading: {json_file}. Error: {e}')\n    write_file(combined_data, Path(directory) / 'instruct.json')\n    purpose_question = [item['text'] for item in questions if item['id'] == 'file_purpose'][0]\n    purpose_question = purpose_question.split('{filename}')[0]\n    purpose_data = [item for item in combined_data if item['instruction'].startswith(purpose_question)]\n    document_code = [{'document': item['output'], 'code': item['input']} for item in purpose_data]\n    for i, item in enumerate(document_code):\n        item['code filename'] = code_filename[i]\n    write_file(document_code, Path(directory) / 'document_code.json')\n    sharegpt = [{'conversation': [{'from': 'system', 'value': f\"code documentation: {item['document']}\"}, {'from': 'human', 'value': 'Output the Python code described by the code documentation.'}, {'from': 'gpt', 'value': item['code']}], 'nbytes': '0', 'source': item['code filename']} for item in document_code]\n    for item in sharegpt:\n        nbytes = 0\n        for conv in item['conversation']:\n            nbytes += len(conv['value'].encode('utf-8'))\n        item['nbytes'] = nbytes\n    write_file(sharegpt, Path(directory) / 'shargpt.json')\n    if html:\n        logging.info('Converting JSON files to HTML')\n        convert_json_to_html(directory)\n    return {'instruct_list': combined_data}\n```",
        "output": "'nbytes', 'purpose_data', 'sharegpt', 'cleaned_name', 'skip_files', 'questions', 'file_data', 'directory', 'purpose_question', 'document_code', 'html'\n\nPurpose and Significance:\nIn the `combine_json_files` function within the py2dataset script's save_output module, several variables play crucial roles to manage JSON file merging and generating specific output files. Their purposes are as follows:\n\n1) nbytes: This variable keeps track of edge weights in the generated 'sharegpt' JSON file representing the number of bytes for each conversation edge within the code graph. It calculates the byte lengths of messages exchanged between nodes by summing up encoded string lengths using Python's `len()` function on edge values related to \"target_inputs\" and \"target_returns\".\n\n2) purpose_data: A list extracted from combined JSON data containing entries where instruction text starts with a given purpose query ('file_purpose') about code documentation. This data forms part of 'document_code', holding Python file outputs related to described documentations.\n\n3) sharegpt: An ordered dictionary simulating NetworkX edge weights with sources (Python function nodes) linked to target destinations inferred from processed files. Its contents contribute significantly towards final JSON structures depicting how source codes operate together during downstream processing steps.\n\n4) cleaned_name: This variable helps construct filenames for 'instruct' and 'document_code' JSON files without extensions. It refines input paths relative to the given output directory into simple, comprehensible aliases using various String Operations, required particularly in structuring resulting output dataset format representations within file saving logic.\n\n5) skip_files: It defines a pre-decided list consisting of excludable file names ('instruct.json', 'shargpt.json') from JSON processing operations. These files are intentionally omitted during data combination stages to maintain a standardized workflow across datasets.\n\n6) questions: An abstract list housing key-value pairs relating instructions about query strings targeting certain identifiers (\"file_purpose\"). Here it identifies strings leading towards filtering instructions pertinent for the file description task from comprehensive input dataset aggregations ('combined_data').\n\n7) file_data: Initially undefined in `combine_json_files`, but related to the function's argument 'combined_data'. This variable holds JSON data read from Python files, contributing as initial information when combining multiple datasets into consolidated representations like instruct.json or training.json (when `html` flag is True).\n\n8) directory: Passed as an input parameter and widely utilized within different utility methods \u2013 represents a String addressing where processed files lie to provide broader scope over interacted folders with respect to reading-writing functions during script executions. Primarily navigating to correct source path location ('Path()'). It also determines HTML conversion for combined JSON files when instructed.\n\n9) purpose_question: An element extracted from 'questions' list when filtering instruction entries relevant to Python file descriptions ('file_purpose'). It assists in pinpointing desired instruction lines based on pattern matching.\n\n10) document_code: A structured list comprising Python code documentation objects. Each dictionary inside holds \"document\" as output and \"code filename\". Thus encapsulating key results associated with documentation efforts resulting in consolidated graph form during saving actions related to this metadata collection and maintenance ('training').\n\n11) html (entering 'combine_json_files'): A boolean flag controlling HTML conversion functionality within the function. If set True, it initiates JSON files transformation into HTML format for visual representation purposes.\"''"
    },
    {
        "instruction": "Returns from `combine_json_files` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef combine_json_files(directory: str, html: bool, questions: Dict) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and\n    then remove duplicates. Also generate a 'training.json' file.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n    logging.info(f'Combining JSON files in {directory}')\n    combined_data, code_filename = ([], [])\n    skip_files = {'instruct.json', 'shargpt.json', 'document_code.json'}\n    for json_file in Path(directory).rglob('*.json'):\n        if json_file.name in skip_files:\n            continue\n        try:\n            file_data = read_file(json_file)\n            if file_data:\n                combined_data.extend(file_data)\n                cleaned_name = json_file.relative_to(directory).with_suffix('').as_posix().replace('.instruct', '')\n                code_filename.append(cleaned_name)\n        except Exception as e:\n            logging.info(f'Failed reading: {json_file}. Error: {e}')\n    write_file(combined_data, Path(directory) / 'instruct.json')\n    purpose_question = [item['text'] for item in questions if item['id'] == 'file_purpose'][0]\n    purpose_question = purpose_question.split('{filename}')[0]\n    purpose_data = [item for item in combined_data if item['instruction'].startswith(purpose_question)]\n    document_code = [{'document': item['output'], 'code': item['input']} for item in purpose_data]\n    for i, item in enumerate(document_code):\n        item['code filename'] = code_filename[i]\n    write_file(document_code, Path(directory) / 'document_code.json')\n    sharegpt = [{'conversation': [{'from': 'system', 'value': f\"code documentation: {item['document']}\"}, {'from': 'human', 'value': 'Output the Python code described by the code documentation.'}, {'from': 'gpt', 'value': item['code']}], 'nbytes': '0', 'source': item['code filename']} for item in document_code]\n    for item in sharegpt:\n        nbytes = 0\n        for conv in item['conversation']:\n            nbytes += len(conv['value'].encode('utf-8'))\n        item['nbytes'] = nbytes\n    write_file(sharegpt, Path(directory) / 'shargpt.json')\n    if html:\n        logging.info('Converting JSON files to HTML')\n        convert_json_to_html(directory)\n    return {'instruct_list': combined_data}\n```",
        "output": "'{'instruct_list': combined_data}'\n\nPurpose and Significance:\nThe 'combine_json_files' function within the py2dataset framework serves a crucial role in consolidating multiple JSON files present in a specified directory into two primary outputs - 'instruct.json' and 'document_code.json'. Its main purpose is to merge all JSON data while eliminating duplicates for efficient handling of instruction datasets. This amalgamated information is stored under the key 'instruct_list', which represents an organized collection of combined data extracted from various input files. The primary tasks performed by this function are as follows:\n\n1. Combining JSON files excluding predefined exceptions ('instruct.json' and 'shargpt.json') within the specified output directory path to form the single consolidated JSON object ('combined_data'). These data integrate after skipping specified exception files during the loop execution where paths do not match provided lists 'skip_files'.\n2. Processing purpose detection: A text query term named 'purpose_question' identifies descriptions corresponding to a specific code functionality extracted from question items of 'combined_data'. Only those instructions starting with 'purpose_question' are selected into the 'purpose_data' list.\n3. Generating 'document_code': A list comprehension creates a data structure ('document_code') containing Python code documentation with their respective 'code filename', extracted from 'purpose_data'. This data arrangement facilitates easier association between code snippets and corresponding documents in downstream tasks.\n4. Updating nbytes field: For each edge within the graph structure ('G'), a variable 'nbytes' stores byte lengths for conversational interactions by iterating through its nodes and edges information, utilizing encoding conversion for length calculations.\n5. Saving updated data structures: Finally, it writes combined data into 'instruct.json', generates 'training.json', and creates HTML versions of these files when 'html' parameter is set to True (via calling convert_json_to_html()). Therefore,'{'instruct_list': combined_data}' offers simplified accessibility of accumulated JSON data sets needed in later workflows while the rest ensures their cleanliness with merging operations, data format modifications or documentation generation."
    },
    {
        "instruction": "Inputs to `create_code_graph` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    graph_type = 'entire_code_graph'\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details['file_info'][graph_type]['nodes'])\n    for edge in file_details['file_info'][graph_type]['edges']:\n        source, target = (edge['source'], edge['target'])\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(source, target, **{k: v for k, v in edge.items() if k in ['target_inputs', 'target_returns']})\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(G, pos, with_labels=True, font_weight='bold', font_size=8, node_shape='s', node_size=500, width=1, arrowsize=12)\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if 'target_returns' in edge[2] and edge[2]['target_returns']:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[edge[0], edge[1]] = '\\n'.join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n    output_file = output_subdir / f'{base_name}.{graph_type}.png'\n    plt.savefig(output_file)\n    plt.close()\n```",
        "output": "'file_details', 'base_name', 'output_subdir'\n\nPurpose and Significance:\nIn the context of `create_code_graph` function within py2dataset's save_output.py module, the inputs 'file_details', 'base_name', and 'output_subdir' play crucial roles in generating visual representations of Python file code structures as PNG images. These parameters hold essential information required for constructing directed graphs that depict the relationships between different elements within a Python script.\n\n1) file_details: This argument represents a dictionary containing extracted details from a specific Python file. It serves as the blueprint for generating graph nodes and edges in `create_code_graph`. It carries vital information about code entities like functions, classes, modules, variables, etc., enabling NetworkX library to build an accurate graph representation reflecting Python script dependencies.\n\n2) base_name: A string variable representing the base name assigned to output files resulting from this function's execution. It forms part of image file names in 'output_dir', facilitating easy identification and organization of generated code graphs. The '.png' extension is appended automatically, making it easier to differentiate between various graph types (e.g., entire_code_graph).\n\n3) output_subdir: A Path object referring to a subdirectory within the main 'output_dir'. This parameter specifies where generated code graphs will be saved after creation by `create_code_graph`. It ensures all images are stored in an organized manner for future reference or further processing. By using Path methods such as mkdir('parents=True, exist_ok=True'), this subdirectory gets created if necessary prior to saving the graphs. Once implemented, 'output_subdir' provides a common destination for Python file code graphs produced during script execution.'''"
    },
    {
        "instruction": "Docstring of `create_code_graph` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    graph_type = 'entire_code_graph'\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details['file_info'][graph_type]['nodes'])\n    for edge in file_details['file_info'][graph_type]['edges']:\n        source, target = (edge['source'], edge['target'])\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(source, target, **{k: v for k, v in edge.items() if k in ['target_inputs', 'target_returns']})\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(G, pos, with_labels=True, font_weight='bold', font_size=8, node_shape='s', node_size=500, width=1, arrowsize=12)\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if 'target_returns' in edge[2] and edge[2]['target_returns']:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[edge[0], edge[1]] = '\\n'.join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n    output_file = output_subdir / f'{base_name}.{graph_type}.png'\n    plt.savefig(output_file)\n    plt.close()\n```",
        "output": "Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None"
    },
    {
        "instruction": "Calls made in `create_code_graph` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    graph_type = 'entire_code_graph'\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details['file_info'][graph_type]['nodes'])\n    for edge in file_details['file_info'][graph_type]['edges']:\n        source, target = (edge['source'], edge['target'])\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(source, target, **{k: v for k, v in edge.items() if k in ['target_inputs', 'target_returns']})\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(G, pos, with_labels=True, font_weight='bold', font_size=8, node_shape='s', node_size=500, width=1, arrowsize=12)\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if 'target_returns' in edge[2] and edge[2]['target_returns']:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[edge[0], edge[1]] = '\\n'.join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n    output_file = output_subdir / f'{base_name}.{graph_type}.png'\n    plt.savefig(output_file)\n    plt.close()\n```",
        "output": "'nx.DiGraph', 'G.add_nodes_from', 'G.add_edge', 'edge.items', 'plt.figure', 'nx.spring_layout', 'nx.draw', 'G.edges', 'label.append', '', '.join', '\\\\n'.join', 'nx.draw_networkx_edge_labels', 'plt.savefig', 'plt.close'\n\nPurpose and Significance:\nIn `create_code_graph` within py2dataset's save_output module, several crucial Python libraries are utilized to generate visual representations of Python file code structures as PNG images. The primary purpose is to create directed graphs representing interconnections between various elements in a script for better understanding its internal workflow. Here's a breakdown of each significant call made within this function:\n\n1. `nx.DiGraph()`: Instantiates an empty Directed Graph object ('G') from the networkX library to store nodes and edges representing Python code components. This data structure helps in efficiently modeling complex relationships between different parts of the script.\n2. `G.add_nodes_from(file_details[\"file_info\"][graph_type][\"nodes\"])`: Retrieves relevant nodes (scripts elements) from provided 'file_details' dictionary under 'file_info', specifically for a specified graph type ('entire_code_graph'), and adds them to the initialized Directed Graph ('G').\n3. `for edge in G.edges(data=True):`: Iterates through edges connecting nodes within the Directed Graph ('G') based on their metadata provided in 'file_details'. This step helps constructing connections between source and target elements with additional information like input requirements and output returns.\n4. `label = []`: Initializes an empty list to store multi-line labels for each graph edge display on image. Each label combines target inputs ('edge[2][\"target_inputs\"]') with corresponding outputs ('edge[2][\"target_returns\"]').\n5. `edge_labels[(edge[0], edge[1]] = \"\\n\".join(label)`: Creates a key-value pair in dictionary 'edge_labels' mapping tuple of source and target nodes to their combined multi-line label extracted from the previous step. This allows proper rendering of edge labels on the graph image.\n6. `plt.figure(figsize=(20, 20))`: Sets up a new figure with dimensions (width=20 inches, height=20 inches) for plotting purposes using matplotlib's PyPlot library ('plt'). This ensures adequate space to display complex graph structures clearly.\n7. `nx.draw(G, pos, ...)`: Draws the Directed Graph ('G') with node positions determined by 'pos', font properties (bold labels, large nodes), edges with arrowheads and custom edge labels from 'edge_labels'. This step generates a visual representation of Python code dependencies on screen.\n8. `plt.savefig(output_file)`: Saves the drawn graph as a PNG image using matplotlib's PyPlot save function ('plt.savefig'). It takes output file path ('output_file') obtained from combining base name and '.png' extension to store visualized code structure permanently.\n9. `plt.close()`: Closes the current figure instance created by matplotlib PyPlot to prevent unexpected graph renderings later on within code execution flow.```''"
    },
    {
        "instruction": "Variables in `create_code_graph` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    graph_type = 'entire_code_graph'\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details['file_info'][graph_type]['nodes'])\n    for edge in file_details['file_info'][graph_type]['edges']:\n        source, target = (edge['source'], edge['target'])\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(source, target, **{k: v for k, v in edge.items() if k in ['target_inputs', 'target_returns']})\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(G, pos, with_labels=True, font_weight='bold', font_size=8, node_shape='s', node_size=500, width=1, arrowsize=12)\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if 'target_returns' in edge[2] and edge[2]['target_returns']:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[edge[0], edge[1]] = '\\n'.join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n    output_file = output_subdir / f'{base_name}.{graph_type}.png'\n    plt.savefig(output_file)\n    plt.close()\n```",
        "output": "'pos', 'G', 'output_file', 'output_subdir', 'label', 'graph_type', 'base_name', 'edge_labels', 'file_details'\n\nPurpose and Significance:\nIn `create_code_graph` function within py2dataset's save_output.py module, several variables play crucial roles to generate code graphs representing Python file structures as PNG images. Their purposes are outlined below:\n1. **pos**: Represents coordinate placements of nodes during NetworkX directed graph rendering (`nx.spring_layout` calls utilize it to lay out graph edges elegantly before image saving. Its role primarily impacts graph aesthetic qualities ensuring clarity of visualization.\n2. **G**: An instance of `DiGraph` class from NetworkX library, which represents the Python code structure as a directed graph. This object stores nodes and edges extracted from given `file_details`, enabling further manipulation to create graph representations.\n3. **output_file**: A PathLib Path object referring to the final output image file path appended with '.png' extension. It indicates where generated graphs will be saved within the user-defined output directory. This variable is used in `plt.savefig()` call for saving visualizations.\n4. **label**: An intermediate list holding multi-line labels for edges in HTML format within `nx.draw_networkx_edge_labels`. It collects input/output details from graph edges to provide contextual information when hovering over connections in the generated image.\n5. **graph_type**: A string parameter passed into `create_code_graph` indicating which graph type ('entire_code_graph') is being created and processed. This helps identify the nature of relationships captured within the Python file code structure visualization.\n6. **base_name**: A base name derived from the input Python script's relative path used for naming generated image files consistently across different graph types (e.g., 'entire_code_graph'). It simplifies tracking output files during analysis or documentation generation tasks.\n7. **edge_labels**: A dictionary mapping edge tuples to multi-line labels constructed in `label`. This data structure connects graph edges with their corresponding descriptions extracted from file details ('file_details') and assists in generating informative visualizations.\n8. **file_details**: A dictionary containing Python script information extracted during function invocation. It serves as input for `create_code_graph` to understand the code structure and generate accurate graph representations.'''"
    },
    {
        "instruction": "Inputs to `save_python_data` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef save_python_data(file_details: dict, instruct_list: list, relative_path: Path, output_dir: str) -> None:\n    \"\"\"\n    Save the details of the Python file as a YAML file, the instruction data as JSON files,\n    and generate and save code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path of the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parent\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    base_name = relative_path.name\n    write_file(instruct_list, output_subdir / f'{base_name}.instruct.json')\n    write_file(file_details, output_subdir / f'{base_name}.details.yaml')\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except Exception as e:\n        logging.error(f'Error creating graph for {base_name}: {e}', exc_info=True)\n```",
        "output": "'file_details', 'instruct_list', 'relative_path', 'output_dir'\n\nPurpose and Significance:\nThe `save_python_data` function within the py2dataset framework serves as a crucial component for managing output generation related to Python scripts. It processes essential inputs that facilitate saving data extracted from a specific Python file into appropriate formats while generating visual representations of its code structure. These inputs play distinct roles in accomplishing this task:\n\n1) `file_details`: This parameter represents the parsed information about a Python script obtained through analysis or extraction procedures within the py2dataset pipeline. It holds details required to generate code graphs representing the script's structure and other relevant metadata necessary for documentation purposes. By passing file_details as an argument, save_python_data ensures accurate representation of the Python file in subsequent operations like creating graph images.\n\n2) `instruct_list`: The 'instruct_list' parameter contains instruction datasets extracted from the analyzed Python script. It consists of parsed JSON data derived from combining multiple JSON files located within the directory mentioned in relative_path, enabling downstream tasks such as training models or generating instruction manuals. save_python_data saves this dataset into instruct.json format to maintain organized documentation of processed instructions.\n\n3) `relative_path`: This Path object refers to the location of the Python script within the file system relative to the current working directory. It helps in identifying the correct position for saving generated output files like YAML ('details.yaml') containing information about the input Python file's attributes and creating the respective directories needed to organize graph images alongside instruct.json files or any other resources related to Python scripts documentation process flow.\n\n4) `output_dir`: A string argument specifying the target directory where all generated outputs will be saved. It allows users to customize the output location according to their project structure and organization preferences, ensuring easy accessibility of results in a desired location. save_python_data leverages this path input while saving extracted data from Python files into required formats as well as producing graphical representations related to code analysis in its subdirectory (created with 'output_subdir').'''"
    },
    {
        "instruction": "Docstring of `save_python_data` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef save_python_data(file_details: dict, instruct_list: list, relative_path: Path, output_dir: str) -> None:\n    \"\"\"\n    Save the details of the Python file as a YAML file, the instruction data as JSON files,\n    and generate and save code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path of the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parent\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    base_name = relative_path.name\n    write_file(instruct_list, output_subdir / f'{base_name}.instruct.json')\n    write_file(file_details, output_subdir / f'{base_name}.details.yaml')\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except Exception as e:\n        logging.error(f'Error creating graph for {base_name}: {e}', exc_info=True)\n```",
        "output": "Save the details of the Python file as a YAML file, the instruction data as JSON files,\n    and generate and save code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path of the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None"
    },
    {
        "instruction": "Calls made in `save_python_data` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef save_python_data(file_details: dict, instruct_list: list, relative_path: Path, output_dir: str) -> None:\n    \"\"\"\n    Save the details of the Python file as a YAML file, the instruction data as JSON files,\n    and generate and save code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path of the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parent\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    base_name = relative_path.name\n    write_file(instruct_list, output_subdir / f'{base_name}.instruct.json')\n    write_file(file_details, output_subdir / f'{base_name}.details.yaml')\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except Exception as e:\n        logging.error(f'Error creating graph for {base_name}: {e}', exc_info=True)\n```",
        "output": "'Path', 'output_subdir.mkdir', 'write_file', 'create_code_graph', 'logging.error'\n\nPurpose and Significance:\nThe `save_python_data` function in py2dataset's save_output module serves as a comprehensive data saving mechanism for Python scripts along with their associated instruction datasets. It involves multiple crucial operations to ensure proper storage and visualization of information extracted from Python files. Its primary responsibilities include:\n\n1. `Path` Objects: These objects represent paths within the file system used for managing input/output locations efficiently. They facilitate navigation between directories and handling relative paths while maintaining compatibility with Python's pathlib library functionality. Instantiating `Path` allows smooth handling of inputs such as Python script paths (`relative_path`) or output subdirectories (`output_dir`).\n\n2. `output_subdir.mkdir(parents=True, exist_ok=True)`: This call creates the specified output directory if it doesn't exist already and ensures no error is raised when attempting to create a directory within an existing parent chain. It prepares the target location for saving generated files without interrupting other processes in the code pipeline.\n\n3. `write_file(instruct_list, output_subdir / f\"{base_name}.instruct.json\")`: The function writes the 'instruct_list' dataset into JSON format at the designated location (`output_subdir`) appended with \"{base_name}.instruct.json\". It saves the combined instruction data from multiple JSON files processed earlier in a structured manner for further analysis or training purposes.\n\n4. `write_file(file_details, output_subdir / f\"{base_name}.details.yaml\")`: This call writes Python file details extracted as YAML format into the same output directory appended with \"{base_name}.details.yaml\". It preserves metadata related to the processed script for documentation or reference purposes.\n\n5. `create_code_graph(file_details, base_name, output_subdir)`: Generate graphical representations of Python code structure based on input `file_details`. The visualization aids in understanding program flow and dependencies between different parts of the script. It saves these graphs as PNG images within the output subdirectory specified by `output_subdir`.\n\n6. `logging.error(f\"Error creating graph for {base_name}: {e}\", exc_info=True)`: In case an error occurs during code graph creation, this logging statement records the incident with relevant details (error message and traceback). It ensures developers can identify issues while debugging complex tasks like graph generation."
    },
    {
        "instruction": "Variables in `save_python_data` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef save_python_data(file_details: dict, instruct_list: list, relative_path: Path, output_dir: str) -> None:\n    \"\"\"\n    Save the details of the Python file as a YAML file, the instruction data as JSON files,\n    and generate and save code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path of the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parent\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    base_name = relative_path.name\n    write_file(instruct_list, output_subdir / f'{base_name}.instruct.json')\n    write_file(file_details, output_subdir / f'{base_name}.details.yaml')\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except Exception as e:\n        logging.error(f'Error creating graph for {base_name}: {e}', exc_info=True)\n```",
        "output": "'relative_path', 'instruct_list', 'output_subdir', 'base_name', 'file_details', 'output_dir'\n\nPurpose and Significance:\nIn the `save_python_data` function within the py2dataset's save_output.py module, several variables play crucial roles to facilitate handling Python file data along with its associated instruction set conversion into suitable formats for further processing or documentation purposes. These variables are listed below with their respective descriptions:\n\n1) relative_path (Path): Represents the path location of a specific Python script relatively positioned within an output directory structure. It assists in extracting information about this particular file while preparing it for storing data in required formats and generating graphs if necessary.\n2) instruct_list (List): Contains the extracted instruction dataset from the processed Python script, which is later saved as JSON files ('instruct.json'). This list holds valuable insights into how the code should be interpreted or utilized during training models or documentation generation tasks.\n3) output_subdir (Path): Represents a subdirectory within the main output directory where generated images and other artifacts will be stored after executing `create_code_graph`. It ensures well-organized saving mechanisms across operations within `save_python_data` calls.\n4) base_name (str): Refers to a common naming scheme applied for creating various file names originating from the processed Python script's information in specified output formats. These include '.details.yaml', '.instruct.json', and generated graphs ('*.png'). Base_name serves as an identifier within this diverse array of output files, enabling better identification and organization.\n5) file_details (dict): Captures details extracted from the Python script under analysis. This dictionary stores essential metadata required for generating code graphs using `create_code_graph`. It contains critical information about the Python file's structure and dependencies needed to visualize its internal relationships as directed graphs.\n6) output_dir (str): Represents the main directory where all outputs from `save_python_data` will be saved, including YAML files ('details.yaml'), JSON datasets ('instruct.json'), and graph images in subdirectories like 'output_subdir'. It helps maintain a systematic organization of generated artifacts throughout the script execution.'''"
    },
    {
        "instruction": "Inputs to `write_file` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file.\n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n```",
        "output": "'data', 'file_path'\n\nPurpose and Significance:\nThe `write_file` function inside py2dataset's save_output script aims to effectively save stored information either in JSON or YAML formats onto specific user-designated file locations specified via the supplied inputs ('data', 'file_path'). More particularly:\n\n1. 'data': Represents the dictionary content that needs serialization into the desired format before being written onto a file. This could be any structured data retrieved from diverse sources or generated during processing within the py2dataset workflow. The function's responsibility is to convert this Python dictionary object into either JSON or YAML representation based on the suffix of 'file_path'.\n\n2. 'file_path': Denotes the target file path where the serialized data ('data') will be stored. It serves as a reference to locate the output destination in the filesystem, allowing users to manage storage locations according to their needs. This parameter ensures accurate writing into either JSON or YAML files depending on its suffix ('json'/'yaml').\n\nBoth inputs collectively enable efficient data persistence and organization within the py2dataset toolchain by enabling users to write structured information in a desired format at customized locations.'''"
    },
    {
        "instruction": "Docstring of `write_file` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file.\n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n```",
        "output": "Writes a dictionary to a JSON or YAML file.\n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None"
    },
    {
        "instruction": "Calls made in `write_file` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file.\n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n```",
        "output": "'file_path.open', 'json.dump', 'yaml.dump'\n\nPurpose and Significance:\nIn the `write_file` function within the 'py2dataset\\save_output.py' module, three distinct operations are carried out to save a given dictionary data ('data') into either JSON or YAML format based on the file extension at the specified path ('file_path'). These key operations include:\n\n1. `file_path.open()` - This call opens the output file with read-write access for writing purposes. It allows reading/writing content depending on further function flow (JSON or YAML format). Pathlib's 'Path' object ('file_path') simplifies handling filesystem paths and operations across different operating systems.\n\n2. `json.dump()` - When the file extension suggests JSON format (identified by '.json'), this built-in Python library function is invoked to serialize ('data') into a human-readable format with an indentation level of 4 spaces for improved readability. This ensures easier inspection and comprehension when editing or debugging JSON files manually if needed.\n\n3. `yaml.SafeDumper.dump()` - If the file extension indicates YAML format (identified by '.yaml'), 'yaml' library's SafeDumper object is utilized to serialize ('data') into a YAML format with 'SortKeys=False', avoiding ordering modifications that may impact downstream tool interpretations which might rely on the exact structure.\n\nCombining these calls, `write_file` effectively saves Python data objects into suitable persistence formats catering to users' varying needs, be it JSON or YAML file formats with a readable appearance upon inspection and manipulation if required.'"
    },
    {
        "instruction": "Variables in `write_file` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file.\n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n```",
        "output": "'file_type', 'data', 'file_path'\n\nPurpose and Significance:\nIn the `write_file` function within the py2dataset's save_output module, three critical variables - 'file_type', 'data', and 'file_path' hold vital information crucial for handling input/output operations on JSON or YAML data files. These parameters play significant roles in determining how to serialize and store data into desired formats accordingly.\n\n1) file_type: This variable captures the suffix of the given `file_path`, denoting whether it corresponds to a JSON or YAML file ('json'/'yaml'). It acts as a switch condition allowing appropriate libraries (json/'yaml') for reading and writing data based on file types. The identifier enables dynamic behavior for serializing complex structures, ensuring compatibility with either format specified in input arguments.\n\n2) data: The 'data' parameter refers to the dictionary containing content to be saved into an output file. It serves as the actual data source which will get written after reading or derived during operations within the script. This variable can hold JSON or YAML data structures, making it versatile for diverse use cases in handling input files of different formats.\n\n3) file_path: A `Path` object representing the path to the input file, providing contextual information about its location within the system. During writing operations, this helps in identifying the correct output format ('file_type') and directing data to be stored at the specified location through open() function calls for writing access ('with file_path.open()'). Thus,'file_path' contributes significantly to precise data management while preserving its provenance in serialized formats.'''"
    },
    {
        "instruction": "Inputs to `read_file` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        if file_type == 'yaml':\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}\n```",
        "output": "'file_path'\n\nPurpose and Significance:\nIn the context of 'py2dataset\\save_output.py', the variable `file_path` serves as a crucial input parameter within the `read_file` function. It represents the path towards an incoming JSON or YAML file containing structured data. Its significance lies in identifying the source location for retrieving the file content required for conversion to dictionary format in the form of usable information processed later across several tasks throughout this module.\nWithin the `read_file()` function execution flow, `file_path` influences its core activity as follows:\n1. Determining the file type by extracting its suffix ('file_type') using `file_path[1:]`. This step helps identify whether the input is a JSON or YAML file based on its extension (either '.json' or '.yaml').\n2. Utilizing the `Path` class method to open and read data from the given location using `file_path.open()`. It facilitates seamless handling of diverse input files as both formats use distinct Python libraries - `json` for JSON files and `yaml` for YAML files. Thus, ensuring compatibility with different storage structures while maintaining consistency in returning data as a dictionary object irrespective of the file format.\nIn summary, `file_path` is an essential argument providing input accessibility to `read_file()`, enabling extraction of structured data from JSON or YAML sources for further processing within the py2dataset toolchain."
    },
    {
        "instruction": "Docstring of `read_file` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        if file_type == 'yaml':\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}\n```",
        "output": "Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary."
    },
    {
        "instruction": "Calls made in `read_file` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        if file_type == 'yaml':\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}\n```",
        "output": "'file_path.open', 'json.load', 'yaml.load'\n\nPurpose and Significance:\nIn the `read_file` function within the py2dataset utility module, three crucial operations are performed to read JSON or YAML files and return their contents as a dictionary format. These operations involve interacting with file streams using `open()`, leveraging built-in Python libraries `json` and `yaml`.\n\n1. `file_path.open()`: This call opens the specified input file for reading in text mode. It accepts the `file_path` argument which represents the path to a JSON or YAML file within the py2dataset script. Opening the file enables accessing its contents for further processing.\n\n2. `json.load(f)`: After opening the file stream with `file_path.open()`, this call loads the content into memory by parsing it as a JSON object using the `json.load()` function. If the file suffix is \".json\", Python's json library reads and converts the data accordingly into dictionary format before closing the stream (`f`).\n\n3. `yaml.load(f, Loader=yaml.SafeLoader)`: For a file with \".yaml\" suffix, the YAML loader handles parsing the input file content using `yaml.load()`. The `Loader=yaml.SafeLoader` argument ensures safe loading by ignoring aliases during the process. Similar to JSON handling, it converts the data into dictionary format before closing the stream (`f`).\n\nTogether, these operations enable `read_file()` to read input files seamlessly in their respective formats and return the contents as a unified dictionary structure regardless of whether they are JSON or YAML. This functionality forms the basis for flexible data processing throughout the py2dataset framework.''"
    },
    {
        "instruction": "Variables in `read_file` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        if file_type == 'yaml':\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}\n```",
        "output": "'file_type', 'file_path'\n\nPurpose and Significance:\nIn the `read_file` function within the py2dataset's save_output.py module, the variables '_file_type'_ and '_file_path'_ serve essential purposes as part of handling JSON or YAML data files manipulation operations.\n\n1) '_file_type'_ holds information on a particular file's extension extracted through removing the suffix ('[1:]') from '_file_path'_'s suffix. This allows identifying whether the input file is in JSON or YAML format for appropriate data processing using either json or yaml libraries respectively.\n2) '_file_path'_ represents the actual path to the input file that needs reading operations. It acts as an argument passed into `read_file` function which opens and reads its contents efficiently based on the determined file type ('_file_type'). This enables processing data from various formats while maintaining consistency throughout the script execution.\n\nTogether, these variables facilitate seamless interaction with different file types (JSON or YAML) ensuring appropriate data loading into a dictionary format for further processing in the py2dataset toolchain. Their roles contribute to efficient handling of diverse input files while keeping code modularity and flexibility intact. ''"
    },
    {
        "instruction": "Returns from `read_file` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        if file_type == 'yaml':\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}\n```",
        "output": "'{}', 'json.load(f)', 'yaml.load(f', 'Loader=yaml.SafeLoader)'\n\nPurpose and Significance:\nThe 'read_file' function within the py2dataset framework serves a crucial role in handling input data processing by reading JSON or YAML files efficiently. It returns essential contents after identifying their respective formats based on file extensions ('json'/'yaml'). Its primary responsibilities include accepting a `file_path` argument representing the location of the input file and decoding its content into Python dictionaries for further manipulation within the script. Two significant operations are performed using built-in libraries:\n1. 'json.load(f)' reads JSON files by loading their contents from opened streams provided via a path referred as 'file_path', extracting essential details structured under Python's default data dictionary schema (dictionaries in case of JSON files). This streamlines JSON decoding procedure facilitated with standard `open` functionalities and json library methods.\n2. 'yaml.load(f', 'Loader=yaml.SafeLoader)' handles YAML files similarly but uses the yaml library alongside a custom Loader ('yaml.SafeLoader') to ensure safe loading of user-generated data without falling prey to unexpected references or aliases. This loader guarantees secure parsing by default, ensuring proper file handling even with potentially unsafe content. Both JSON and YAML return appropriate representations facilitating subsequent actions such as data modifications and writing in a human readable manner as initially structured.'{}'"
    },
    {
        "instruction": "Inputs to `preserve_spacing` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef preserve_spacing(text: str, tab_width: int=4) -> str:\n    \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n    return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n```",
        "output": "'text', 'tab_width'\n\nPurpose and Significance:\nThe `preserve_spacing` function within the `convert_json_to_html` method serves a crucial role in maintaining original whitespaces and tabs when converting JSON files into HTML format during documentation generation. It ensures proper formatting is preserved while replacing spaces with non-breaking space entities ('&nbsp;') and converting tabs into sequences of specified width ('tab_width'). This helps retain the structure and readability of input fields in the HTML representation, ensuring accurate translation from JSON data to web presentation format.\n\nIn detail:\n1) 'text': Represents the input string containing spaces or tabs that require preservation during conversion to HTML format. It can be any text extracted from a JSON file while processing each row of table cells within HTML content generation.\n2) 'tab_width': A numerical parameter defining the width of tab equivalents when replacing multiple spaces with `&nbsp;` sequences in `preserve_spacing`. By default set to 4, it signifies how many characters represent a single tab after conversion, thus influencing spacing accuracy during formatting adjustments.\nCombining their applications helps avoid layout mismatch while showing tables originally arranged based on indention styles carried forward into web HTML renderings. Their interaction secures original semantic alignments through well-kept code structures amid transition into markup language without breaking layout integrity.'''"
    },
    {
        "instruction": "Docstring of `preserve_spacing` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef preserve_spacing(text: str, tab_width: int=4) -> str:\n    \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n    return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n```",
        "output": "Preserve spaces and tabs in the provided text."
    },
    {
        "instruction": "Calls made in `preserve_spacing` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef preserve_spacing(text: str, tab_width: int=4) -> str:\n    \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n    return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n```",
        "output": "'text.replace(', '&nbsp;').replace', 'text.replace'\n\nPurpose and Significance:\nThe `preserve_spacing` function within the `convert_json_to_html` method serves a crucial role in maintaining whitespaces and tabs while converting JSON files into HTML format during the table rendering process. Its functionality involves substituting specific characters in a given string to preserve spaces visually similar in nature as in original JSON files for seamless display when transferred onto an HTML webpage layout. Two substitution actions are performed:\n\n1. Replacing standard spaces (' ') with non-breaking space entities ('&nbsp;'): This ensures text does not break across lines unintentionally during rendering, preserving the original whitespace appearance.\n2. Transforming tabs ('\\t') into sequences of '&nbsp;' characters with a specified width (defaulted to 4). By doing so, it emulates tab indentation in HTML format, ensuring consistent formatting across multiple columns within tables generated from JSON data. The `preserve_spacing` function supports consistent presentation for readability when moving from JSON/YAML structured data to the web page view in HTML output. As a result, this improves usability and visual clarity of tabular representations created during conversion processes."
    },
    {
        "instruction": "Variables in `preserve_spacing` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef preserve_spacing(text: str, tab_width: int=4) -> str:\n    \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n    return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n```",
        "output": "'text', 'tab_width'\n\nPurpose and Significance:\nThe variables 'text' and 'tab_width' play significant roles within the 'preserve_spacing' utility function defined in py2dataset\\save_output.py module. This function is primarily utilized during conversion of JSON files into HTML format by `convert_json_to_html`, retaining the initial indentations as necessary. Its task centers on reformatting spaces to ensure data portrayal stays close to original appearance within a converted tabular presentation for easier interpretation and improved readability after transfer from code syntax into an HTML file.\n\n1) 'text': Represents a string from which whitespaces should be handled before HTML encoding substitutions. As the JSON file data may consist of formatted strings including tabs or spaces denoting indentations in Python code, it requires special attention to maintain the visualization's semantic equivalence. 'preserve_spacing' modifies these spaces with HTML entities like '&nbsp;' (non-breaking space) for better rendering while retaining original whitespace structure.\n2) 'tab_width': Determines the replacement width of tabs during transformation from Python indentation to HTML presentation. Set as a default value of 4, it defines how many characters should represent each tab in converted HTML output. This variable ensures consistent visualization across different coding styles where developers might use varying numbers of spaces for indentations. By converting tabs into sequences of equal width 'tab_width' spaces, the readability and aesthetics are improved within generated HTML files without disrupting their contextual meaning.'''"
    },
    {
        "instruction": "Returns from `preserve_spacing` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef preserve_spacing(text: str, tab_width: int=4) -> str:\n    \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n    return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n```",
        "output": "'text.replace(', '&nbsp;').replace('\\\\t', '&nbsp;' * tab_width)'\n\nPurpose and Significance:\nThe `preserve_spacing` function within the `convert_json_to_html` method serves a crucial role in maintaining whitespaces and tabs during HTML conversion for JSON files. It ensures proper formatting of input fields while transforming data into web-friendly representations. The function takes two arguments - 'text' as the content string to be modified and an optional keyword argument `tab_width` (default value set to 4). By applying replacement operations on 'text', it converts spaces (' ') to HTML non-breaking space entities ('&nbsp;') and tabs ('\\t') into sequences of '&nbsp;' with specified width ('tab_width'). This operation guarantees consistent visual appearance in web browsers, retaining the original structure and indentation from JSON files when displayed as HTML tables. In essence, `preserve_spacing` helps preserve whitespaces for better readability while converting JSON data into HTML format.'''"
    }
]