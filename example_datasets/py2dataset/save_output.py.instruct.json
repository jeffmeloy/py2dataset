[
    {
        "instruction": "Dependencies in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\n\"\"\"\nUtility functions for reading the input and saving the output of the py2dataset script.\nRequirements:\n[req01] The `read_file` function shall:\n        a. Accept a file path as an argument.\n        b. Read and return the contents of a JSON or YAML file as a dictionary.\n[req02] The `write_file` function shall:\n        a. Accept a dictionary and a file path as arguments.\n        b. Write the dictionary to a file in either JSON or YAML format.\n[req03] The `convert_json_to_html` function shall:\n        a. Convert JSON files within a given directory to HTML format.\n        b. Save each converted file with a .html extension.\n        c. Preserve spacing and tabs for the 'input' field.\n[req04] The `combine_json_files` function shall:\n        a. Accept a directory path as an argument.\n        b. Merge all JSON files in the directory.\n        c. Remove duplicates from the combined JSON files.\n        d. Write the combined data to 'instruct.json' files.\n        e. Convert the merged JSON files to HTML format.\n        f. Return the 'instruct_list' datasets.\n[req05] The `create_code_graph` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Generate code graphs based on the provided file details.\n        c. Save the graphs as PNG images in the specified output directory.\n[req06] The `save_python_data` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Save the details of the Python file as a YAML file.\n        c. Save the instruction data as JSON files.\n        d. Generate and save code graphs.\n\"\"\"\nimport json\nimport logging\nfrom html import escape\nfrom pathlib import Path\nfrom typing import Dict, List\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport yaml\n\n\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == \"json\":\n            return json.load(f)\n        if file_type == \"yaml\":\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}\n\n\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file.\n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open(\"w\") as f:\n        if file_type == \"json\":\n            json.dump(data, f, indent=4)\n        elif file_type == \"yaml\":\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n\n\ndef convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format and save .html file.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int = 4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(\" \", \"&nbsp;\").replace(\"\\t\", \"&nbsp;\" * tab_width)\n\n    for json_file in Path(directory).rglob(\"*.json\"):\n        try:\n            dataset = read_file(json_file)\n            if not dataset:\n                continue\n        except Exception:\n            continue\n\n        html_content = \"\"\"\n        <html>\n        <head>\n            <style>\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\n                th, td {\n                    border: 1px solid black;\n                    padding: 8px;\n                    text-align: left;\n                    white-space: pre-line;\n                    vertical-align: top;\n                    word-wrap: break-word;\n                }\n            </style>\n        </head>\n        <body>\n            <table>\n                <thead>\n                    <tr>\n        \"\"\"\n        column_count = len(dataset[0].keys())\n        column_width = round(100 / column_count, 2)\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += \"\"\"\n                    </tr>\n                </thead>\n                <tbody>\n        \"\"\"\n        html_rows = []\n        for entry in dataset:\n            row_parts = [\"<tr>\"]\n            for key in entry:\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace(\"\\n\", \"<br/>\")\n                row_parts.append(f\"<td>{value}</td>\")\n            row_parts.append(\"</tr>\")\n            html_rows.append(\"\".join(row_parts))\n        html_content += \"\".join(html_rows)\n\n        html_content += \"\"\"\n                </tbody>\n            </table>\n        </body>\n        </html>\n        \"\"\"\n        html_file_path = json_file.with_suffix(\".html\")\n        try:\n            with open(html_file_path, \"w\", encoding=\"utf-8\") as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f\"Failed saving: {html_file_path}\")\n\n\ndef combine_json_files(\n    directory: str, html: bool, questions: Dict\n) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and\n    then remove duplicates. Also generate a 'training.json' file.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n    logging.info(f\"Combining JSON files in {directory}\")\n\n    # Save instruct.json file\n    combined_data, code_filename = [], []\n    skip_files = {\"instruct.json\", \"chat_ml.json\", \"document_code.json\"}\n    for json_file in Path(directory).rglob(\"*.json\"):\n        if json_file.name in skip_files:\n            continue\n        try:\n            file_data = read_file(json_file)\n            if file_data:\n                combined_data.extend(file_data)\n                cleaned_name = (\n                    json_file.relative_to(directory)\n                    .with_suffix(\"\")\n                    .as_posix()\n                    .replace(\".instruct\", \"\")\n                )\n                code_filename.append(cleaned_name)\n        except Exception as e:\n            logging.info(f\"Failed reading: {json_file}. Error: {e}\")\n    write_file(combined_data, Path(directory) / \"instruct.json\")\n\n    # Generate document_code.json file\n    purpose_question = [\n        item[\"text\"] for item in questions if item[\"id\"] == \"file_purpose\"\n    ][0]\n    purpose_question = purpose_question.split(\"{filename}\")[0]\n    purpose_data = [\n        item\n        for item in combined_data\n        if item[\"instruction\"].startswith(purpose_question)\n    ]\n\n    document_code = [\n        {\n            \"document\": item[\"output\"],\n            \"code\": item[\"input\"],\n        }\n        for item in purpose_data\n    ]\n    for i, item in enumerate(document_code):\n        item[\"code filename\"] = code_filename[i]\n\n    write_file(document_code, Path(directory) / \"document_code.json\")\n\n    chat_ml = [\n        {\n            \"conversation\": [\n                {\"from\": \"system\", \"value\": f\"code documentation: {item['document']}\"},\n                {\n                    \"from\": \"human\",\n                    \"value\": \"Output the Python code described by the code documentation.\",\n                },\n                {\"from\": \"gpt\", \"value\": item[\"code\"]},\n            ],\n            \"nbytes\": \"0\",\n            \"source\": item[\"code filename\"],\n        }\n        for item in document_code\n    ]\n\n    # Compute the number of bytes for each conversation and update the nbytes field\n    for item in chat_ml:\n        nbytes = 0\n        for conv in item[\"conversation\"]:\n            nbytes += len(conv[\"value\"].encode(\"utf-8\"))\n        item[\"nbytes\"] = nbytes\n\n    write_file(chat_ml, Path(directory) / \"chat_ml.json\")\n\n    if html:\n        logging.info(\"Converting JSON files to HTML\")\n        convert_json_to_html(directory)\n\n    return {\"instruct_list\": combined_data}\n\n\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    # create graph\n    graph_type = \"entire_code_graph\"\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details[\"file_info\"][graph_type][\"nodes\"])\n    for edge in file_details[\"file_info\"][graph_type][\"edges\"]:\n        source, target = edge[\"source\"], edge[\"target\"]\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(\n                source,\n                target,\n                **{\n                    k: v\n                    for k, v in edge.items()\n                    if k in [\"target_inputs\", \"target_returns\"]\n                },\n            )\n\n    # draw graph\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(\n        G,\n        pos,\n        with_labels=True,\n        font_weight=\"bold\",\n        font_size=8,\n        node_shape=\"s\",\n        node_size=500,\n        width=1,\n        arrowsize=12,\n    )\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if \"target_inputs\" in edge[2] and edge[2][\"target_inputs\"]:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if \"target_returns\" in edge[2] and edge[2][\"target_returns\"]:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[(edge[0], edge[1])] = \"\\n\".join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n\n    # save graph\n    output_file = output_subdir / f\"{base_name}.{graph_type}.png\"\n    plt.savefig(output_file)\n    plt.close()\n\n\ndef save_python_data(\n    file_details: dict, instruct_list: list, relative_path: Path, output_dir: str\n) -> None:\n    \"\"\"\n    Save the details of the Python file as a YAML file, the instruction data as JSON files,\n    and generate and save code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path of the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parent\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    base_name = relative_path.name\n    write_file(instruct_list, output_subdir / f\"{base_name}.instruct.json\")\n    write_file(file_details, output_subdir / f\"{base_name}.details.yaml\")\n\n    # Generating and saving the code graph\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except Exception as e:\n        logging.error(f\"Error creating graph for {base_name}: {e}\", exc_info=True)\n\n```",
        "output": "'yaml', 'networkx', 'logging', 'html', 'json', 'typing', 'matplotlib.pyplot', 'pathlib'\n\nPurpose and Significance:\n1. `yaml` - PyYAML library facilitates reading and writing YAML files within the script. It enables loading YAML data securely with Loader=SafeLoader option for avoiding alias issues ('yaml.load') while saving YAML data in a structured manner using SafeDumper (`write_file()`, `combine_json_files()`).\n\n2. `networkx` - A Python library focused on creating graphs and networks, employed here to build graph representations from extracted file details ('create_code_graph'). NetworkX supports creating graph layouts through methods like DiGraph object construction and edge connections adding essential metadata during node/edge processing (`G`, `nx.add_edges()`) prior to rendering into an image using Matplotlib (Plotly in this case).\n\n3. `logging` - A standard Python logging library applied to trace exceptions raised across diverse functions such as `create_code_graph()`, handling failures or errors efficiently throughout script execution while offering crucial details on where and when issues occur. This enables easier debugging.\n\n4. `html` - While not directly imported as a dependency, HTML serves as the target output format in converting JSON files into tabular text format using HTML tags and CSS styles ('convert_json_to_html()') for better readability. It also determines whether to convert JSON files to HTML after combining them ('combine_json_files()').\n\n5. `json` - Python's standard library handling JSON data manipulation is utilized in reading ('read_file()'), writing ('write_file()'), and generating HTML tables from JSON content ('convert_json_to_html()') with json module functions like 'load()', 'dump()'. It also helps in merging multiple JSON files into a single instruct.json file ('combine_json_files()').\n\n6. `typing` - A Python module offering type hints and annotations, although not explicitly used within the given code snippet, its presence implies type checking for improved readability and clarity in function signatures (e.g., `def convert_json_to_html(directory: str) -> None`, `def save_python_data(file_details: dict, instruct_list: list, relative_path: Path, output_dir: str) -> None`).\n\n7. `matplotlib.pyplot` - A Python 2D plotting library used to create graphical representations of code dependencies ('create_code_graph()') by generating graphs from file details and saving them as '.png' images in specified subdirectories. It includes functions like 'plt.figure', 'nx.draw', 'plt.savefig'.\n\n8. `pathlib` - A Python standard library providing Path class for handling filesystem paths ('Path') which simplifies operations on file paths (e.g., `directory = Path(root)`, `json_file = Path(directory).rglob(\"*.json\")`) and directory management ('output_subdir.mkdir()'). It ensures cross-platform compatibility in accessing files with consistent syntax across various operating systems."
    },
    {
        "instruction": "Call code graph in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\n\"\"\"\nUtility functions for reading the input and saving the output of the py2dataset script.\nRequirements:\n[req01] The `read_file` function shall:\n        a. Accept a file path as an argument.\n        b. Read and return the contents of a JSON or YAML file as a dictionary.\n[req02] The `write_file` function shall:\n        a. Accept a dictionary and a file path as arguments.\n        b. Write the dictionary to a file in either JSON or YAML format.\n[req03] The `convert_json_to_html` function shall:\n        a. Convert JSON files within a given directory to HTML format.\n        b. Save each converted file with a .html extension.\n        c. Preserve spacing and tabs for the 'input' field.\n[req04] The `combine_json_files` function shall:\n        a. Accept a directory path as an argument.\n        b. Merge all JSON files in the directory.\n        c. Remove duplicates from the combined JSON files.\n        d. Write the combined data to 'instruct.json' files.\n        e. Convert the merged JSON files to HTML format.\n        f. Return the 'instruct_list' datasets.\n[req05] The `create_code_graph` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Generate code graphs based on the provided file details.\n        c. Save the graphs as PNG images in the specified output directory.\n[req06] The `save_python_data` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Save the details of the Python file as a YAML file.\n        c. Save the instruction data as JSON files.\n        d. Generate and save code graphs.\n\"\"\"\nimport json\nimport logging\nfrom html import escape\nfrom pathlib import Path\nfrom typing import Dict, List\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport yaml\n\n\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == \"json\":\n            return json.load(f)\n        if file_type == \"yaml\":\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}\n\n\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file.\n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open(\"w\") as f:\n        if file_type == \"json\":\n            json.dump(data, f, indent=4)\n        elif file_type == \"yaml\":\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n\n\ndef convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format and save .html file.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int = 4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(\" \", \"&nbsp;\").replace(\"\\t\", \"&nbsp;\" * tab_width)\n\n    for json_file in Path(directory).rglob(\"*.json\"):\n        try:\n            dataset = read_file(json_file)\n            if not dataset:\n                continue\n        except Exception:\n            continue\n\n        html_content = \"\"\"\n        <html>\n        <head>\n            <style>\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\n                th, td {\n                    border: 1px solid black;\n                    padding: 8px;\n                    text-align: left;\n                    white-space: pre-line;\n                    vertical-align: top;\n                    word-wrap: break-word;\n                }\n            </style>\n        </head>\n        <body>\n            <table>\n                <thead>\n                    <tr>\n        \"\"\"\n        column_count = len(dataset[0].keys())\n        column_width = round(100 / column_count, 2)\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += \"\"\"\n                    </tr>\n                </thead>\n                <tbody>\n        \"\"\"\n        html_rows = []\n        for entry in dataset:\n            row_parts = [\"<tr>\"]\n            for key in entry:\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace(\"\\n\", \"<br/>\")\n                row_parts.append(f\"<td>{value}</td>\")\n            row_parts.append(\"</tr>\")\n            html_rows.append(\"\".join(row_parts))\n        html_content += \"\".join(html_rows)\n\n        html_content += \"\"\"\n                </tbody>\n            </table>\n        </body>\n        </html>\n        \"\"\"\n        html_file_path = json_file.with_suffix(\".html\")\n        try:\n            with open(html_file_path, \"w\", encoding=\"utf-8\") as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f\"Failed saving: {html_file_path}\")\n\n\ndef combine_json_files(\n    directory: str, html: bool, questions: Dict\n) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and\n    then remove duplicates. Also generate a 'training.json' file.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n    logging.info(f\"Combining JSON files in {directory}\")\n\n    # Save instruct.json file\n    combined_data, code_filename = [], []\n    skip_files = {\"instruct.json\", \"chat_ml.json\", \"document_code.json\"}\n    for json_file in Path(directory).rglob(\"*.json\"):\n        if json_file.name in skip_files:\n            continue\n        try:\n            file_data = read_file(json_file)\n            if file_data:\n                combined_data.extend(file_data)\n                cleaned_name = (\n                    json_file.relative_to(directory)\n                    .with_suffix(\"\")\n                    .as_posix()\n                    .replace(\".instruct\", \"\")\n                )\n                code_filename.append(cleaned_name)\n        except Exception as e:\n            logging.info(f\"Failed reading: {json_file}. Error: {e}\")\n    write_file(combined_data, Path(directory) / \"instruct.json\")\n\n    # Generate document_code.json file\n    purpose_question = [\n        item[\"text\"] for item in questions if item[\"id\"] == \"file_purpose\"\n    ][0]\n    purpose_question = purpose_question.split(\"{filename}\")[0]\n    purpose_data = [\n        item\n        for item in combined_data\n        if item[\"instruction\"].startswith(purpose_question)\n    ]\n\n    document_code = [\n        {\n            \"document\": item[\"output\"],\n            \"code\": item[\"input\"],\n        }\n        for item in purpose_data\n    ]\n    for i, item in enumerate(document_code):\n        item[\"code filename\"] = code_filename[i]\n\n    write_file(document_code, Path(directory) / \"document_code.json\")\n\n    chat_ml = [\n        {\n            \"conversation\": [\n                {\"from\": \"system\", \"value\": f\"code documentation: {item['document']}\"},\n                {\n                    \"from\": \"human\",\n                    \"value\": \"Output the Python code described by the code documentation.\",\n                },\n                {\"from\": \"gpt\", \"value\": item[\"code\"]},\n            ],\n            \"nbytes\": \"0\",\n            \"source\": item[\"code filename\"],\n        }\n        for item in document_code\n    ]\n\n    # Compute the number of bytes for each conversation and update the nbytes field\n    for item in chat_ml:\n        nbytes = 0\n        for conv in item[\"conversation\"]:\n            nbytes += len(conv[\"value\"].encode(\"utf-8\"))\n        item[\"nbytes\"] = nbytes\n\n    write_file(chat_ml, Path(directory) / \"chat_ml.json\")\n\n    if html:\n        logging.info(\"Converting JSON files to HTML\")\n        convert_json_to_html(directory)\n\n    return {\"instruct_list\": combined_data}\n\n\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    # create graph\n    graph_type = \"entire_code_graph\"\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details[\"file_info\"][graph_type][\"nodes\"])\n    for edge in file_details[\"file_info\"][graph_type][\"edges\"]:\n        source, target = edge[\"source\"], edge[\"target\"]\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(\n                source,\n                target,\n                **{\n                    k: v\n                    for k, v in edge.items()\n                    if k in [\"target_inputs\", \"target_returns\"]\n                },\n            )\n\n    # draw graph\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(\n        G,\n        pos,\n        with_labels=True,\n        font_weight=\"bold\",\n        font_size=8,\n        node_shape=\"s\",\n        node_size=500,\n        width=1,\n        arrowsize=12,\n    )\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if \"target_inputs\" in edge[2] and edge[2][\"target_inputs\"]:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if \"target_returns\" in edge[2] and edge[2][\"target_returns\"]:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[(edge[0], edge[1])] = \"\\n\".join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n\n    # save graph\n    output_file = output_subdir / f\"{base_name}.{graph_type}.png\"\n    plt.savefig(output_file)\n    plt.close()\n\n\ndef save_python_data(\n    file_details: dict, instruct_list: list, relative_path: Path, output_dir: str\n) -> None:\n    \"\"\"\n    Save the details of the Python file as a YAML file, the instruction data as JSON files,\n    and generate and save code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path of the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parent\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    base_name = relative_path.name\n    write_file(instruct_list, output_subdir / f\"{base_name}.instruct.json\")\n    write_file(file_details, output_subdir / f\"{base_name}.details.yaml\")\n\n    # Generating and saving the code graph\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except Exception as e:\n        logging.error(f\"Error creating graph for {base_name}: {e}\", exc_info=True)\n\n```",
        "output": "{'nodes': ['read_file', 'write_file', 'convert_json_to_html', 'preserve_spacing', 'combine_json_files', 'create_code_graph', 'save_python_data', 'file_path.open', 'json.load', 'yaml.load', 'json.dump', 'yaml.dump', \"text.replace(' ', '&nbsp;').replace\", 'text.replace', 'Path(directory).rglob', 'Path', 'len', 'dataset[0].keys', 'round', 'escape', 'str', 'value.replace', 'row_parts.append', 'html_rows.append', \"''.join\", 'json_file.with_suffix', 'open', 'file.write', 'logging.info', 'combined_data.extend', \"json_file.relative_to(directory).with_suffix('').as_posix().replace\", \"json_file.relative_to(directory).with_suffix('').as_posix\", 'json_file.relative_to(directory).with_suffix', 'json_file.relative_to', 'code_filename.append', 'purpose_question.split', \"item['instruction'].startswith\", 'enumerate', \"conv['value'].encode\", 'nx.DiGraph', 'G.add_nodes_from', 'G.add_edge', 'edge.items', 'plt.figure', 'nx.spring_layout', 'nx.draw', 'G.edges', 'label.append', \"', '.join\", \"'\\\\n'.join\", 'nx.draw_networkx_edge_labels', 'plt.savefig', 'plt.close', 'output_subdir.mkdir', 'logging.error'], 'edges': [{'source': 'read_file', 'target': 'file_path.open', 'target_inputs': []}, {'source': 'read_file', 'target': 'json.load', 'target_inputs': ['f']}, {'source': 'read_file', 'target': 'yaml.load', 'target_inputs': ['f']}, {'source': 'write_file', 'target': 'file_path.open', 'target_inputs': [\"'w'\"]}, {'source': 'write_file', 'target': 'json.dump', 'target_inputs': ['data', 'f']}, {'source': 'write_file', 'target': 'yaml.dump', 'target_inputs': ['data', 'f']}, {'source': 'convert_json_to_html', 'target': \"text.replace(' ', '&nbsp;').replace\", 'target_inputs': [\"'\\\\t'\", \"'&nbsp;' * tab_width\"]}, {'source': 'convert_json_to_html', 'target': 'text.replace', 'target_inputs': [\"' '\", \"'&nbsp;'\"]}, {'source': 'convert_json_to_html', 'target': 'Path(directory).rglob', 'target_inputs': [\"'*.json'\"]}, {'source': 'convert_json_to_html', 'target': 'Path', 'target_inputs': ['directory']}, {'source': 'convert_json_to_html', 'target': 'read_file', 'target_inputs': ['json_file'], 'target_returns': ['{}', 'yaml.load(f, Loader=yaml.SafeLoader)', 'json.load(f)']}, {'source': 'convert_json_to_html', 'target': 'len', 'target_inputs': ['dataset[0].keys()']}, {'source': 'convert_json_to_html', 'target': 'dataset[0].keys', 'target_inputs': []}, {'source': 'convert_json_to_html', 'target': 'round', 'target_inputs': ['100 / column_count', '2']}, {'source': 'convert_json_to_html', 'target': 'escape', 'target_inputs': ['str(entry[key])']}, {'source': 'convert_json_to_html', 'target': 'str', 'target_inputs': ['entry[key]']}, {'source': 'convert_json_to_html', 'target': 'preserve_spacing', 'target_inputs': ['value'], 'target_returns': [\"text.replace(' ', '&nbsp;').replace('\\\\t', '&nbsp;' * tab_width)\"]}, {'source': 'convert_json_to_html', 'target': 'value.replace', 'target_inputs': [\"'\\\\n'\", \"'<br/>'\"]}, {'source': 'convert_json_to_html', 'target': 'row_parts.append', 'target_inputs': [\"f'<td>{value}</td>'\", \"'</tr>'\"]}, {'source': 'convert_json_to_html', 'target': 'html_rows.append', 'target_inputs': [\"''.join(row_parts)\"]}, {'source': 'convert_json_to_html', 'target': \"''.join\", 'target_inputs': ['row_parts', 'html_rows']}, {'source': 'convert_json_to_html', 'target': 'json_file.with_suffix', 'target_inputs': [\"'.html'\"]}, {'source': 'convert_json_to_html', 'target': 'open', 'target_inputs': ['html_file_path', \"'w'\"]}, {'source': 'convert_json_to_html', 'target': 'file.write', 'target_inputs': ['html_content']}, {'source': 'convert_json_to_html', 'target': 'logging.info', 'target_inputs': [\"f'Failed saving: {html_file_path}'\"]}, {'source': 'preserve_spacing', 'target': \"text.replace(' ', '&nbsp;').replace\", 'target_inputs': [\"'\\\\t'\", \"'&nbsp;' * tab_width\"]}, {'source': 'preserve_spacing', 'target': 'text.replace', 'target_inputs': [\"' '\", \"'&nbsp;'\"]}, {'source': 'combine_json_files', 'target': 'logging.info', 'target_inputs': [\"f'Combining JSON files in {directory}'\", \"f'Failed reading: {json_file}. Error: {e}'\", \"'Converting JSON files to HTML'\"]}, {'source': 'combine_json_files', 'target': 'Path(directory).rglob', 'target_inputs': [\"'*.json'\"]}, {'source': 'combine_json_files', 'target': 'Path', 'target_inputs': ['directory', 'directory', 'directory', 'directory']}, {'source': 'combine_json_files', 'target': 'read_file', 'target_inputs': ['json_file'], 'target_returns': ['{}', 'yaml.load(f, Loader=yaml.SafeLoader)', 'json.load(f)']}, {'source': 'combine_json_files', 'target': 'combined_data.extend', 'target_inputs': ['file_data']}, {'source': 'combine_json_files', 'target': \"json_file.relative_to(directory).with_suffix('').as_posix().replace\", 'target_inputs': [\"'.instruct'\", \"''\"]}, {'source': 'combine_json_files', 'target': \"json_file.relative_to(directory).with_suffix('').as_posix\", 'target_inputs': []}, {'source': 'combine_json_files', 'target': 'json_file.relative_to(directory).with_suffix', 'target_inputs': [\"''\"]}, {'source': 'combine_json_files', 'target': 'json_file.relative_to', 'target_inputs': ['directory']}, {'source': 'combine_json_files', 'target': 'code_filename.append', 'target_inputs': ['cleaned_name']}, {'source': 'combine_json_files', 'target': 'write_file', 'target_inputs': ['combined_data', \"Path(directory) / 'instruct.json'\", 'document_code', \"Path(directory) / 'document_code.json'\", 'chat_ml', \"Path(directory) / 'chat_ml.json'\"], 'target_returns': []}, {'source': 'combine_json_files', 'target': 'purpose_question.split', 'target_inputs': [\"'{filename}'\"]}, {'source': 'combine_json_files', 'target': \"item['instruction'].startswith\", 'target_inputs': ['purpose_question']}, {'source': 'combine_json_files', 'target': 'enumerate', 'target_inputs': ['document_code']}, {'source': 'combine_json_files', 'target': 'len', 'target_inputs': [\"conv['value'].encode('utf-8')\"]}, {'source': 'combine_json_files', 'target': \"conv['value'].encode\", 'target_inputs': [\"'utf-8'\"]}, {'source': 'combine_json_files', 'target': 'convert_json_to_html', 'target_inputs': ['directory'], 'target_returns': [\"text.replace(' ', '&nbsp;').replace('\\\\t', '&nbsp;' * tab_width)\"]}, {'source': 'create_code_graph', 'target': 'nx.DiGraph', 'target_inputs': []}, {'source': 'create_code_graph', 'target': 'G.add_nodes_from', 'target_inputs': [\"file_details['file_info'][graph_type]['nodes']\"]}, {'source': 'create_code_graph', 'target': 'G.add_edge', 'target_inputs': ['source', 'target']}, {'source': 'create_code_graph', 'target': 'edge.items', 'target_inputs': []}, {'source': 'create_code_graph', 'target': 'plt.figure', 'target_inputs': []}, {'source': 'create_code_graph', 'target': 'nx.spring_layout', 'target_inputs': ['G']}, {'source': 'create_code_graph', 'target': 'nx.draw', 'target_inputs': ['G', 'pos']}, {'source': 'create_code_graph', 'target': 'G.edges', 'target_inputs': []}, {'source': 'create_code_graph', 'target': 'label.append', 'target_inputs': ['f\"Inputs: {\\', \\'.join(edge[2][\\'target_inputs\\'])}\"', 'f\"\\\\nReturns: {\\', \\'.join(edge[2][\\'target_returns\\'])}\"']}, {'source': 'create_code_graph', 'target': \"', '.join\", 'target_inputs': [\"edge[2]['target_inputs']\", \"edge[2]['target_returns']\"]}, {'source': 'create_code_graph', 'target': \"'\\\\n'.join\", 'target_inputs': ['label']}, {'source': 'create_code_graph', 'target': 'nx.draw_networkx_edge_labels', 'target_inputs': ['G', 'pos']}, {'source': 'create_code_graph', 'target': 'plt.savefig', 'target_inputs': ['output_file']}, {'source': 'create_code_graph', 'target': 'plt.close', 'target_inputs': []}, {'source': 'save_python_data', 'target': 'Path', 'target_inputs': ['output_dir']}, {'source': 'save_python_data', 'target': 'output_subdir.mkdir', 'target_inputs': []}, {'source': 'save_python_data', 'target': 'write_file', 'target_inputs': ['instruct_list', \"output_subdir / f'{base_name}.instruct.json'\", 'file_details', \"output_subdir / f'{base_name}.details.yaml'\"], 'target_returns': []}, {'source': 'save_python_data', 'target': 'create_code_graph', 'target_inputs': ['file_details', 'base_name', 'output_subdir'], 'target_returns': []}, {'source': 'save_python_data', 'target': 'logging.error', 'target_inputs': [\"f'Error creating graph for {base_name}: {e}'\"]}]}"
    },
    {
        "instruction": "Functions in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\n\"\"\"\nUtility functions for reading the input and saving the output of the py2dataset script.\nRequirements:\n[req01] The `read_file` function shall:\n        a. Accept a file path as an argument.\n        b. Read and return the contents of a JSON or YAML file as a dictionary.\n[req02] The `write_file` function shall:\n        a. Accept a dictionary and a file path as arguments.\n        b. Write the dictionary to a file in either JSON or YAML format.\n[req03] The `convert_json_to_html` function shall:\n        a. Convert JSON files within a given directory to HTML format.\n        b. Save each converted file with a .html extension.\n        c. Preserve spacing and tabs for the 'input' field.\n[req04] The `combine_json_files` function shall:\n        a. Accept a directory path as an argument.\n        b. Merge all JSON files in the directory.\n        c. Remove duplicates from the combined JSON files.\n        d. Write the combined data to 'instruct.json' files.\n        e. Convert the merged JSON files to HTML format.\n        f. Return the 'instruct_list' datasets.\n[req05] The `create_code_graph` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Generate code graphs based on the provided file details.\n        c. Save the graphs as PNG images in the specified output directory.\n[req06] The `save_python_data` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Save the details of the Python file as a YAML file.\n        c. Save the instruction data as JSON files.\n        d. Generate and save code graphs.\n\"\"\"\nimport json\nimport logging\nfrom html import escape\nfrom pathlib import Path\nfrom typing import Dict, List\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport yaml\n\n\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == \"json\":\n            return json.load(f)\n        if file_type == \"yaml\":\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}\n\n\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file.\n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open(\"w\") as f:\n        if file_type == \"json\":\n            json.dump(data, f, indent=4)\n        elif file_type == \"yaml\":\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n\n\ndef convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format and save .html file.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int = 4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(\" \", \"&nbsp;\").replace(\"\\t\", \"&nbsp;\" * tab_width)\n\n    for json_file in Path(directory).rglob(\"*.json\"):\n        try:\n            dataset = read_file(json_file)\n            if not dataset:\n                continue\n        except Exception:\n            continue\n\n        html_content = \"\"\"\n        <html>\n        <head>\n            <style>\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\n                th, td {\n                    border: 1px solid black;\n                    padding: 8px;\n                    text-align: left;\n                    white-space: pre-line;\n                    vertical-align: top;\n                    word-wrap: break-word;\n                }\n            </style>\n        </head>\n        <body>\n            <table>\n                <thead>\n                    <tr>\n        \"\"\"\n        column_count = len(dataset[0].keys())\n        column_width = round(100 / column_count, 2)\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += \"\"\"\n                    </tr>\n                </thead>\n                <tbody>\n        \"\"\"\n        html_rows = []\n        for entry in dataset:\n            row_parts = [\"<tr>\"]\n            for key in entry:\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace(\"\\n\", \"<br/>\")\n                row_parts.append(f\"<td>{value}</td>\")\n            row_parts.append(\"</tr>\")\n            html_rows.append(\"\".join(row_parts))\n        html_content += \"\".join(html_rows)\n\n        html_content += \"\"\"\n                </tbody>\n            </table>\n        </body>\n        </html>\n        \"\"\"\n        html_file_path = json_file.with_suffix(\".html\")\n        try:\n            with open(html_file_path, \"w\", encoding=\"utf-8\") as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f\"Failed saving: {html_file_path}\")\n\n\ndef combine_json_files(\n    directory: str, html: bool, questions: Dict\n) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and\n    then remove duplicates. Also generate a 'training.json' file.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n    logging.info(f\"Combining JSON files in {directory}\")\n\n    # Save instruct.json file\n    combined_data, code_filename = [], []\n    skip_files = {\"instruct.json\", \"chat_ml.json\", \"document_code.json\"}\n    for json_file in Path(directory).rglob(\"*.json\"):\n        if json_file.name in skip_files:\n            continue\n        try:\n            file_data = read_file(json_file)\n            if file_data:\n                combined_data.extend(file_data)\n                cleaned_name = (\n                    json_file.relative_to(directory)\n                    .with_suffix(\"\")\n                    .as_posix()\n                    .replace(\".instruct\", \"\")\n                )\n                code_filename.append(cleaned_name)\n        except Exception as e:\n            logging.info(f\"Failed reading: {json_file}. Error: {e}\")\n    write_file(combined_data, Path(directory) / \"instruct.json\")\n\n    # Generate document_code.json file\n    purpose_question = [\n        item[\"text\"] for item in questions if item[\"id\"] == \"file_purpose\"\n    ][0]\n    purpose_question = purpose_question.split(\"{filename}\")[0]\n    purpose_data = [\n        item\n        for item in combined_data\n        if item[\"instruction\"].startswith(purpose_question)\n    ]\n\n    document_code = [\n        {\n            \"document\": item[\"output\"],\n            \"code\": item[\"input\"],\n        }\n        for item in purpose_data\n    ]\n    for i, item in enumerate(document_code):\n        item[\"code filename\"] = code_filename[i]\n\n    write_file(document_code, Path(directory) / \"document_code.json\")\n\n    chat_ml = [\n        {\n            \"conversation\": [\n                {\"from\": \"system\", \"value\": f\"code documentation: {item['document']}\"},\n                {\n                    \"from\": \"human\",\n                    \"value\": \"Output the Python code described by the code documentation.\",\n                },\n                {\"from\": \"gpt\", \"value\": item[\"code\"]},\n            ],\n            \"nbytes\": \"0\",\n            \"source\": item[\"code filename\"],\n        }\n        for item in document_code\n    ]\n\n    # Compute the number of bytes for each conversation and update the nbytes field\n    for item in chat_ml:\n        nbytes = 0\n        for conv in item[\"conversation\"]:\n            nbytes += len(conv[\"value\"].encode(\"utf-8\"))\n        item[\"nbytes\"] = nbytes\n\n    write_file(chat_ml, Path(directory) / \"chat_ml.json\")\n\n    if html:\n        logging.info(\"Converting JSON files to HTML\")\n        convert_json_to_html(directory)\n\n    return {\"instruct_list\": combined_data}\n\n\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    # create graph\n    graph_type = \"entire_code_graph\"\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details[\"file_info\"][graph_type][\"nodes\"])\n    for edge in file_details[\"file_info\"][graph_type][\"edges\"]:\n        source, target = edge[\"source\"], edge[\"target\"]\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(\n                source,\n                target,\n                **{\n                    k: v\n                    for k, v in edge.items()\n                    if k in [\"target_inputs\", \"target_returns\"]\n                },\n            )\n\n    # draw graph\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(\n        G,\n        pos,\n        with_labels=True,\n        font_weight=\"bold\",\n        font_size=8,\n        node_shape=\"s\",\n        node_size=500,\n        width=1,\n        arrowsize=12,\n    )\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if \"target_inputs\" in edge[2] and edge[2][\"target_inputs\"]:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if \"target_returns\" in edge[2] and edge[2][\"target_returns\"]:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[(edge[0], edge[1])] = \"\\n\".join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n\n    # save graph\n    output_file = output_subdir / f\"{base_name}.{graph_type}.png\"\n    plt.savefig(output_file)\n    plt.close()\n\n\ndef save_python_data(\n    file_details: dict, instruct_list: list, relative_path: Path, output_dir: str\n) -> None:\n    \"\"\"\n    Save the details of the Python file as a YAML file, the instruction data as JSON files,\n    and generate and save code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path of the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parent\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    base_name = relative_path.name\n    write_file(instruct_list, output_subdir / f\"{base_name}.instruct.json\")\n    write_file(file_details, output_subdir / f\"{base_name}.details.yaml\")\n\n    # Generating and saving the code graph\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except Exception as e:\n        logging.error(f\"Error creating graph for {base_name}: {e}\", exc_info=True)\n\n```",
        "output": "'read_file', 'write_file', 'convert_json_to_html', 'preserve_spacing', 'combine_json_files', 'create_code_graph', 'save_python_data'\n\nPurpose and Significance:\n1. read_file(file_path: This function serves as a utility for reading data stored in JSON or YAML files efficiently. It takes a file path argument represented by 'file_path' object of Path class which contains necessary metadata about the file location. Upon opening the file using 'with' statement, it identifies the file type based on its suffix after indexing the second character ('file_type'). The contents are read successfully in corresponding formats: JSON ('json.load') for .json files and YAML ('yaml.load(f)', considering safe loader to maintain safety against malformed YAML data). Finally, it returns the parsed data as a Python dictionary.\n   Purpose: Facilitate easy access to JSON or YAML file contents in structured format.\n\n2. write_file(data, file_path): This function writes given dictionary 'data' into either JSON or YAML files based on their respective suffixes appended to the provided path ('file_path'). It opens the file for writing using 'with' statement and determines the format accordingly ('file_type'), employing 'json.dump()' for JSON files and customized YAML dumping with SafeDumper ignoring aliases ('yaml.SafeDumper') to ensure safe conversion.\n   Purpose: Offer a way to save structured data into desired formats maintaining data integrity in output files.\n\n3. convert_json_to_html(directory): Operating within given directories holding JSON files specified as 'directory', it utilizes another auxiliary function preserve_spacing() for conserving white spaces and tabs within the content during conversion to HTML format. It processes each JSON file by reading their contents ('file_data'), creating HTML representation using <html>, <table> tags along with relevant headers, rows and columns data extracted from datasets. Then it writes converted files with '.html' extension after appending .html suffix to the original file name. Exception handling ensures error logging in case of failure while saving output files.\n   Purpose: Transform JSON data into readable HTML tables maintaining formatting specifications for better human comprehension.\n\n4. preserve_spacing(text, tab_width): This function takes a string input 'text' and an optional integer argument 'tab_width', substituting '\\n' with '&nbsp;' and '\\t' with space sequences equal to 'tab_width'. It returns the transformed text suitable for HTML formatting without disrupting whitespaces.\n   Purpose: Ensure spaces and tabs are preserved while converting JSON data into HTML representation.\n\n5. combine_json_files(directory, html=bool, questions): Combining all JSON files excluding a few skip names ('instruct.json', 'chat_ml.json') in specified directory creates an aggregated dataset ('combined_data'). It generates another list named 'document_code' by filtering purposeful entries from combined data based on specific text criteria ('purpose_question'). After computing byte counts for each conversation entry, it produces a JSON file named 'training.json'. If html=True is provided, it further converts JSON files to HTML format. Finally, returns an object containing the consolidated instruction datasets ('instruct_list') extracted from processed JSON entries.\n   Purpose: Consolidate multiple JSON files into comprehensive datasets and generate additional output formats as per requirement.\n\n6. create_code_graph(file_details, base_name, output_subdir): Utilizing given Python file details ('file_details'), it constructs a graph representation using NetworkX library to visualize code dependencies ('nx.DiGraph'). Edges and nodes are added accordingly based on input data structure ('G.add_nodes_from', 'G.add_edge'). It creates a graphical layout with Matplotlib ('plt.figure', 'nx.spring_layout') followed by saving the output as '.png' image in specified subdirectory ('output_subdir'). Exception handling ensures logging errors during execution.\n   Purpose: Generate visual representations of Python file dependency structure to improve comprehension.\n\n7. save_python_data(file_details, instruct_list, relative_path, output_dir): It receives Python file details ('file_details') alongside instruction data ('instruct_list') from parsed inputs, directing the contents for storage in 'yaml', 'JSON files with distinct file paths according to supplied path names using write_file(). Simultaneously triggers creation of code graphs with optional usage of 'create_code_graph()' while managing exceptions during execution.\n   Purpose: Save Python file data into desired formats alongside graphical outputs for better understanding and analysis.'''"
    },
    {
        "instruction": "1) Describe the overall Purpose and Processing Approach of Python file: `py2dataset\\save_output.py`; 2) Define API Signatures for each applicable Function and Class Method; 3) Derive Functional Requirements to specify the code's behavior and purpose; 4) Explain what each variable does in the code.",
        "input": "```python\n\"\"\"\nUtility functions for reading the input and saving the output of the py2dataset script.\nRequirements:\n[req01] The `read_file` function shall:\n        a. Accept a file path as an argument.\n        b. Read and return the contents of a JSON or YAML file as a dictionary.\n[req02] The `write_file` function shall:\n        a. Accept a dictionary and a file path as arguments.\n        b. Write the dictionary to a file in either JSON or YAML format.\n[req03] The `convert_json_to_html` function shall:\n        a. Convert JSON files within a given directory to HTML format.\n        b. Save each converted file with a .html extension.\n        c. Preserve spacing and tabs for the 'input' field.\n[req04] The `combine_json_files` function shall:\n        a. Accept a directory path as an argument.\n        b. Merge all JSON files in the directory.\n        c. Remove duplicates from the combined JSON files.\n        d. Write the combined data to 'instruct.json' files.\n        e. Convert the merged JSON files to HTML format.\n        f. Return the 'instruct_list' datasets.\n[req05] The `create_code_graph` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Generate code graphs based on the provided file details.\n        c. Save the graphs as PNG images in the specified output directory.\n[req06] The `save_python_data` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Save the details of the Python file as a YAML file.\n        c. Save the instruction data as JSON files.\n        d. Generate and save code graphs.\n\"\"\"\nimport json\nimport logging\nfrom html import escape\nfrom pathlib import Path\nfrom typing import Dict, List\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport yaml\n\n\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == \"json\":\n            return json.load(f)\n        if file_type == \"yaml\":\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}\n\n\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file.\n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open(\"w\") as f:\n        if file_type == \"json\":\n            json.dump(data, f, indent=4)\n        elif file_type == \"yaml\":\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n\n\ndef convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format and save .html file.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int = 4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(\" \", \"&nbsp;\").replace(\"\\t\", \"&nbsp;\" * tab_width)\n\n    for json_file in Path(directory).rglob(\"*.json\"):\n        try:\n            dataset = read_file(json_file)\n            if not dataset:\n                continue\n        except Exception:\n            continue\n\n        html_content = \"\"\"\n        <html>\n        <head>\n            <style>\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\n                th, td {\n                    border: 1px solid black;\n                    padding: 8px;\n                    text-align: left;\n                    white-space: pre-line;\n                    vertical-align: top;\n                    word-wrap: break-word;\n                }\n            </style>\n        </head>\n        <body>\n            <table>\n                <thead>\n                    <tr>\n        \"\"\"\n        column_count = len(dataset[0].keys())\n        column_width = round(100 / column_count, 2)\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += \"\"\"\n                    </tr>\n                </thead>\n                <tbody>\n        \"\"\"\n        html_rows = []\n        for entry in dataset:\n            row_parts = [\"<tr>\"]\n            for key in entry:\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace(\"\\n\", \"<br/>\")\n                row_parts.append(f\"<td>{value}</td>\")\n            row_parts.append(\"</tr>\")\n            html_rows.append(\"\".join(row_parts))\n        html_content += \"\".join(html_rows)\n\n        html_content += \"\"\"\n                </tbody>\n            </table>\n        </body>\n        </html>\n        \"\"\"\n        html_file_path = json_file.with_suffix(\".html\")\n        try:\n            with open(html_file_path, \"w\", encoding=\"utf-8\") as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f\"Failed saving: {html_file_path}\")\n\n\ndef combine_json_files(\n    directory: str, html: bool, questions: Dict\n) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and\n    then remove duplicates. Also generate a 'training.json' file.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n    logging.info(f\"Combining JSON files in {directory}\")\n\n    # Save instruct.json file\n    combined_data, code_filename = [], []\n    skip_files = {\"instruct.json\", \"chat_ml.json\", \"document_code.json\"}\n    for json_file in Path(directory).rglob(\"*.json\"):\n        if json_file.name in skip_files:\n            continue\n        try:\n            file_data = read_file(json_file)\n            if file_data:\n                combined_data.extend(file_data)\n                cleaned_name = (\n                    json_file.relative_to(directory)\n                    .with_suffix(\"\")\n                    .as_posix()\n                    .replace(\".instruct\", \"\")\n                )\n                code_filename.append(cleaned_name)\n        except Exception as e:\n            logging.info(f\"Failed reading: {json_file}. Error: {e}\")\n    write_file(combined_data, Path(directory) / \"instruct.json\")\n\n    # Generate document_code.json file\n    purpose_question = [\n        item[\"text\"] for item in questions if item[\"id\"] == \"file_purpose\"\n    ][0]\n    purpose_question = purpose_question.split(\"{filename}\")[0]\n    purpose_data = [\n        item\n        for item in combined_data\n        if item[\"instruction\"].startswith(purpose_question)\n    ]\n\n    document_code = [\n        {\n            \"document\": item[\"output\"],\n            \"code\": item[\"input\"],\n        }\n        for item in purpose_data\n    ]\n    for i, item in enumerate(document_code):\n        item[\"code filename\"] = code_filename[i]\n\n    write_file(document_code, Path(directory) / \"document_code.json\")\n\n    chat_ml = [\n        {\n            \"conversation\": [\n                {\"from\": \"system\", \"value\": f\"code documentation: {item['document']}\"},\n                {\n                    \"from\": \"human\",\n                    \"value\": \"Output the Python code described by the code documentation.\",\n                },\n                {\"from\": \"gpt\", \"value\": item[\"code\"]},\n            ],\n            \"nbytes\": \"0\",\n            \"source\": item[\"code filename\"],\n        }\n        for item in document_code\n    ]\n\n    # Compute the number of bytes for each conversation and update the nbytes field\n    for item in chat_ml:\n        nbytes = 0\n        for conv in item[\"conversation\"]:\n            nbytes += len(conv[\"value\"].encode(\"utf-8\"))\n        item[\"nbytes\"] = nbytes\n\n    write_file(chat_ml, Path(directory) / \"chat_ml.json\")\n\n    if html:\n        logging.info(\"Converting JSON files to HTML\")\n        convert_json_to_html(directory)\n\n    return {\"instruct_list\": combined_data}\n\n\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    # create graph\n    graph_type = \"entire_code_graph\"\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details[\"file_info\"][graph_type][\"nodes\"])\n    for edge in file_details[\"file_info\"][graph_type][\"edges\"]:\n        source, target = edge[\"source\"], edge[\"target\"]\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(\n                source,\n                target,\n                **{\n                    k: v\n                    for k, v in edge.items()\n                    if k in [\"target_inputs\", \"target_returns\"]\n                },\n            )\n\n    # draw graph\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(\n        G,\n        pos,\n        with_labels=True,\n        font_weight=\"bold\",\n        font_size=8,\n        node_shape=\"s\",\n        node_size=500,\n        width=1,\n        arrowsize=12,\n    )\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if \"target_inputs\" in edge[2] and edge[2][\"target_inputs\"]:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if \"target_returns\" in edge[2] and edge[2][\"target_returns\"]:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[(edge[0], edge[1])] = \"\\n\".join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n\n    # save graph\n    output_file = output_subdir / f\"{base_name}.{graph_type}.png\"\n    plt.savefig(output_file)\n    plt.close()\n\n\ndef save_python_data(\n    file_details: dict, instruct_list: list, relative_path: Path, output_dir: str\n) -> None:\n    \"\"\"\n    Save the details of the Python file as a YAML file, the instruction data as JSON files,\n    and generate and save code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path of the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parent\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    base_name = relative_path.name\n    write_file(instruct_list, output_subdir / f\"{base_name}.instruct.json\")\n    write_file(file_details, output_subdir / f\"{base_name}.details.yaml\")\n\n    # Generating and saving the code graph\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except Exception as e:\n        logging.error(f\"Error creating graph for {base_name}: {e}\", exc_info=True)\n\n```",
        "output": "Overview: The given `py2dataset\\save_output.py` script involves several utility functions focused on processing data files, transforming formats, and generating graphical outputs. These utilities primarily function as data readers, writers, and converters while interacting with Python file details. Main operations include handling JSON/YAML content management through `read_file()` and `write_file()`, translating JSON files to HTML format using `convert_json_to_html()`, consolidating multiple JSON files into 'instruct.json' with duplicate removal via `combine_json_files()`, creating code graphs as PNG images utilizing `create_code_graph()`, and saving Python file data in YAML/JSON formats along with generating graphs through `save_python_data()`.\n\n1) read_file(file_path: This function reads contents from a given JSON or YAML file path, identified by 'file_type' extracted from file suffix ('json', 'yaml'), accepting an instance of 'Path' class (for accessibility with standard libraries), returns respective format contents as Python dictionary ('json.load()' for JSON; 'yaml.load(f)' for YAML).\n2) write_file(data, file_path): Accepting a dictionary and a file path as arguments, it writes data into either JSON or YAML format depending on the suffix of 'file_path'. If JSON ('json.dump(data)', if YAML SafeDumper manages potential aliases disregard ('yaml.SafeDumper'), returning 'None'.\n3) convert_json_to_html(directory): Looping over '.json' files within directory specified ('Path(directory).rglob()'), reads JSON data into 'dataset', generates HTML content preserving spaces and tabs ('preserve_spacing()') in a table format, writes the result as '.html'. Handles exceptions during file saving ('logging.info').\n4) preserve_spacing(text, tab_width): Accepts text input and tab width to replace '\\n', '\\t' with '&nbsp;', maintaining spacing in text for HTML representation (returning transformed string).\n5) combine_json_files(directory, html=bool, questions): Processes all JSON files from the provided directory excluding a few skip-names ('instruct.json', 'chat_ml.json'), composites merged data as 'combined_data'. Afterwards, generates 'document_code.json' with filtered purposeful entries ('purpose_question') and updates 'nbytes' field in each entry by encoding conversation values ('len'). It further converts JSON files to HTML format if 'html=True'. Returns a dictionary containing 'instruct_list' datasets.\n6) create_code_graph(file_details, base_name, output_subdir): Constructs a graph from file details using NetworkX library ('nx.DiGraph'), adds nodes and edges according to input data structure ('G.add_nodes_from', 'G.add_edge'). Creates graphical layout with Matplotlib ('plt.figure', 'nx.spring_layout') then saves it as '.png' in specified output directory ('output_subdir', 'plt.savefig'), managing exceptions along the way ('logging.error').\n7) save_python_data(file_details, instruct_list, relative_path, output_dir): Takes details from Python files ('file_details') as input and extracted instruction list ('instruct_list'). Handles saving as YAML and JSON in a specific subdirectory created from 'output_dir' using write_file() functions. Lastly invokes code graph generation through `create_code_graph()` while logging errors if any occurrence.\n\nFunctional Requirements:\n1) read_file(): Reads JSON or YAML files into Python dictionary format based on file extension.\n2) write_file(): Writes a given dictionary data to JSON or YAML file formats according to suffix.\n3) convert_json_to_html(): Converts JSON files within a directory to HTML format, preserving spaces and tabs in text content.\n4) combine_json_files(): Combines all JSON files into 'instruct.json', eliminates duplicates, generates training data ('document_code'), updates nbytes fields in each conversation entry and optionally converts to HTML format on 'html' input specification returning an object of combined data set (('instruct_list') extracted JSON entries.\n5) create_code_graph(): Visualizes the file graph representation with given details creating .png outputs for graph visualizations with user-specified base name and subdirectory.\n6) save_python_data(): Handles saving Python file data as YAML, generates instruction datasets as JSON files alongside graphical outputs (utilizing `create_code_graph()`) considering exceptions during execution.\n\nVariables:\nread_file(): file_path (Path), file_type, f - temporary file handler, dataset (dict), json_file (Path)\nwrite_file(): data (Dict), file_path (Path), file_type\nconvert_json_to_html(): html_content (str), tab_width (int=4), column_count (float), row_parts (list), column_width (float), html_rows (list), json_file (Path), cleaned_name (str), purpose_question (str), edge_labels (dict)\npreserve_spacing(): text (str), tab_width (int)\ncombine_json_files(): directory (str), combined_data (List[Dict]), code_filename (List[str]), skip_files (Set[str]), purpose_question (str), file_data (List), graph_type (str), chat_ml (List[dict])\ncreate_code_graph(): output_subdir (Path), edge_labels (dict), pos (dict), label (list), G (networkx.DiGraph)\nsave_python_data(): file_details (dict), instruct_list (List[Dict]), relative_path (Path), output_dir (str), output_file (Path), base_name (str), graph_type (str), edge (tuple), plt._current_fig_manager()"
    },
    {
        "instruction": "Inputs to `convert_json_to_html` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format and save .html file.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int=4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n    for json_file in Path(directory).rglob('*.json'):\n        try:\n            dataset = read_file(json_file)\n            if not dataset:\n                continue\n        except Exception:\n            continue\n        html_content = '\\n        <html>\\n        <head>\\n            <style>\\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\\n                th, td {\\n                    border: 1px solid black;\\n                    padding: 8px;\\n                    text-align: left;\\n                    white-space: pre-line;\\n                    vertical-align: top;\\n                    word-wrap: break-word;\\n                }\\n            </style>\\n        </head>\\n        <body>\\n            <table>\\n                <thead>\\n                    <tr>\\n        '\n        column_count = len(dataset[0].keys())\n        column_width = round(100 / column_count, 2)\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += '\\n                    </tr>\\n                </thead>\\n                <tbody>\\n        '\n        html_rows = []\n        for entry in dataset:\n            row_parts = ['<tr>']\n            for key in entry:\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                row_parts.append(f'<td>{value}</td>')\n            row_parts.append('</tr>')\n            html_rows.append(''.join(row_parts))\n        html_content += ''.join(html_rows)\n        html_content += '\\n                </tbody>\\n            </table>\\n        </body>\\n        </html>\\n        '\n        html_file_path = json_file.with_suffix('.html')\n        try:\n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f'Failed saving: {html_file_path}')\n```",
        "output": "'directory'\n\nPurpose and Significance:\n\"In `convert_json_to_html` within py2dataset\\save_output.py script, 'directory' is provided as an input argument that defines a path pointing towards the directory containing JSON files intended for conversion into HTML format. This parameter serves as a crucial entry point to locate the source data for transformation. Inside the function `convert_json_to_html`, it iterates through all JSON files present in this specified directory using Pathlib's recursive globbing method (`Path(directory).rglob(\"*.json\")`. Each JSON file is processed individually, and its contents are read into a dictionary format using `read_file()`. The primary purpose of 'directory' input here lies in accessing the files that need to be converted from JSON to HTML. By handling these files, `convert_json_to_html` ultimately creates HTML representations with preserved spacing and tabulation from their original structure.\""
    },
    {
        "instruction": "Docstring of `convert_json_to_html` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format and save .html file.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int=4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n    for json_file in Path(directory).rglob('*.json'):\n        try:\n            dataset = read_file(json_file)\n            if not dataset:\n                continue\n        except Exception:\n            continue\n        html_content = '\\n        <html>\\n        <head>\\n            <style>\\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\\n                th, td {\\n                    border: 1px solid black;\\n                    padding: 8px;\\n                    text-align: left;\\n                    white-space: pre-line;\\n                    vertical-align: top;\\n                    word-wrap: break-word;\\n                }\\n            </style>\\n        </head>\\n        <body>\\n            <table>\\n                <thead>\\n                    <tr>\\n        '\n        column_count = len(dataset[0].keys())\n        column_width = round(100 / column_count, 2)\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += '\\n                    </tr>\\n                </thead>\\n                <tbody>\\n        '\n        html_rows = []\n        for entry in dataset:\n            row_parts = ['<tr>']\n            for key in entry:\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                row_parts.append(f'<td>{value}</td>')\n            row_parts.append('</tr>')\n            html_rows.append(''.join(row_parts))\n        html_content += ''.join(html_rows)\n        html_content += '\\n                </tbody>\\n            </table>\\n        </body>\\n        </html>\\n        '\n        html_file_path = json_file.with_suffix('.html')\n        try:\n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f'Failed saving: {html_file_path}')\n```",
        "output": "Convert JSON files within a given directory to HTML format and save .html file.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None"
    },
    {
        "instruction": "Calls made in `convert_json_to_html` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format and save .html file.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int=4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n    for json_file in Path(directory).rglob('*.json'):\n        try:\n            dataset = read_file(json_file)\n            if not dataset:\n                continue\n        except Exception:\n            continue\n        html_content = '\\n        <html>\\n        <head>\\n            <style>\\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\\n                th, td {\\n                    border: 1px solid black;\\n                    padding: 8px;\\n                    text-align: left;\\n                    white-space: pre-line;\\n                    vertical-align: top;\\n                    word-wrap: break-word;\\n                }\\n            </style>\\n        </head>\\n        <body>\\n            <table>\\n                <thead>\\n                    <tr>\\n        '\n        column_count = len(dataset[0].keys())\n        column_width = round(100 / column_count, 2)\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += '\\n                    </tr>\\n                </thead>\\n                <tbody>\\n        '\n        html_rows = []\n        for entry in dataset:\n            row_parts = ['<tr>']\n            for key in entry:\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                row_parts.append(f'<td>{value}</td>')\n            row_parts.append('</tr>')\n            html_rows.append(''.join(row_parts))\n        html_content += ''.join(html_rows)\n        html_content += '\\n                </tbody>\\n            </table>\\n        </body>\\n        </html>\\n        '\n        html_file_path = json_file.with_suffix('.html')\n        try:\n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f'Failed saving: {html_file_path}')\n```",
        "output": "'text.replace(', '&nbsp;').replace', 'text.replace', 'Path(directory).rglob', 'Path', 'read_file', 'len', 'dataset[0].keys', 'round', 'escape', 'str', 'preserve_spacing', 'value.replace', 'row_parts.append', 'html_rows.append', '.join', 'json_file.with_suffix', 'open', 'file.write', 'logging.info'\n\nPurpose and Significance:\nThe function `convert_json_to_html` primarily focuses on transforming JSON files within a given directory into HTML format while preserving spaces and tabs within text content. It performs several operations to achieve this goal, which are explained below for each identified call made in the process:\n\n1) `text.replace(', '&nbsp;')` & `text.replace(' ', '&nbsp;')`: These replace operations convert newline characters ('\\n') and spaces respectively into HTML entities ('&nbsp;'), ensuring HTML doesn't misinterpret text elements like code tabs during output creation, which enhances its rendering later.\n   a) By transforming '\\n', lines get replaced by a non-breakable space HTML element - avoiding disruptions of inline HTML tables across rows when rendering data as HTML.\n   b) Similarly, spaces are also substituted with HTML's equivalent non-breaking space ('&nbsp;') for preserving spacing between text elements within columns of table cells.\n\n2) `Path(directory).rglob('*.json')`: This call retrieves all JSON files from the specified directory using pathlib module's Path class method 'rglob'. It searches recursively through directories and subdirectories for '.json' files in the given input directory ('directory').\n\n3) `read_file(json_file)`: Reads each JSON file content into a Python dictionary format by invoking the `read_file()` function. This function determines a JSON/YAML data based on extension through '.suffix'. 'read_file' function, processing all located '.json' files accordingly before advancing operations on found contents as specified previously described under corresponding context for its behavioral flow (See \"Functional Requirements\").\n   a) Exception handling catches issues in file reading to report logs for future rectifications.\n\n4) `len(conv['value'].encode('utf-8'))`: Measures byte size of text elements in conversations ('conv') by encoding them into bytes using 'utf-8' encoding scheme, which is later used to update 'nbytes' field in chat_ml JSON file entries.\n\n5) `json_file.with_suffix('.html')`: It changes the suffix of processed JSON files into '.html', readying them for saving transformed HTML data output while preserving other parts of file paths (original directory and filename).\n\n6) `open(html_file_path, 'w', encoding='utf-8') as file:` Opens a new file with write permission ('w') in the specified path ('html_file_path'), using UTF-8 encoding to ensure proper handling of characters during writing HTML content.\n   a) Handles saving HTML table representations output from previous `html_content`. This saves HTML generated output with all converted '.json' files contents transformed within this function.\n   b) Any issues are caught as logging info for analysis.\n\n7) `nx.spring_layout(G)`: Uses NetworkX library's spring_layout() algorithm to compute graph node positions in an attractive arrangement suitable for visualization of code relationships represented by 'G'. It organizes nodes for aesthetically pleasing presentation within graphs.\n   a) 'nx.draw', 'plt.figure(figsize=...)': Sets figure size while preparing matplotlib for graph plotting; NetworkX function nx.draw() plots the constructed graph considering edge labels from `edge_labels` dict with proper formatting settings.\n\n8) `output_file = output_subdir / f\"{base_name}.{graph_type}.png\"`: Generates output file name ('base_name') by concatenating specified 'graph_type' (entire_code_graph or others if implemented) as '.png' extension within user-specified output directory ('output_subdir'). This facilitates access to visual outputs stored.\n9) `plt.savefig(output_file)`: Outputs code graphs using given layout details, as previously built using 'plt.figure', saved at calculated filename ('output_file') with PNG format.\n   a) Error handling catches issues during saving images to log them for future debugging purposes.```"
    },
    {
        "instruction": "Variables in `convert_json_to_html` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format and save .html file.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int=4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n    for json_file in Path(directory).rglob('*.json'):\n        try:\n            dataset = read_file(json_file)\n            if not dataset:\n                continue\n        except Exception:\n            continue\n        html_content = '\\n        <html>\\n        <head>\\n            <style>\\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\\n                th, td {\\n                    border: 1px solid black;\\n                    padding: 8px;\\n                    text-align: left;\\n                    white-space: pre-line;\\n                    vertical-align: top;\\n                    word-wrap: break-word;\\n                }\\n            </style>\\n        </head>\\n        <body>\\n            <table>\\n                <thead>\\n                    <tr>\\n        '\n        column_count = len(dataset[0].keys())\n        column_width = round(100 / column_count, 2)\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += '\\n                    </tr>\\n                </thead>\\n                <tbody>\\n        '\n        html_rows = []\n        for entry in dataset:\n            row_parts = ['<tr>']\n            for key in entry:\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                row_parts.append(f'<td>{value}</td>')\n            row_parts.append('</tr>')\n            html_rows.append(''.join(row_parts))\n        html_content += ''.join(html_rows)\n        html_content += '\\n                </tbody>\\n            </table>\\n        </body>\\n        </html>\\n        '\n        html_file_path = json_file.with_suffix('.html')\n        try:\n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f'Failed saving: {html_file_path}')\n```",
        "output": "'row_parts', 'column_width', 'html_rows', 'value', 'column_count', 'html_content', 'html_file_path', 'dataset', 'directory'\n\nPurpose and Significance:\nIn the `convert_json_to_html` function within the `py2dataset.save_output` module, several variables play crucial roles to convert JSON files into HTML format while preserving spaces and tabs within their content. These variables are as follows:\n\n1) `row_parts`: A list used temporarily during HTML table row construction for each entry in a JSON dataset. It holds strings representing individual table cells before joining them into a single HTML row string.\n\n2) `column_width`: A calculated float value representing the percentage width allocated to each column in the generated HTML table layout, determined by dividing 100 by the number of columns present in the dataset. This ensures proper table alignment.\n\n3) `html_rows`: An accumulating list containing HTML row strings built from individual `row_parts`. It stores the formatted data rows read from each JSON file within a specified directory after being prepared using 'escape' function to render well in an HTML format.\n\n4) `value`: During parsing, extracted value as a string that contains Python dictionary elements for conversion into HTML representation while maintaining whitespaces and tabs intact by applying the `preserve_spacing()` function call. This variable is used within `row_parts`.\n\n5) `column_count`: Derived from the number of keys present in the first entry of the dataset, it helps determine column width percentage for proper table layout rendering in HTML format.\n\n6) `html_content`: A comprehensive string containing HTML tags and dynamic data to create a readable table structure with headers and body content based on combined JSON file data. It is assembled by concatenating necessary elements such as <html>, <head>, <tbody>, row strings from `html_rows`, </tbody>, </html>.\n\n7) `html_file_path`: A PathLib instance representing the output HTML filename generated by appending '.html' suffix to the original JSON file name. It stores the path where converted HTML content will be saved after writing operations.\n\n8) `dataset`: A dictionary obtained through reading JSON files using `read_file()`. This variable contains the entire dataset of input JSON data for conversion into HTML format within the function scope.\n\n9) `directory`: Strings representing the path to a directory where JSON files are located. It serves as an argument passed from the caller function to locate JSON files for processing in `convert_json_to_html`. This variable helps iterate over relevant files and convert them into HTML format."
    },
    {
        "instruction": "Returns from `convert_json_to_html` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format and save .html file.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int=4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n    for json_file in Path(directory).rglob('*.json'):\n        try:\n            dataset = read_file(json_file)\n            if not dataset:\n                continue\n        except Exception:\n            continue\n        html_content = '\\n        <html>\\n        <head>\\n            <style>\\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\\n                th, td {\\n                    border: 1px solid black;\\n                    padding: 8px;\\n                    text-align: left;\\n                    white-space: pre-line;\\n                    vertical-align: top;\\n                    word-wrap: break-word;\\n                }\\n            </style>\\n        </head>\\n        <body>\\n            <table>\\n                <thead>\\n                    <tr>\\n        '\n        column_count = len(dataset[0].keys())\n        column_width = round(100 / column_count, 2)\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += '\\n                    </tr>\\n                </thead>\\n                <tbody>\\n        '\n        html_rows = []\n        for entry in dataset:\n            row_parts = ['<tr>']\n            for key in entry:\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                row_parts.append(f'<td>{value}</td>')\n            row_parts.append('</tr>')\n            html_rows.append(''.join(row_parts))\n        html_content += ''.join(html_rows)\n        html_content += '\\n                </tbody>\\n            </table>\\n        </body>\\n        </html>\\n        '\n        html_file_path = json_file.with_suffix('.html')\n        try:\n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f'Failed saving: {html_file_path}')\n```",
        "output": "'text.replace(', '&nbsp;').replace('\\\\t', '&nbsp;' * tab_width)'\n\nPurpose and Significance:\nThe `convert_json_to_html` function primarily focuses on transforming JSON files within a given directory into HTML format while preserving spaces and tabs found within their contents. It iterates through all '.json' files present in the specified directory using `Path(directory).rglob(\"*.json\")`. For each file, it reads data using `read_file()` function and stores it as 'dataset'. Further on, a comprehensive HTML template is prepared by appending HTML header, table headers according to JSON keys from 'dataset[0]', creating table rows ('row_parts') containing encoded escape versions of dataset values while substituting line breaks with '<br/>' using `escape()` function and `preserve_spacing()`. The generated HTML content is then saved as '.html' file with the original JSON filename modified accordingly. If any error occurs during reading or saving operations, logging messages are logged for information purposes.\n\nIn summary, this utility function converts JSON files into human-readable HTML format while maintaining spaces and tabs in textual data.'''"
    },
    {
        "instruction": "Inputs to `combine_json_files` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef combine_json_files(directory: str, html: bool, questions: Dict) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and\n    then remove duplicates. Also generate a 'training.json' file.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n    logging.info(f'Combining JSON files in {directory}')\n    combined_data, code_filename = ([], [])\n    skip_files = {'instruct.json', 'chat_ml.json', 'document_code.json'}\n    for json_file in Path(directory).rglob('*.json'):\n        if json_file.name in skip_files:\n            continue\n        try:\n            file_data = read_file(json_file)\n            if file_data:\n                combined_data.extend(file_data)\n                cleaned_name = json_file.relative_to(directory).with_suffix('').as_posix().replace('.instruct', '')\n                code_filename.append(cleaned_name)\n        except Exception as e:\n            logging.info(f'Failed reading: {json_file}. Error: {e}')\n    write_file(combined_data, Path(directory) / 'instruct.json')\n    purpose_question = [item['text'] for item in questions if item['id'] == 'file_purpose'][0]\n    purpose_question = purpose_question.split('{filename}')[0]\n    purpose_data = [item for item in combined_data if item['instruction'].startswith(purpose_question)]\n    document_code = [{'document': item['output'], 'code': item['input']} for item in purpose_data]\n    for i, item in enumerate(document_code):\n        item['code filename'] = code_filename[i]\n    write_file(document_code, Path(directory) / 'document_code.json')\n    chat_ml = [{'conversation': [{'from': 'system', 'value': f\"code documentation: {item['document']}\"}, {'from': 'human', 'value': 'Output the Python code described by the code documentation.'}, {'from': 'gpt', 'value': item['code']}], 'nbytes': '0', 'source': item['code filename']} for item in document_code]\n    for item in chat_ml:\n        nbytes = 0\n        for conv in item['conversation']:\n            nbytes += len(conv['value'].encode('utf-8'))\n        item['nbytes'] = nbytes\n    write_file(chat_ml, Path(directory) / 'chat_ml.json')\n    if html:\n        logging.info('Converting JSON files to HTML')\n        convert_json_to_html(directory)\n    return {'instruct_list': combined_data}\n```",
        "output": "'directory', 'html', 'questions'\n\nPurpose and Significance:\nThe `combine_json_files` function plays a crucial role in consolidating multiple JSON files within a specified directory into a single 'instruct.json' file while removing duplicate entries. It also generates an additional 'training.json' file containing conversation data extracted from parsed inputs related to a defined instruction purposive text named as \"file_purpose\". Given parameters for the `combine_json_files()`, namely the 'directory': points to a specified directory having JSON files stored. Whereas `html` defines whether HTML conversion should be performed after combining JSON files into instruct format (default behavior is False). Lastly, 'questions' represents a list comprehension that filters out instruction data from combined JSON entries based on a matching ID (\"file_purpose\") to create a chatbot-like conversation scenario between human and AI agent for understanding the Python file context. The human asks \"Output the Python code described by the code documentation.\", system responds with the actual code as output, followed by returns specification from targeted functions in edge labels of code graphs created through other utilities such as `create_code_graph()`. Here,'questions' list acts as a connector between parsed JSON files and creating interpretative instructions for the subsequent processes.''''''"
    },
    {
        "instruction": "Docstring of `combine_json_files` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef combine_json_files(directory: str, html: bool, questions: Dict) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and\n    then remove duplicates. Also generate a 'training.json' file.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n    logging.info(f'Combining JSON files in {directory}')\n    combined_data, code_filename = ([], [])\n    skip_files = {'instruct.json', 'chat_ml.json', 'document_code.json'}\n    for json_file in Path(directory).rglob('*.json'):\n        if json_file.name in skip_files:\n            continue\n        try:\n            file_data = read_file(json_file)\n            if file_data:\n                combined_data.extend(file_data)\n                cleaned_name = json_file.relative_to(directory).with_suffix('').as_posix().replace('.instruct', '')\n                code_filename.append(cleaned_name)\n        except Exception as e:\n            logging.info(f'Failed reading: {json_file}. Error: {e}')\n    write_file(combined_data, Path(directory) / 'instruct.json')\n    purpose_question = [item['text'] for item in questions if item['id'] == 'file_purpose'][0]\n    purpose_question = purpose_question.split('{filename}')[0]\n    purpose_data = [item for item in combined_data if item['instruction'].startswith(purpose_question)]\n    document_code = [{'document': item['output'], 'code': item['input']} for item in purpose_data]\n    for i, item in enumerate(document_code):\n        item['code filename'] = code_filename[i]\n    write_file(document_code, Path(directory) / 'document_code.json')\n    chat_ml = [{'conversation': [{'from': 'system', 'value': f\"code documentation: {item['document']}\"}, {'from': 'human', 'value': 'Output the Python code described by the code documentation.'}, {'from': 'gpt', 'value': item['code']}], 'nbytes': '0', 'source': item['code filename']} for item in document_code]\n    for item in chat_ml:\n        nbytes = 0\n        for conv in item['conversation']:\n            nbytes += len(conv['value'].encode('utf-8'))\n        item['nbytes'] = nbytes\n    write_file(chat_ml, Path(directory) / 'chat_ml.json')\n    if html:\n        logging.info('Converting JSON files to HTML')\n        convert_json_to_html(directory)\n    return {'instruct_list': combined_data}\n```",
        "output": "Combine all JSON files in the output directory into 'instruct.json', and\n    then remove duplicates. Also generate a 'training.json' file.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets."
    },
    {
        "instruction": "Calls made in `combine_json_files` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef combine_json_files(directory: str, html: bool, questions: Dict) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and\n    then remove duplicates. Also generate a 'training.json' file.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n    logging.info(f'Combining JSON files in {directory}')\n    combined_data, code_filename = ([], [])\n    skip_files = {'instruct.json', 'chat_ml.json', 'document_code.json'}\n    for json_file in Path(directory).rglob('*.json'):\n        if json_file.name in skip_files:\n            continue\n        try:\n            file_data = read_file(json_file)\n            if file_data:\n                combined_data.extend(file_data)\n                cleaned_name = json_file.relative_to(directory).with_suffix('').as_posix().replace('.instruct', '')\n                code_filename.append(cleaned_name)\n        except Exception as e:\n            logging.info(f'Failed reading: {json_file}. Error: {e}')\n    write_file(combined_data, Path(directory) / 'instruct.json')\n    purpose_question = [item['text'] for item in questions if item['id'] == 'file_purpose'][0]\n    purpose_question = purpose_question.split('{filename}')[0]\n    purpose_data = [item for item in combined_data if item['instruction'].startswith(purpose_question)]\n    document_code = [{'document': item['output'], 'code': item['input']} for item in purpose_data]\n    for i, item in enumerate(document_code):\n        item['code filename'] = code_filename[i]\n    write_file(document_code, Path(directory) / 'document_code.json')\n    chat_ml = [{'conversation': [{'from': 'system', 'value': f\"code documentation: {item['document']}\"}, {'from': 'human', 'value': 'Output the Python code described by the code documentation.'}, {'from': 'gpt', 'value': item['code']}], 'nbytes': '0', 'source': item['code filename']} for item in document_code]\n    for item in chat_ml:\n        nbytes = 0\n        for conv in item['conversation']:\n            nbytes += len(conv['value'].encode('utf-8'))\n        item['nbytes'] = nbytes\n    write_file(chat_ml, Path(directory) / 'chat_ml.json')\n    if html:\n        logging.info('Converting JSON files to HTML')\n        convert_json_to_html(directory)\n    return {'instruct_list': combined_data}\n```",
        "output": "'logging.info', 'Path(directory).rglob', 'Path', 'read_file', 'combined_data.extend', 'json_file.relative_to(directory).with_suffix('').as_posix().replace', 'json_file.relative_to(directory).with_suffix('').as_posix', 'json_file.relative_to(directory).with_suffix', 'json_file.relative_to', 'code_filename.append', 'write_file', 'purpose_question.split', 'item['instruction'].startswith', 'enumerate', 'len', 'conv['value'].encode', 'convert_json_to_html'\n\nPurpose and Significance:\nIn `combine_json_files`, several crucial operations are performed to consolidate JSON files within a directory into structured data sets. Key functions employed here include logging messages (`logging.info()`) for informative updates during progression, `Path(directory).rglob()` scans the target folder to obtain '.json' file paths; `read_file()` reads contents from each identified JSON file into a list named 'combined_data'. Relative paths are processed using methods like `json_file.relative_to(directory).with_suffix('').as_posix().replace()`, `json_file.relative_to(directory).with_suffix('').as_posix()`, and `json_file.relative_to(directory).with_suffix()` for better referencing.\n\n'write_file()' writes the accumulated data into 'instruct.json', while skipping certain files ('skip_files') to maintain consistency in operations. The function 'purpose_question.split()` extracts purposeful instructions from JSON entries with matching IDs (\"file_purpose\"). `enumerate()` iterates over document code generation ('document_code'), counting indices for 'code filename' assignment within the list. Length calculation using 'len()` determines byte counts for each conversation entry ('nbytes').\n\nFurthermore, `convert_json_to_html()` is invoked to convert JSON files into HTML format if specified ('html=True') as an argument. This function preserves spaces and tabs in text content through `preserve_spacing()`, creating '.html' outputs for readability enhancement.\n\nThe overall purpose of these calls within `combine_json_files` is to consolidate JSON files into a single 'instruct_list', remove duplicates, generate additional datasets ('document_code'), update metadata ('nbytes') and optionally convert JSON files to HTML format upon user request.'''"
    },
    {
        "instruction": "Variables in `combine_json_files` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef combine_json_files(directory: str, html: bool, questions: Dict) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and\n    then remove duplicates. Also generate a 'training.json' file.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n    logging.info(f'Combining JSON files in {directory}')\n    combined_data, code_filename = ([], [])\n    skip_files = {'instruct.json', 'chat_ml.json', 'document_code.json'}\n    for json_file in Path(directory).rglob('*.json'):\n        if json_file.name in skip_files:\n            continue\n        try:\n            file_data = read_file(json_file)\n            if file_data:\n                combined_data.extend(file_data)\n                cleaned_name = json_file.relative_to(directory).with_suffix('').as_posix().replace('.instruct', '')\n                code_filename.append(cleaned_name)\n        except Exception as e:\n            logging.info(f'Failed reading: {json_file}. Error: {e}')\n    write_file(combined_data, Path(directory) / 'instruct.json')\n    purpose_question = [item['text'] for item in questions if item['id'] == 'file_purpose'][0]\n    purpose_question = purpose_question.split('{filename}')[0]\n    purpose_data = [item for item in combined_data if item['instruction'].startswith(purpose_question)]\n    document_code = [{'document': item['output'], 'code': item['input']} for item in purpose_data]\n    for i, item in enumerate(document_code):\n        item['code filename'] = code_filename[i]\n    write_file(document_code, Path(directory) / 'document_code.json')\n    chat_ml = [{'conversation': [{'from': 'system', 'value': f\"code documentation: {item['document']}\"}, {'from': 'human', 'value': 'Output the Python code described by the code documentation.'}, {'from': 'gpt', 'value': item['code']}], 'nbytes': '0', 'source': item['code filename']} for item in document_code]\n    for item in chat_ml:\n        nbytes = 0\n        for conv in item['conversation']:\n            nbytes += len(conv['value'].encode('utf-8'))\n        item['nbytes'] = nbytes\n    write_file(chat_ml, Path(directory) / 'chat_ml.json')\n    if html:\n        logging.info('Converting JSON files to HTML')\n        convert_json_to_html(directory)\n    return {'instruct_list': combined_data}\n```",
        "output": "'questions', 'purpose_data', 'html', 'purpose_question', 'cleaned_name', 'skip_files', 'file_data', 'directory', 'nbytes', 'document_code', 'chat_ml'\n\nPurpose and Significance:\nIn `combine_json_files`, several variables contribute towards merging JSON files from a specified directory, handling data structuring and preparation of relevant output files. These key parameters perform significant tasks within its workflow:\n\n1) **questions**: An array storing particular extracted items filtered based on conditions, mostly for identifying file purpose (\"file_purpose\" ID). It helps in creating 'document_code' list containing Python code descriptions and their corresponding outputs.\n2) **purpose_data**: A subset of combined_data filtered by starting strings matching \"file_purpose\", representing a crucial part in building 'document_code', a collection containing documented codes along with respective filenames as keys ('code filename').\n3) **html**: This boolean flag triggers HTML conversion of merged JSON files if set to True during function call, enabling additional functionality beyond JSON consolidation.\n4) **purpose_question**: A string variable holding the extracted query pattern for identifying file purpose within 'questions' list (\"{filename}\" removal). It facilitates selection of required instructions while compiling data sets for \"document_code\".\n5) **cleaned_name**: Manipulating relative Python path with necessary edits excluding suffix and \"instruct\" or \"chat_ml\" segments; required when naming the files post data merging and manipulation in 'write_file()' call.\n6) **skip_files**: Set comprising exclusions to circumvent selected JSON file paths (\"instruct.json\", \"chat_ml.json\") during iterations over directory contents ('Path(directory).rglob(\"*.json\")').\n7) **file_data**: A temporary list holding parsed JSON data from each processed file in the loop, ultimately accumulating the comprehensive set in combined_data array within 'for' clause loops.\n8) **directory**: Main parameter providing source location (\"path of files being amalgamated); fundamental in creating a singular instruction_list encompassing every merged entry ('Path(directory)').\n9) **nbytes**: Summed length encoding ('len()') of conversations ('source'/'target' values) per each edge connection in 'G'; essential to update the nbytes field. This metric indicates conversation sizes when converted to JSON ('{item[\"nbytes\"] = ...') and supports consistency checking downstream code graph calculations.\n10) **document_code**: Generated after processing 'purpose_data', an aggregated collection representing combined codes with their respective filenames as keys ('code filename'). It's saved as \"document_code.json\" in the output directory.\n11) **chat_ml**: A list of structured conversation data for each edge in graph format; updated with 'nbytes' values from code edges, written as JSON file (\"write_file()\") named \"chat_ml.json\". /nThese variables work collectively to ensure comprehensive data processing and structuring while consolidating multiple JSON files into 'instruct_list', a central outcome of the function along with HTML conversion if desired (by html parameter). Their individual roles enable smooth functioning and cohesive functionality in the overall `combine_json_files` methodology."
    },
    {
        "instruction": "Returns from `combine_json_files` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef combine_json_files(directory: str, html: bool, questions: Dict) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and\n    then remove duplicates. Also generate a 'training.json' file.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n    logging.info(f'Combining JSON files in {directory}')\n    combined_data, code_filename = ([], [])\n    skip_files = {'instruct.json', 'chat_ml.json', 'document_code.json'}\n    for json_file in Path(directory).rglob('*.json'):\n        if json_file.name in skip_files:\n            continue\n        try:\n            file_data = read_file(json_file)\n            if file_data:\n                combined_data.extend(file_data)\n                cleaned_name = json_file.relative_to(directory).with_suffix('').as_posix().replace('.instruct', '')\n                code_filename.append(cleaned_name)\n        except Exception as e:\n            logging.info(f'Failed reading: {json_file}. Error: {e}')\n    write_file(combined_data, Path(directory) / 'instruct.json')\n    purpose_question = [item['text'] for item in questions if item['id'] == 'file_purpose'][0]\n    purpose_question = purpose_question.split('{filename}')[0]\n    purpose_data = [item for item in combined_data if item['instruction'].startswith(purpose_question)]\n    document_code = [{'document': item['output'], 'code': item['input']} for item in purpose_data]\n    for i, item in enumerate(document_code):\n        item['code filename'] = code_filename[i]\n    write_file(document_code, Path(directory) / 'document_code.json')\n    chat_ml = [{'conversation': [{'from': 'system', 'value': f\"code documentation: {item['document']}\"}, {'from': 'human', 'value': 'Output the Python code described by the code documentation.'}, {'from': 'gpt', 'value': item['code']}], 'nbytes': '0', 'source': item['code filename']} for item in document_code]\n    for item in chat_ml:\n        nbytes = 0\n        for conv in item['conversation']:\n            nbytes += len(conv['value'].encode('utf-8'))\n        item['nbytes'] = nbytes\n    write_file(chat_ml, Path(directory) / 'chat_ml.json')\n    if html:\n        logging.info('Converting JSON files to HTML')\n        convert_json_to_html(directory)\n    return {'instruct_list': combined_data}\n```",
        "output": "'{'instruct_list': combined_data}'\n\nPurpose and Significance:\nThe 'combine_json_files' function serves a pivotal role within the script by consolidating multiple JSON files located in a specified directory into a unified dataset named 'instruct_list'. This combined data holds crucial information extracted from individual JSON entries after removing duplicate instances. The primary objectives achieved through this utility are:\n\n1. Reading JSON files excluding certain skip files ('instruct.json', 'chat_ml.json') from the given directory using Pathlib's recursive globbing feature ('Path(directory).rglob(\"*.json\")').\n2. Building an array named 'combined_data' by appending each read file data into it while also collecting cleaned names of code files related to each JSON entry ('code_filename') for further processing.\n3. Writing the consolidated dataset as 'instruct.json' using write_file() function within the specified directory path ('write_file(combined_data, Path(directory) / \"instruct.json\")'.\n4. Extracting purposeful data from JSON entries related to a specific question ('purpose_question') for generating 'document_code', which is a list containing Python code descriptions and their corresponding input files ('{...}\"outputs\":item['document'], \"{...}'input':item['input']}\"'), preserving relative paths to identify file contents (their connections in \"file information\"). These objects become critical entries under the category of 'document_code'.\n5. Preparation for another JSON data storage titled as 'chat_ml.json' involving artificial conversational structures comprising GPT queries/replies to model the interactions related to input files along with an estimation of byte usage ('nbytes') based on encoding size calculation (len('conv[\"value\"].encode(\"utf-8\")). Each conversation is updated with this estimated 'nbytes'.\n6. Eventually writing both 'document_code' and 'chat_ml' JSON datasets within the output directory ('write_file(chat_ml, Path(directory) / \"chat_ml.json\"). Additionally, it has an optional feature to convert consolidated JSON files into HTML format when invoked with 'html=True'.\n7. The returned object from `combine_json_files` is a dictionary containing the key 'instruct_list', which represents the merged JSON data extracted from processed files.'''"
    },
    {
        "instruction": "Inputs to `create_code_graph` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    graph_type = 'entire_code_graph'\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details['file_info'][graph_type]['nodes'])\n    for edge in file_details['file_info'][graph_type]['edges']:\n        source, target = (edge['source'], edge['target'])\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(source, target, **{k: v for k, v in edge.items() if k in ['target_inputs', 'target_returns']})\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(G, pos, with_labels=True, font_weight='bold', font_size=8, node_shape='s', node_size=500, width=1, arrowsize=12)\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if 'target_returns' in edge[2] and edge[2]['target_returns']:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[edge[0], edge[1]] = '\\n'.join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n    output_file = output_subdir / f'{base_name}.{graph_type}.png'\n    plt.savefig(output_file)\n    plt.close()\n```",
        "output": "'file_details', 'base_name', 'output_subdir'\n\nPurpose and Significance:\nThe inputs for the `create_code_graph` function primarily influence how the code graph visualization process is customized within the given Python script context. They aid in shaping up distinct yet critical elements related to the resulting image generation steps:\n\n1) `file_details`: This parameter contains extracted details from a Python file analyzed by the program. It essentially serves as an informative resource that includes specific attributes such as nodes and edges for network graph construction. In other words, `file_details` encapsulates data necessary to understand function call relationships in the codebase which is eventually translated into visual form using NetworkX library functions like `nx.DiGraph()`, `G.add_nodes_from()`, and `G.add_edge()`. These details enable generating a graphical representation of Python file dependencies.\n\n2) `base_name`: This input argument serves as the base name for output files created during the code graph generation process. It acts as a common prefix to uniquely identify each resulting image generated from `create_code_graph()`. In particular, it forms part of the final filename when saving the graphical representation as '.png'. This allows users to distinguish different graphs visually while organizing them systematically in their desired output directory.\n\n3) `output_subdir`: The third input parameter, `output_subdir`, refers to a specific subdirectory where the graph visualizations are stored. It essentially sets the path where '.png' files will be saved upon successful completion of the code graph creation process initiated by `create_code_graph()`. This directory structure helps users manage generated images efficiently while maintaining orderliness in their output storage. Additionally, it ensures proper accessibility to the final product within a project folder hierarchy.```"
    },
    {
        "instruction": "Docstring of `create_code_graph` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    graph_type = 'entire_code_graph'\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details['file_info'][graph_type]['nodes'])\n    for edge in file_details['file_info'][graph_type]['edges']:\n        source, target = (edge['source'], edge['target'])\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(source, target, **{k: v for k, v in edge.items() if k in ['target_inputs', 'target_returns']})\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(G, pos, with_labels=True, font_weight='bold', font_size=8, node_shape='s', node_size=500, width=1, arrowsize=12)\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if 'target_returns' in edge[2] and edge[2]['target_returns']:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[edge[0], edge[1]] = '\\n'.join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n    output_file = output_subdir / f'{base_name}.{graph_type}.png'\n    plt.savefig(output_file)\n    plt.close()\n```",
        "output": "Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None"
    },
    {
        "instruction": "Calls made in `create_code_graph` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    graph_type = 'entire_code_graph'\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details['file_info'][graph_type]['nodes'])\n    for edge in file_details['file_info'][graph_type]['edges']:\n        source, target = (edge['source'], edge['target'])\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(source, target, **{k: v for k, v in edge.items() if k in ['target_inputs', 'target_returns']})\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(G, pos, with_labels=True, font_weight='bold', font_size=8, node_shape='s', node_size=500, width=1, arrowsize=12)\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if 'target_returns' in edge[2] and edge[2]['target_returns']:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[edge[0], edge[1]] = '\\n'.join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n    output_file = output_subdir / f'{base_name}.{graph_type}.png'\n    plt.savefig(output_file)\n    plt.close()\n```",
        "output": "'nx.DiGraph', 'G.add_nodes_from', 'G.add_edge', 'edge.items', 'plt.figure', 'nx.spring_layout', 'nx.draw', 'G.edges', 'label.append', '', '.join', '\\\\n'.join', 'nx.draw_networkx_edge_labels', 'plt.savefig', 'plt.close'\n\nPurpose and Significance:\nIn `create_code_graph`, several Python libraries are employed to construct a graph representation of code relationships from provided file details, eventually saving it as an image. Key method calls with their significance include:\n\n1) `nx.DiGraph()`: Creates a directed graph object used for network structure generation. It will act as a central repository storing nodes and edges within the code.\n2) `G.add_nodes_from()`: Retrieves node names from file details provided and populates this 'G' object with them, setting up initial graph elements.\n3) `G.add_edge()`: Adds connections between nodes based on input data structure ('source', 'target'), incorporating additional attributes related to edges like 'target_inputs' and 'target_returns'. This enriches each edge in the code graph with meaningful details.\n4) `plt.figure(figsize=(20, 20))`: Configures the graph layout dimensions before any visualization begins; ensures optimal visibility of elements during plotting.\n5) `nx.spring_layout()`: Places nodes within graph for a visually pleasing layout utilizing Spring-Embedding algorithm for positioning them without edge crossings or overlaps.\n6) `nx.draw()`: Draws the graph using the computed positions ('pos') with labels enabled, node shapes as small squares ('node_shape=\"s\"'), size set to 500 pixels ('node_size=500'), edges width as thin lines ('width=1'), and arrow sizes ('arrowsize=12').\n7) `nx.draw_networkx_edge_labels()`: Adds edge labels using a dictionary ('edge_labels') with line breaks for better readability. It handles text wrapping ('font_size=6') within the graph edges.\n8) `plt.savefig()`: Exports the constructed graph as a '.png' image file in specified output directory.\n9) `plt.close()`: Closes the current Matplotlib figure after saving the graph image, preparing for future operations without residual data from previous plots.\n\nThese steps collectively create code graphs representing Python file dependencies visually for better understanding of interconnected modules or functions.'''"
    },
    {
        "instruction": "Variables in `create_code_graph` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    graph_type = 'entire_code_graph'\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details['file_info'][graph_type]['nodes'])\n    for edge in file_details['file_info'][graph_type]['edges']:\n        source, target = (edge['source'], edge['target'])\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(source, target, **{k: v for k, v in edge.items() if k in ['target_inputs', 'target_returns']})\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(G, pos, with_labels=True, font_weight='bold', font_size=8, node_shape='s', node_size=500, width=1, arrowsize=12)\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if 'target_returns' in edge[2] and edge[2]['target_returns']:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[edge[0], edge[1]] = '\\n'.join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n    output_file = output_subdir / f'{base_name}.{graph_type}.png'\n    plt.savefig(output_file)\n    plt.close()\n```",
        "output": "'output_subdir', 'edge_labels', 'pos', 'file_details', 'graph_type', 'label', 'output_file', 'base_name', 'G'\n\nPurpose and Significance:\nIn `create_code_graph`, several variables play crucial roles to accomplish generating code graphs from Python file details as PNG images within specified output directories. Their purposes are elaborated below:\n\n1) `output_subdir`: This variable represents a Path object instantiated by combining the given `output_dir` with the relative path of the Python file's parent directory using `Path(output_dir) / relative_path.parent`. It serves as the target location to save generated graphical outputs after creating code graphs.\n   - Significance: Directory management for saving graph images in an organized manner related to the processed Python file.\n\n2) `edge_labels`: A dictionary initialized within `create_code_graph`, which stores tuples of source-target node pairs as keys with their corresponding edge labels generated from graph edges. It helps annotate graphical connections between nodes for better visualization understanding.\n   - Significance: Assisting in labeling graph edges with relevant information about input and return values associated with each connection during the code graph construction phase.\n\n3) `pos`: Created through calling `nx.spring_layout()`, it denotes a dictionary holding x-y coordinates for positioning nodes appropriately while generating visualization using Matplotlib. The `networkx.draw_networkx_edge_labels()` function utilizes this positional data to display edge labels accurately.\n   - Significance: Ensuring proper node placement in the graph layout for better readability and comprehension of code dependencies.\n\n4) `file_details`: A dictionary containing extracted details from a Python file processed as input arguments. It serves as essential information for constructing the code graph using NetworkX DiGraph object 'G'.\n   - Significance: Defining node names (functions, variables etc.) and their connections within the Python file structure to visualize dependencies effectively.\n\n5) `graph_type`: A string variable initialized with \"entire_code_graph\", typically set during function call but could vary as per use cases to distinguish between diverse graph representations within code complexity levels (function, class or entire file graphs).\n   - Significance: Defining the graph scope within NetworkX DiGraph construction for generating appropriate visualizations.\n\n6) `label`: A list holding labels for each edge in the graph created from 'G'. It is populated with inputs and returns associated with respective edges during graph generation.\n   - Significance: Providing contextual information about nodes' connections by annotating edges with input arguments and return values of functions/methods within code graphs.\n\n7) `output_file`: A Path object created by combining the `base_name` appended with \".png\" suffix to uniquely identify graph outputs within `output_subdir`. It indicates where to save the final PNG file for visual representation after plotting graphs.\n   - Significance: Saving generated code graphs in a specific format and location related to processed Python files.\n\n8) `base_name`: String variable representing the base name of output files, typically extracted from the relative path of the Python file ('relative_path.name'). It is used to construct output file names with appropriate extensions.\n   - Significance: Simplifying naming conventions for graphical representations created in `create_code_graph()`.\n\n9) `G` (networkx.DiGraph()): Empty multi-graph initially containing nodes from extracted file details that undergo expansion based on read node lists and edges between functions within Python codebase through NetworkX APIs. Its properties are further modified for plotting the graphical representation of dependencies in the Python script.\n   - Significance: Central data structure for code graph construction to understand and visualize code dependencies as a directed acyclic graph."
    },
    {
        "instruction": "Inputs to `save_python_data` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef save_python_data(file_details: dict, instruct_list: list, relative_path: Path, output_dir: str) -> None:\n    \"\"\"\n    Save the details of the Python file as a YAML file, the instruction data as JSON files,\n    and generate and save code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path of the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parent\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    base_name = relative_path.name\n    write_file(instruct_list, output_subdir / f'{base_name}.instruct.json')\n    write_file(file_details, output_subdir / f'{base_name}.details.yaml')\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except Exception as e:\n        logging.error(f'Error creating graph for {base_name}: {e}', exc_info=True)\n```",
        "output": "'file_details', 'instruct_list', 'relative_path', 'output_dir'\n\nPurpose and Significance:\nThe `save_python_data` function serves as a central point for saving Python file details along with associated instruction data extracted from it into various formats while generating graphical outputs. Its primary inputs are fourfold:\n\n1) `file_details`: This argument encapsulates information about the analyzed Python file which contains diverse properties relating to file details necessary for further operations such as constructing code graphs within `create_code_graph()`. These data points serve as essential context in processing Python scripts, thus allowing proper representation and documentation of code elements.\n\n2) `instruct_list`: It consists of the instruction dataset extracted from the processed Python file using previous functions like `combine_json_files()`. This list of dictionaries contains instructions to generate JSON files that help comprehending programming context and enable conversational data management with the script contents.\n\n3) `relative_path`: It specifies the path within the workspace related to a particular Python file relatively referring from the output directory. In simpler terms, this helps pinpointing Python source files that will undergo further transformation while providing proper handling and context in terms of navigation during the operation execution.\n\n4) `output_dir`: Signifying the targeted output folder path where processed results must be stored (in terms of YAML/JSON files along with generated graphical outputs). This parameter sets the destination for saving data derived from Python file details and instruction datasets in an organized manner, allowing users to customize output locations as per their needs.\n\nEach input plays a significant role in facilitating smooth workflow between various utilities while consolidating different types of files in required formats (YAML/JSON) alongside generating code graphs as PNG images.`save_python_data()` encapsulates all these steps under one function and efficiently maintains proper connection across intermediary steps by acting as an assembly function with dependencies over related modules mentioned before its introduction within the program execution process./n\"\"\""
    },
    {
        "instruction": "Docstring of `save_python_data` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef save_python_data(file_details: dict, instruct_list: list, relative_path: Path, output_dir: str) -> None:\n    \"\"\"\n    Save the details of the Python file as a YAML file, the instruction data as JSON files,\n    and generate and save code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path of the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parent\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    base_name = relative_path.name\n    write_file(instruct_list, output_subdir / f'{base_name}.instruct.json')\n    write_file(file_details, output_subdir / f'{base_name}.details.yaml')\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except Exception as e:\n        logging.error(f'Error creating graph for {base_name}: {e}', exc_info=True)\n```",
        "output": "Save the details of the Python file as a YAML file, the instruction data as JSON files,\n    and generate and save code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path of the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None"
    },
    {
        "instruction": "Calls made in `save_python_data` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef save_python_data(file_details: dict, instruct_list: list, relative_path: Path, output_dir: str) -> None:\n    \"\"\"\n    Save the details of the Python file as a YAML file, the instruction data as JSON files,\n    and generate and save code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path of the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parent\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    base_name = relative_path.name\n    write_file(instruct_list, output_subdir / f'{base_name}.instruct.json')\n    write_file(file_details, output_subdir / f'{base_name}.details.yaml')\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except Exception as e:\n        logging.error(f'Error creating graph for {base_name}: {e}', exc_info=True)\n```",
        "output": "'Path', 'output_subdir.mkdir', 'write_file', 'create_code_graph', 'logging.error'\n\nPurpose and Significance:\nIn `save_python_data`, four primary actions are executed related to data processing and file handling. Each action corresponds to a specific call as follows:\n\n1) 'Path': This built-in Python class is utilized to create relative paths for organizing filesystem navigation within the script. It helps in constructing output directory paths for saving generated files efficiently while maintaining standard library compatibility. In `save_python_data`, it creates an intermediate subdirectory (output_subdir) for storing outputs of graphical representations ('output_subdir / relative_path.parent').\n\n2) 'output_subdir.mkdir': This function call ensures that the previously generated output directory exists prior to saving files by creating missing parent directories recursively with `parents=True` (if not present). It's employed before invoking other functionalities ensuring seamless accessibility of the designated subdirectory for output files.\n\n3) 'write_file': This function call saves Python dictionary data as YAML format ('write_file(instruct_list, output_subdir / f\"{base_name}.instruct.json\") and `file_details` as YAML ('write_file(file_details, output_subdir / f\"{base_name}.details.yaml\")'. It writes data into respective formats based on file suffixes (JSON or YAML).\n\n4) 'create_code_graph': An integral operation invokes the graph visualization procedure in `save_python_data`, producing image representations ('plt.savefig') for the coded graphs in output_subdir using given file details. The code snippet tries to generate graphical outputs as PNG files and catches potential errors through 'logging.error' during execution if any occurs. This visualization process assists users with better understanding of Python code structures and relationships.\n\n5) 'logging.error': This standard library module helps logging exception information during `create_code_graph` when errors surface in creating graphical representations for user insights on problems faced during code graph generation.'''"
    },
    {
        "instruction": "Variables in `save_python_data` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef save_python_data(file_details: dict, instruct_list: list, relative_path: Path, output_dir: str) -> None:\n    \"\"\"\n    Save the details of the Python file as a YAML file, the instruction data as JSON files,\n    and generate and save code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path of the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parent\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    base_name = relative_path.name\n    write_file(instruct_list, output_subdir / f'{base_name}.instruct.json')\n    write_file(file_details, output_subdir / f'{base_name}.details.yaml')\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except Exception as e:\n        logging.error(f'Error creating graph for {base_name}: {e}', exc_info=True)\n```",
        "output": "'output_subdir', 'output_dir', 'instruct_list', 'file_details', 'relative_path', 'base_name'\n\nPurpose and Significance:\nIn `save_python_data`, crucial variables contribute to organizing and managing output files resulting from Python data processing. Their roles are outlined below:\n\n1) `output_subdir` (Path): Specifies a target directory path where graphical outputs shall be stored during code graph generation within the larger process flow. Created as part of a relative file path leading up to the input Python script using given `output_dir`. This subdirectory facilitates organized storage for generated images and other files related to the primary Python file under analysis.\n2) `output_dir` (str): Represents the main directory where output files will be saved, including YAML/JSON data files and graphical outputs. It is passed as an argument to create subdirectories when necessary ('Path(output_dir).mkdir()').\n3) `instruct_list` (List[Dict]): Holds instruction data extracted from the Python file after consolidation in `combine_json_files()`. This list contains JSON datasets resulting from combined JSON files, ready to be saved as 'instruct.json'.\n4) `file_details` (dict): Contains details extracted from the analyzed Python script. It serves as input data for generating code graphs ('create_code_graph()') and saving YAML file contents related to the Python file itself ('write_file()').\n5) `relative_path` (Path): Identifies the exact location of the Python script within a larger directory structure relative to its parent. It is employed while constructing output subdirectories and graph names ('output_subdir', 'base_name'). As this is primarily associated with a particular input file, it directs outputs according to where the input lies within overall organizational schemas.\n6) `base_name` (str): Extracts filename details without any directory structure from `relative_path`, creating meaningful labels for output files generated in course of processing (graph images, JSON/YAML data).'"
    },
    {
        "instruction": "Inputs to `write_file` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file.\n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n```",
        "output": "'data', 'file_path'\n\nPurpose and Significance:\nThe `write_file` function within the given script primarily focuses on saving data from a dictionary format into either JSON or YAML files based on their respective file extensions. It serves as a crucial component for managing output writing operations. The inputs of this utility function are specified as follows:\n\n1) 'data' (Dict): This argument represents the content to be saved within the target file, which could be a dictionary generated after data manipulations and preprocessing steps across multiple modules within `py2dataset` script execution flow. Once generated in-memory representation of dataset, `write_file` saves this content persistently for later reference or downstream applications usage.\n2) 'file_path' (Path): This input denotes the path to the file where the data will be written. It could be either JSON or YAML format depending on the suffix of the provided path ('file_type'). `write_file` uses this information to determine the output file type and write the data accordingly using suitable Python libraries ('json' module for JSON, 'yaml' module for YAML).\n\nThe significance of these inputs lies in their combined role in facilitating seamless storage of processed data from diverse sources into appropriate formats. `write_file` function acts as a bridge between memory-based representations and stable storage systems (filesystem), ensuring efficient utilization of the generated insights while maintaining data integrity throughout the workflow. This simplifies access to intermediate and final output management. /n"
    },
    {
        "instruction": "Docstring of `write_file` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file.\n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n```",
        "output": "Writes a dictionary to a JSON or YAML file.\n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None"
    },
    {
        "instruction": "Calls made in `write_file` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file.\n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n```",
        "output": "'file_path.open', 'json.dump', 'yaml.dump'\n\nPurpose and Significance:\nIn `write_file` function within the given script 'py2dataset\\save_output.py', three key method invocations are executed to write data into JSON or YAML files depending on their respective formats. These include:\n\n1) `file_path.open()`: This operation opens a file handler for reading/writing purposes in the `write_file` function with specified path ('file_path') as an argument. It allows accessing file contents during data manipulation and writing operations.\n\n2) `json.dump(data, f, indent=4)`: After opening the file using `file_path.open()`, this call serializes Python dictionary 'data' into JSON format by utilizing `json.dump()`. The optional argument 'indent=4' ensures readability with four spaces for better formatting while writing to the file.\n\n3) `yaml.SafeDumper.ignore_aliases = lambda *args: True` & `yaml.dump(data, F, Dumper=yaml.SafeDumper, sort_keys=False)`: In cases when dealing with a YAML file format ('file_type == \"yaml\"'), these statements prepare the data for writing by setting alias ignoring behavior using `yaml.SafeDumper`. Then `yaml.dump()` is used to write the given dictionary 'data' into the opened file handler ('f') in YAML format with disabled sorting of keys ('sort_keys=False'). This ensures preservation of original key order while writing data to the file.\n\nThese operations collectively contribute towards saving a dictionary as either JSON or YAML based on its extension (suffix), providing necessary serialization capabilities to handle distinct formats for diverse use cases within `write_file` functionality./n'''"
    },
    {
        "instruction": "Variables in `write_file` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file.\n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n```",
        "output": "'file_path', 'data', 'file_type'\n\nPurpose and Significance:\nIn the `write_file()` function within the `py2dataset\\save_output.py` script, three crucial variables play a role during writing dictionary data into either JSON or YAML formats depending upon file suffixes: 'file_path', 'data', and 'file_type'. Their specific contributions are elaborated as follows:\n\n1) file_path (Path): It acts as the destination path to store output after transforming given dictionary 'data' content into targeted format. The `write_file()` accepts this instance of `Path`, ensuring efficient accessibility by utilizing standard Python libraries' functionality such as handling relative and absolute paths with its various methods like opening, creating directories etc.\n\n2) data (Dict): This variable carries the main payload being processed to save in JSON or YAML format through the function. As `write_file()` serves to store input data of Python dictionaries as respective outputs depending upon the suffixes read from `file_path`, this dictionary undergoes the serialization operation either as 'json.dump(data)' for JSON files or yaml manipulation with SafeDumper ignoring aliases ('yaml.SafeDumper').\n\n3) file_type: This variable is derived from the suffix of `file_path` by slicing off its last characters until right before dot notation using [`file_path`.`suffix[1:]`, providing the crucial differentiator to understand if incoming content belongs to JSON ('json') or YAML ('yaml') type for choosing relevant conversion processes in `write_file()`. It helps determine how to handle data serialization accordingly. /n"
    },
    {
        "instruction": "Inputs to `read_file` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        if file_type == 'yaml':\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}\n```",
        "output": "'file_path'\n\nPurpose and Significance:\nIn the context of 'py2dataset\\save_output.py', `file_path` is a crucial input argument for the `read_file()` function within its defined scope. This parameter serves as a pathway pointing to either JSON or YAML files stored in the file system, directing `read_file()` on which file it should focus upon for reading contents into memory. In simple terms, `file_path` identifies an object containing vital location details to guide effective retrieval of data saved within supported formats like JSON or YAML. It plays a pivotal role in determining the file type ('file_type') and initiating the reading process using Python's built-in 'open()' function wrapped under 'with' statement, ensuring proper handling of file resources during operations. As `read_file()` progresses further to decode contents as a dictionary using 'json.load()' for JSON files or 'yaml.load()' for YAML ones, `file_path` sets the stage for accurate data processing towards specified outputs with potential future adjustments on diverse storage layouts accordingly. With both options incorporated as functional features within one routine -`read_file()`, 'file_path' brings an accessible degree of adaptability during operation which ensures ease when operating with distinct file types without code modification.'''"
    },
    {
        "instruction": "Docstring of `read_file` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        if file_type == 'yaml':\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}\n```",
        "output": "Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary."
    },
    {
        "instruction": "Calls made in `read_file` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        if file_type == 'yaml':\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}\n```",
        "output": "'file_path.open', 'json.load', 'yaml.load'\n\nPurpose and Significance:\nThe 'file_path.open' operation opens the given file path provided as an argument in a readable mode to access its contents within the 'read_file' function. It initiates communication between Python filesystem interactions and JSON/YAML loading procedures further down the script. Once opened, subsequent functions use this handler ('f') to extract data from either JSON or YAML format based on file suffix ('file_type').\n\nThe 'json.load(f)' call loads contents of a JSON file into Python dictionary format by parsing its string representation using JSON decoder. This ensures seamless integration with the rest of the codebase as Python objects can be easily manipulated and processed further. If the file is not in JSON format ('file_type != \"json\"'), 'yaml.load(f, Loader=yaml.SafeLoader)' function handles loading YAML files, a language-agnostic data serialization system used for human-readable data representation that's converted into a dictionary compatible with Python processing steps ahead. In summary, both operations serve to fetch required input data in suitable structures depending upon the file format from `file_path`. \n'''"
    },
    {
        "instruction": "Variables in `read_file` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        if file_type == 'yaml':\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}\n```",
        "output": "'file_path', 'file_type'\n\nPurpose and Significance:\nIn the `read_file()` function within the `py2dataset.save_output` module, two primary variables play crucial roles - `file_path` and `file_type`. These variables collectively govern the processing flow as they establish vital input data parameters during file manipulation. 'file_path', being a Path instance (represented by `Path(directory).open()` as argument for both reading operations in JSON and YAML formats, helps identify the specific location of files to be processed. It serves as a path reference that ensures seamless interaction with Python's standard library functions like `json.load()` or `yaml.load()`.\n\nOn the other hand, 'file_type', extracted from file_path suffix using `file_path.suffix[1:]`, is significant in determining the file format being read ('json' or 'yaml'). This information assists Python to apply appropriate methods for reading data accurately - `json.load()` for JSON files and `yaml.load(f, Loader=yaml.SafeLoader` for YAML files. Consequently, these variables collaborate to ensure versatile file reading functionalities with specific extensions as part of an organized mechanism while executing file input processing.  \n\nAs their workload implies different objectives ('Path identification' versus 'File type identification'), the Variables maintain intricate balancing that strengthens readability and flexibility in handling diverse data formats within the `read_file()` function.'''"
    },
    {
        "instruction": "Returns from `read_file` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        if file_type == 'yaml':\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}\n```",
        "output": "'{}', 'json.load(f)', 'yaml.load(f', 'Loader=yaml.SafeLoader)'\n\nPurpose and Significance:\nIn the provided `read_file` utility function within the `py2dataset` script, its primary objective is to retrieve the contents from JSON or YAML files as a Python dictionary format depending on their extensions. It operates upon receiving two essential arguments - `file_path`, representing the path to the file being processed, and returns the parsed data accordingly. The function starts by identifying the file type based on the suffix of `file_path` ('file_type' variable stores extension without '.'). Then it opens the specified file for reading through `Path(file_path).open()`, decoding JSON or YAML data with corresponding libraries: `json.load(f)` or `yaml.load(f, Loader=yaml.SafeLoader`. This process ensures safe loading of YAML files by ignoring aliases using `yaml.SafeDumper = lambda *args: True`. Overall, this function serves as a crucial data extraction tool for further processing in the script.{}\n\nPurpose explanation for each Return value from read_file():\n1) 'json.load(f)' - This return value corresponds to reading JSON files into Python dictionary format when `file_type` equals 'json'. It uses json library's load method to parse JSON data stored in the file opened by `Path(file_path).open()`.\n2) 'yaml.load(f', 'Loader=yaml.SafeLoader)' - When `file_type` indicates YAML extension ('yaml'), it loads the file content into a dictionary using yaml library's load function with SafeLoader for secure processing of potentially unsafe data structures. This ensures safe handling of YAML files during loading."
    },
    {
        "instruction": "Inputs to `preserve_spacing` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef preserve_spacing(text: str, tab_width: int=4) -> str:\n    \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n    return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n```",
        "output": "'text', 'tab_width'\n\nPurpose and Significance:\nThe 'text' parameter refers to the text string obtained from analyzing file metadata which could hold important content related to instruction details within the context of creating code documentation. This string contains relevant text describing 'file_purpose' from merged JSON datasets when consolidating them in `combine_json_files()`. The `preserve_spacing` function predominantly works with this parameter for handling white spaces and tabs within text preservation while generating HTML format for representing table elements neatly. This process ensures the readability of input data without altering its original formatting during conversion from JSON to HTML files using `convert_json_to_html()`.\n\nOn the other hand, 'tab_width' is an optional argument set with default value '4'. This attribute essentially denotes how wide column divisions must appear while preparing tables for every tab indentation noticed within parsed input content during its translation to HTML layout in `preserve_spacing()` function call within `convert_json_to_html()`. By determining column width percentage as a floating-point number derived from 'tab_width', this input plays an instrumental role in preserving tabs and spaces uniformly in HTML conversion.''"
    },
    {
        "instruction": "Docstring of `preserve_spacing` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef preserve_spacing(text: str, tab_width: int=4) -> str:\n    \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n    return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n```",
        "output": "Preserve spaces and tabs in the provided text."
    },
    {
        "instruction": "Calls made in `preserve_spacing` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef preserve_spacing(text: str, tab_width: int=4) -> str:\n    \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n    return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n```",
        "output": "'text.replace(', '&nbsp;').replace', 'text.replace'\n\nPurpose and Significance:\nThe 'preserve_spacing' function serves a crucial role within the 'convert_json_to_html' utility during HTML conversion of JSON files in the given script. Its primary purpose is to maintain original spacing and tabulations present in the input text while transforming it into an HTML format. This preservation ensures that formatted data retains its readability after conversion. The function takes two arguments - 'text' as input string content and 'tab_width' as optional integer argument with default value 4. It performs three replacements on the given text to achieve this goal:\n\n1. Replaces newline characters ('\\n') with HTML non-breaking space entities ('&nbsp;'). This ensures line breaks are rendered as spaces in HTML output without disrupting the visual layout of tabulated data.\n2. Transforms tabs ('\\t') into equivalent sequences of '&nbsp;' based on the specified 'tab_width'. By replacing tabs with multiple non-breaking spaces, the original indentation structure is preserved even after translation to HTML format. This feature aids readability in tables by ensuring appropriate column width distribution across rows when rendered on screen or printed pages.\n\nThe resulting string containing substituted entities maintains white space characteristics while presenting well-structured tabular data in an HTML table, enabling easier human interpretation compared to plain JSON format output. Overall, 'preserve_spacing' guarantees visual fidelity between the input JSON files and generated HTML outputs while enhancing legibility. This helps users analyze the data more efficiently when viewing converted HTML files."
    },
    {
        "instruction": "Variables in `preserve_spacing` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef preserve_spacing(text: str, tab_width: int=4) -> str:\n    \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n    return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n```",
        "output": "'tab_width', 'text'\n\nPurpose and Significance:\nThe 'tab_width' variable holds an integer value assigned as 4 within the `preserve_spacing()` function. It serves a crucial role in maintaining consistent indentation while converting JSON files to HTML format by replacing tabs with spaces equivalent to its width during text transformation. This ensures proper visual representation of tabbed structures in HTML tables without losing their original formatting. On the other hand, 'text' is a string parameter passed into `preserve_spacing()` which gets processed alongside 'tab_width'. It represents the input text that requires preservation of spaces and tabs during conversion to HTML content creation within the `convert_json_to_html()`. This variable stores JSON data lines before replacement in HTML format where it's escaped using HTML entities ('&nbsp;') and newline characters ('\\n') substituted with <br/> tags for line breaks. By doing so, it retains code formatting elements even when converting tabular JSON representations to web page friendly displays.'''"
    },
    {
        "instruction": "Returns from `preserve_spacing` in Python file: `py2dataset\\save_output.py`?",
        "input": "```python\ndef preserve_spacing(text: str, tab_width: int=4) -> str:\n    \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n    return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n```",
        "output": "'text.replace(', '&nbsp;').replace('\\\\t', '&nbsp;' * tab_width)'\n\nPurpose and Significance:\nThe `preserve_spacing` function within the provided codebase serves a crucial role in maintaining original spacing and tabs while transforming text content into HTML format suitable for display purposes. It is primarily utilized during conversion of JSON files to HTML format through the `convert_json_to_html()` method. This function accepts two arguments - `text`, representing the input string, and an optional `tab_width` parameter with a default value of 4. The function substitutes carriage return characters ('\\n') in text strings with '&nbsp;' entities ensuring whitespace is rendered identically on HTML webpages, similarly transforming tabulation symbols (\\t) to an equivalent set of spaces based upon `tab_width`. Thus, `preserve_spacing()` preserves formatting essential for code representations within tables and overall readability while retaining spacing structure during HTML generation from JSON files. \n\nThis specific manipulation enhances visual appeal and aids users in comprehending complex data more easily by preserving whitespace integrity that might otherwise be lost during conversion between formats. By doing so, it helps bridge the gap between textual code representation and its presentation on webpages, making HTML output more legible for human readers analyzing JSON content. \n\nIn `convert_json_to_html()`, this function is applied to each entry's value within a dataset while generating HTML rows for table cells ('value = preserve_spacing(value)'). This ensures that when rendering tables from JSON data, spaces and tabs are accurately translated into HTML entities, enabling proper display of Python code documentation in an organized manner. \n\nIn summary, `preserve_spacing()` plays a vital role in retaining whitespace information during HTML conversion processes within the codebase to ensure readability when presenting JSON files as webpages. It guarantees accurate representation of original formatting details crucial for users analyzing complex data structures like Python code snippets."
    }
]