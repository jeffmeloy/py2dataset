file_info:
    file_code: "import os\nimport sys\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, List\nfrom multiprocessing import Process\nfrom git import Repo, GitCommandError\n\nfrom get_python_file_details import get_python_file_details\nfrom get_python_datasets import get_python_datasets\nfrom get_params import get_questions, get_model, get_output_dir, get_start_dir\nfrom save_output import combine_json_files, save_python_data\n\n\ndef process_single_python_file(params: Dict) -> None:\n    \"\"\"Processes a single Python file to generate question-answer pairs and instructions.\"\"\"\n\n    logging.info(f\"Processing file: {params['python_pathname']}\")\n    file_details = get_python_file_details(params[\"python_pathname\"])\n    if not file_details:\n        logging.error(f\"Failed to get file details for {params['python_pathname']}\")\n        return\n\n    if params[\"model_config\"] is None and params[\"use_llm\"]:\n        params[\"model_config\"] = get_model(params[\"model_config_pathname\"])\n\n    instruct_data = get_python_datasets(\n        params[\"python_pathname\"],\n        file_details,\n        params[\"relative_path\"],\n        params[\"questions\"],\n        params[\"model_config\"],\n        params[\"detailed\"],\n    )\n\n    if instruct_data:\n        save_python_data(\n            file_details, instruct_data, params[\"relative_path\"], params[\"output_dir\"]\n        )\n    else:\n        logging.error(f\"Failed getting {params['python_pathname']} dataset\")\n\n\ndef py2dataset(\n    start: str = \"\",\n    output_dir: str = \"\",\n    questions_pathname: str = \"\",\n    model_config_pathname: str = \"\",\n    use_llm: bool = False,\n    quiet: bool = False,\n    single_process: bool = False,\n    detailed: bool = False,\n    html: bool = False,\n    skip_regen: bool = False,\n) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Generates datasets by processing Python files within a specified directory.\n    Args:\n        start (str, optional): Starting directory for Python files or GitHub repository Python files. Default: current working directory.\n        output_dir (str, optional): Directory to write the output files. Default: ./dataset/.\n        questions_pathname (str, optional): Path and filename of the questions file. Default: ./py2dataset_questions.json.\n        model_config_pathname (str, optional): Path and filename of the model configuration file. Default: ./py2dataset_model_config.yaml.\n        use_llm (bool, optional): Use llm to answer code purpose question. Default: False.\n        quiet (bool, optional): Limit logging output. Default: False.\n        single_process (bool, optional): Use a single process to process Python files if --use_llm. Default: False.\n        detailed (bool, optional): Include detailed analysis. Default: False.\n        html (bool, optional): Generate HTML output. Default: False.\n        skip_regen (bool, optional): Skip regeneration of existing instruct.json files. Default: False.\n    Returns:\n        Dict[str, List[Dict]]: Generated datasets.\n    \"\"\"\n    sys.setrecursionlimit(3000)  # Set recursion limit higher for AST parsing\n    model_config = (\n        get_model(model_config_pathname) if use_llm and single_process else None\n    )\n\n    logging.getLogger().setLevel(logging.WARNING if quiet else logging.INFO)\n\n    params = {\n        \"output_dir\": get_output_dir(output_dir),\n        \"model_config_pathname\": model_config_pathname,\n        \"questions\": get_questions(questions_pathname),\n        \"use_llm\": use_llm,\n        \"model_config\": model_config,\n        \"detailed\": detailed,\n    }\n\n    exclude_dirs = [\"env\", \"venv\", \"__pycache__\", \"build\", \"dist\"]\n\n    for python_pathname in Path(start).rglob(\"*.py\"):\n        if (\n            any(dir in python_pathname.parts for dir in exclude_dirs)\n            or python_pathname.name == \"__init__.py\"\n        ):\n            continue\n\n        params[\"python_pathname\"] = str(python_pathname)\n        params[\"relative_path\"] = Path(\n            os.path.relpath(python_pathname, os.path.dirname(get_start_dir(start)))\n        )\n        instruct_pathname = Path(params[\"output_dir\"]) / params[\n            \"relative_path\"\n        ].with_suffix(\".py.instruct.json\")\n\n        if instruct_pathname.exists() and skip_regen:\n            continue\n\n        if model_config is None and use_llm:\n            # Process each Python file in a separate process to manage memory\n            process = Process(target=process_single_python_file, args=(params,))\n            process.start()\n            process.join()\n            process.close()\n        else:\n            # Process all files using a single process\n            process_single_python_file(params)\n\n    return combine_json_files(output_dir, html, params[\"questions\"])\n\n\ndef clone_github_repo(url: str) -> str:\n    \"\"\"Clone repository or fetch the latest changes and return local repository path.\"\"\"\n    try:\n        repo_name = Path(url).stem\n        repo_path = Path.cwd() / \"githubrepos\" / repo_name\n        if repo_path.exists():\n            repo = Repo(repo_path)\n            repo.remote().fetch()\n            repo.git.reset(\"--hard\", repo.heads[0].commit)\n        else:\n            Repo.clone_from(url, repo_path)\n        return str(repo_path)\n    except GitCommandError as e:\n        logging.info(f\"Error processing repository {url}: {e}\")\n        return \"\"\n\n\ndef get_bool_from_input(input_str: str, current_value: bool) -> bool:\n    \"\"\"Return boolean value based on the user input.\"\"\"\n    if input_str.lower() in [\"t\", \"true\", \"y\", \"yes\"]:\n        return True\n    elif input_str.lower() in [\"f\", \"false\", \"n\", \"no\"]:\n        return False\n    return current_value\n\n\ndef main():\n    \"\"\"\n    Command-line entry point for processing Python files and generating datasets.\n    Usage:\n        python py2dataset.py [options]\n    Options:\n        -h, --help: Show this help message and exit.\n        --start: Starting directory for Python files or GitHub repository Python files. Default: current working directory.\n        --output_dir: Directory to write the output files. Default: ./dataset/.\n        --questions_pathname: Path and filename of the questions file. Default: ./py2dataset_questions.json.\n        --model_config_pathname: Path and filename of the model configuration file. Default: ./py2dataset_model_config.yaml.\n        --use_llm: Use llm to answer code purpose question. Default: False.\n        --quiet: Limit logging output. Default: False.\n        --single_process: Use a single process to process Python files if --use_llm. Default: False.\n        --detailed: Include detailed analysis. Default: False.\n        --html: Generate HTML output. Default: False.\n        --skip_regen: Skip regeneration of existing instruct.json files. Default: False.\n        --I: Interactive mode to enter new values.\n    \"\"\"\n    if len(sys.argv) == 1 or \"-h\" in sys.argv or \"--help\" in sys.argv:\n        print(__doc__)\n        sys.exit()\n\n    params = {\n        \"start\": \".\",\n        \"output_dir\": \"./dataset/\",\n        \"questions_pathname\": \"./py2dataset_questions.json\",\n        \"model_config_pathname\": \"./py2dataset_model_config.yaml\",\n        \"use_llm\": False,\n        \"quiet\": False,\n        \"single_process\": False,\n        \"detailed\": False,\n        \"html\": False,\n        \"skip_regen\": False,\n        \"I\": False,\n    }\n\n    arg_string = \" \".join(sys.argv[1:])\n    for arg, value in params.items():\n        if f\"--{arg}\" in arg_string:\n            if isinstance(value, bool):\n                params[arg] = True\n                arg_string = arg_string.replace(f\"--{arg}\", \"\")\n            else:\n                value_segment = arg_string.split(f\"--{arg} \")[1]\n                params[arg] = value_segment.split(\" --\")[0].strip('\"')\n                arg_string = arg_string.replace(f\"--{arg} {params[arg]}\", \"\")\n\n    if params.pop(\"I\"):\n        print(\"Interactive mode, enter new values or press enter to keep.\")\n        for arg, value in params.items():\n            user_input = input(f\"{arg} [{value}]: \").strip()\n            params[arg] = (\n                get_bool_from_input(user_input, value)\n                if isinstance(value, bool)\n                else user_input or value\n            )\n            print(f\"{arg}: {params[arg]}\")\n\n    params[\"start\"] = (\n        clone_github_repo(params[\"start\"])\n        if params[\"start\"].startswith(\"https://github.com/\")\n        else params[\"start\"]\n        if os.path.isdir(params[\"start\"])\n        else os.getcwd()\n    )\n\n    py2dataset(**params)\n\n\nif __name__ == \"__main__\":\n    main()\n"
    file_dependencies:
    - typing
    - get_python_datasets
    - multiprocessing
    - get_params
    - get_python_file_details
    - save_output
    - os
    - git
    - logging
    - sys
    - pathlib
    file_functions:
    - process_single_python_file
    - py2dataset
    - clone_github_repo
    - get_bool_from_input
    - main
    file_classes: []
    file_constants:
    - file_details=get_python_file_details(params['python_pathname'])
    - instruct_data=get_python_datasets(params['python_pathname'], file_details, params['relative_path'], params['questions'], params['model_config'], params['detailed'])
    - model_config=get_model(model_config_pathname) if use_llm and single_process else None
    - 'params={''output_dir'': get_output_dir(output_dir), ''model_config_pathname'': model_config_pathname, ''questions'': get_questions(questions_pathname), ''use_llm'': use_llm, ''model_config'': model_config, ''detailed'': detailed}'
    - exclude_dirs=['env', 'venv', '__pycache__', 'build', 'dist']
    - instruct_pathname=Path(params['output_dir']) / params['relative_path'].with_suffix('.py.instruct.json')
    - process=Process(target=process_single_python_file, args=(params,))
    - repo_name=Path(url).stem
    - repo_path=Path.cwd() / 'githubrepos' / repo_name
    - repo=Repo(repo_path)
    - 'params={''start'': ''.'', ''output_dir'': ''./dataset/'', ''questions_pathname'': ''./py2dataset_questions.json'', ''model_config_pathname'': ''./py2dataset_model_config.yaml'', ''use_llm'': False, ''quiet'': False, ''single_process'': False, ''detailed'': False, ''html'': False, ''skip_regen'': False, ''I'': False}'
    - arg_string=' '.join(sys.argv[1:])
    - arg_string=arg_string.replace(f'--{arg}', '')
    - value_segment=arg_string.split(f'--{arg} ')[1]
    - arg_string=arg_string.replace(f'--{arg} {params[arg]}', '')
    - 'user_input=input(f''{arg} [{value}]: '').strip()'
    file_summary: '{dependencies: [typing, get_python_datasets, multiprocessing, get_params, get_python_file_details, save_output, os, git, logging, sys, pathlib], function_defs: [{process_single_python_file: {inputs: [params], calls: [logging.info, get_python_file_details, logging.error, get_model, get_python_datasets, save_python_data], call_inputs: {logging.info: [f\Processing file: {params[''python_pathname'']}\], get_python_file_details: [params[''python_pathname'']], logging.error: [f\Failed getting {params[''python_pathname'']} dataset\], get_model: [params[''model_config_pathname'']], get_python_datasets: [params[''python_pathname''], file_details, params[''relative_path''], params[''questions''], params[''model_config''], params[''detailed'']], save_python_data: [file_details, instruct_data, params[''relative_path''], params[''output_dir'']]}, returns: [None]}}, {py2dataset: {inputs: [start, output_dir, questions_pathname, model_config_pathname, use_llm, quiet, single_process, detailed, html, skip_regen], calls: [sys.setrecursionlimit, get_model, logging.getLogger().setLevel, logging.getLogger, get_output_dir, get_questions, Path(start).rglob, Path, any, str, os.path.relpath, os.path.dirname, get_start_dir, params[''relative_path''].with_suffix, instruct_pathname.exists, Process, process.start, process.join, process.close, process_single_python_file, combine_json_files], call_inputs: {sys.setrecursionlimit: [3000], get_model: [model_config_pathname], logging.getLogger().setLevel: [logging.WARNING if quiet else logging.INFO], logging.getLogger: [], get_output_dir: [output_dir], get_questions: [questions_pathname], Path(start).rglob: [''*.py''], Path: [params[''output_dir'']], any: [(dir in python_pathname.parts for dir in exclude_dirs)], str: [python_pathname], os.path.relpath: [python_pathname, os.path.dirname(get_start_dir(start))], os.path.dirname: [get_start_dir(start)], get_start_dir: [start], params[''relative_path''].with_suffix: [''.py.instruct.json''], instruct_pathname.exists: [], Process: [], process.start: [], process.join: [], process.close: [], process_single_python_file: [params], combine_json_files: [output_dir, html, params[''questions'']]}, returns: [combine_json_files(output_dir, html, params[''questions''])]}}, {clone_github_repo: {inputs: [url], calls: [Path, Path.cwd, repo_path.exists, Repo, repo.remote().fetch, repo.remote, repo.git.reset, Repo.clone_from, str, logging.info], call_inputs: {Path: [url], Path.cwd: [], repo_path.exists: [], Repo: [repo_path], repo.remote().fetch: [], repo.remote: [], repo.git.reset: [''--hard'', repo.heads[0].commit], Repo.clone_from: [url, repo_path], str: [repo_path], logging.info: [f''Error processing repository {url}: {e}'']}, returns: [str(repo_path), '''']}}, {get_bool_from_input: {inputs: [input_str, current_value], calls: [input_str.lower], call_inputs: {input_str.lower: []}, returns: [current_value, True, False]}}, {main: {inputs: [], calls: [len, print, sys.exit, '' ''.join, params.items, isinstance, arg_string.replace, arg_string.split, value_segment.split('' --'')[0].strip, value_segment.split, params.pop, input(f''{arg} [{value}]: '').strip, input, get_bool_from_input, params[''start''].startswith, clone_github_repo, os.path.isdir, os.getcwd, py2dataset], call_inputs: {len: [sys.argv], print: [f''{arg}: {params[arg]}''], sys.exit: [], '' ''.join: [sys.argv[1:]], params.items: [], isinstance: [value, bool], arg_string.replace: [f''--{arg} {params[arg]}'', ''''], arg_string.split: [f''--{arg} ''], value_segment.split('' --'')[0].strip: [''\''], value_segment.split: ['' --''], params.pop: [''I''], input(f''{arg} [{value}]: '').strip: [], input: [f''{arg} [{value}]: ''], get_bool_from_input: [user_input, value], params[''start''].startswith: [''https://github.com/''], clone_github_repo: [params[''start'']], os.path.isdir: [params[''start'']], os.getcwd: [], py2dataset: []}, returns: []}}], class_defs: []}'
    file_code_simplified: "import os\nimport sys\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, List\nfrom multiprocessing import Process\nfrom git import Repo, GitCommandError\nfrom get_python_file_details import get_python_file_details\nfrom get_python_datasets import get_python_datasets\nfrom get_params import get_questions, get_model, get_output_dir, get_start_dir\nfrom save_output import combine_json_files, save_python_data\n\ndef process_single_python_file(params: Dict) -> None:\n    logging.info(f\"Processing file: {params['python_pathname']}\")\n    file_details = get_python_file_details(params['python_pathname'])\n    if not file_details:\n        logging.error(f\"Failed to get file details for {params['python_pathname']}\")\n        return\n    if params['model_config'] is None and params['use_llm']:\n        params['model_config'] = get_model(params['model_config_pathname'])\n    instruct_data = get_python_datasets(params['python_pathname'], file_details, params['relative_path'], params['questions'], params['model_config'], params['detailed'])\n    if instruct_data:\n        save_python_data(file_details, instruct_data, params['relative_path'], params['output_dir'])\n    else:\n        logging.error(f\"Failed getting {params['python_pathname']} dataset\")\n\ndef py2dataset(start: str='', output_dir: str='', questions_pathname: str='', model_config_pathname: str='', use_llm: bool=False, quiet: bool=False, single_process: bool=False, detailed: bool=False, html: bool=False, skip_regen: bool=False) -> Dict[str, List[Dict]]:\n    sys.setrecursionlimit(3000)\n    model_config = get_model(model_config_pathname) if use_llm and single_process else None\n    logging.getLogger().setLevel(logging.WARNING if quiet else logging.INFO)\n    params = {'output_dir': get_output_dir(output_dir), 'model_config_pathname': model_config_pathname, 'questions': get_questions(questions_pathname), 'use_llm': use_llm, 'model_config': model_config, 'detailed': detailed}\n    exclude_dirs = ['env', 'venv', '__pycache__', 'build', 'dist']\n    for python_pathname in Path(start).rglob('*.py'):\n        if any((dir in python_pathname.parts for dir in exclude_dirs)) or python_pathname.name == '__init__.py':\n            continue\n        params['python_pathname'] = str(python_pathname)\n        params['relative_path'] = Path(os.path.relpath(python_pathname, os.path.dirname(get_start_dir(start))))\n        instruct_pathname = Path(params['output_dir']) / params['relative_path'].with_suffix('.py.instruct.json')\n        if instruct_pathname.exists() and skip_regen:\n            continue\n        if model_config is None and use_llm:\n            process = Process(target=process_single_python_file, args=(params,))\n            process.start()\n            process.join()\n            process.close()\n        else:\n            process_single_python_file(params)\n    return combine_json_files(output_dir, html, params['questions'])\n\ndef clone_github_repo(url: str) -> str:\n    try:\n        repo_name = Path(url).stem\n        repo_path = Path.cwd() / 'githubrepos' / repo_name\n        if repo_path.exists():\n            repo = Repo(repo_path)\n            repo.remote().fetch()\n            repo.git.reset('--hard', repo.heads[0].commit)\n        else:\n            Repo.clone_from(url, repo_path)\n        return str(repo_path)\n    except GitCommandError as e:\n        logging.info(f'Error processing repository {url}: {e}')\n        return ''\n\ndef get_bool_from_input(input_str: str, current_value: bool) -> bool:\n    if input_str.lower() in ['t', 'true', 'y', 'yes']:\n        return True\n    elif input_str.lower() in ['f', 'false', 'n', 'no']:\n        return False\n    return current_value\n\ndef main():\n    if len(sys.argv) == 1 or '-h' in sys.argv or '--help' in sys.argv:\n        print(__doc__)\n        sys.exit()\n    params = {'start': '.', 'output_dir': './dataset/', 'questions_pathname': './py2dataset_questions.json', 'model_config_pathname': './py2dataset_model_config.yaml', 'use_llm': False, 'quiet': False, 'single_process': False, 'detailed': False, 'html': False, 'skip_regen': False, 'I': False}\n    arg_string = ' '.join(sys.argv[1:])\n    for arg, value in params.items():\n        if f'--{arg}' in arg_string:\n            if isinstance(value, bool):\n                params[arg] = True\n                arg_string = arg_string.replace(f'--{arg}', '')\n            else:\n                value_segment = arg_string.split(f'--{arg} ')[1]\n                params[arg] = value_segment.split(' --')[0].strip('\"')\n                arg_string = arg_string.replace(f'--{arg} {params[arg]}', '')\n    if params.pop('I'):\n        print('Interactive mode, enter new values or press enter to keep.')\n        for arg, value in params.items():\n            user_input = input(f'{arg} [{value}]: ').strip()\n            params[arg] = get_bool_from_input(user_input, value) if isinstance(value, bool) else user_input or value\n            print(f'{arg}: {params[arg]}')\n    params['start'] = clone_github_repo(params['start']) if params['start'].startswith('https://github.com/') else params['start'] if os.path.isdir(params['start']) else os.getcwd()\n    py2dataset(**params)\nif __name__ == '__main__':\n    main()"
    entire_code_graph:
        nodes:
        - process_single_python_file
        - py2dataset
        - clone_github_repo
        - get_bool_from_input
        - main
        - logging.info
        - get_python_file_details
        - logging.error
        - get_model
        - get_python_datasets
        - save_python_data
        - sys.setrecursionlimit
        - logging.getLogger().setLevel
        - logging.getLogger
        - get_output_dir
        - get_questions
        - Path(start).rglob
        - Path
        - any
        - str
        - os.path.relpath
        - os.path.dirname
        - get_start_dir
        - params['relative_path'].with_suffix
        - instruct_pathname.exists
        - Process
        - process.start
        - process.join
        - process.close
        - combine_json_files
        - Path.cwd
        - repo_path.exists
        - Repo
        - repo.remote().fetch
        - repo.remote
        - repo.git.reset
        - Repo.clone_from
        - input_str.lower
        - len
        - print
        - sys.exit
        - ''' ''.join'
        - params.items
        - isinstance
        - arg_string.replace
        - arg_string.split
        - value_segment.split(' --')[0].strip
        - value_segment.split
        - params.pop
        - 'input(f''{arg} [{value}]: '').strip'
        - input
        - params['start'].startswith
        - os.path.isdir
        - os.getcwd
        edges:
        -   source: process_single_python_file
            target: logging.info
            target_inputs:
            - 'f"Processing file: {params[''python_pathname'']}"'
        -   source: process_single_python_file
            target: get_python_file_details
            target_inputs:
            - params['python_pathname']
        -   source: process_single_python_file
            target: logging.error
            target_inputs:
            - f"Failed getting {params['python_pathname']} dataset"
        -   source: process_single_python_file
            target: get_model
            target_inputs:
            - params['model_config_pathname']
        -   source: process_single_python_file
            target: get_python_datasets
            target_inputs:
            - params['python_pathname']
            - file_details
            - params['relative_path']
            - params['questions']
            - params['model_config']
            - params['detailed']
        -   source: process_single_python_file
            target: save_python_data
            target_inputs:
            - file_details
            - instruct_data
            - params['relative_path']
            - params['output_dir']
        -   source: py2dataset
            target: sys.setrecursionlimit
            target_inputs:
            - '3000'
        -   source: py2dataset
            target: get_model
            target_inputs:
            - model_config_pathname
        -   source: py2dataset
            target: logging.getLogger().setLevel
            target_inputs:
            - logging.WARNING if quiet else logging.INFO
        -   source: py2dataset
            target: logging.getLogger
            target_inputs: []
        -   source: py2dataset
            target: get_output_dir
            target_inputs:
            - output_dir
        -   source: py2dataset
            target: get_questions
            target_inputs:
            - questions_pathname
        -   source: py2dataset
            target: Path(start).rglob
            target_inputs:
            - '''*.py'''
        -   source: py2dataset
            target: Path
            target_inputs:
            - params['output_dir']
        -   source: py2dataset
            target: any
            target_inputs:
            - (dir in python_pathname.parts for dir in exclude_dirs)
        -   source: py2dataset
            target: str
            target_inputs:
            - python_pathname
        -   source: py2dataset
            target: os.path.relpath
            target_inputs:
            - python_pathname
            - os.path.dirname(get_start_dir(start))
        -   source: py2dataset
            target: os.path.dirname
            target_inputs:
            - get_start_dir(start)
        -   source: py2dataset
            target: get_start_dir
            target_inputs:
            - start
        -   source: py2dataset
            target: params['relative_path'].with_suffix
            target_inputs:
            - '''.py.instruct.json'''
        -   source: py2dataset
            target: instruct_pathname.exists
            target_inputs: []
        -   source: py2dataset
            target: Process
            target_inputs: []
        -   source: py2dataset
            target: process.start
            target_inputs: []
        -   source: py2dataset
            target: process.join
            target_inputs: []
        -   source: py2dataset
            target: process.close
            target_inputs: []
        -   source: py2dataset
            target: process_single_python_file
            target_inputs:
            - params
            target_returns:
            - None
        -   source: py2dataset
            target: combine_json_files
            target_inputs:
            - output_dir
            - html
            - params['questions']
        -   source: clone_github_repo
            target: Path
            target_inputs:
            - url
        -   source: clone_github_repo
            target: Path.cwd
            target_inputs: []
        -   source: clone_github_repo
            target: repo_path.exists
            target_inputs: []
        -   source: clone_github_repo
            target: Repo
            target_inputs:
            - repo_path
        -   source: clone_github_repo
            target: repo.remote().fetch
            target_inputs: []
        -   source: clone_github_repo
            target: repo.remote
            target_inputs: []
        -   source: clone_github_repo
            target: repo.git.reset
            target_inputs:
            - '''--hard'''
            - repo.heads[0].commit
        -   source: clone_github_repo
            target: Repo.clone_from
            target_inputs:
            - url
            - repo_path
        -   source: clone_github_repo
            target: str
            target_inputs:
            - repo_path
        -   source: clone_github_repo
            target: logging.info
            target_inputs:
            - 'f''Error processing repository {url}: {e}'''
        -   source: get_bool_from_input
            target: input_str.lower
            target_inputs: []
        -   source: main
            target: len
            target_inputs:
            - sys.argv
        -   source: main
            target: print
            target_inputs:
            - 'f''{arg}: {params[arg]}'''
        -   source: main
            target: sys.exit
            target_inputs: []
        -   source: main
            target: ''' ''.join'
            target_inputs:
            - sys.argv[1:]
        -   source: main
            target: params.items
            target_inputs: []
        -   source: main
            target: isinstance
            target_inputs:
            - value
            - bool
        -   source: main
            target: arg_string.replace
            target_inputs:
            - f'--{arg} {params[arg]}'
            - ''''''
        -   source: main
            target: arg_string.split
            target_inputs:
            - f'--{arg} '
        -   source: main
            target: value_segment.split(' --')[0].strip
            target_inputs:
            - '''"'''
        -   source: main
            target: value_segment.split
            target_inputs:
            - ''' --'''
        -   source: main
            target: params.pop
            target_inputs:
            - '''I'''
        -   source: main
            target: 'input(f''{arg} [{value}]: '').strip'
            target_inputs: []
        -   source: main
            target: input
            target_inputs:
            - 'f''{arg} [{value}]: '''
        -   source: main
            target: get_bool_from_input
            target_inputs:
            - user_input
            - value
            target_returns:
            - 'False'
            - 'True'
            - current_value
        -   source: main
            target: params['start'].startswith
            target_inputs:
            - '''https://github.com/'''
        -   source: main
            target: clone_github_repo
            target_inputs:
            - params['start']
            target_returns:
            - ''''''
            - str(repo_path)
        -   source: main
            target: os.path.isdir
            target_inputs:
            - params['start']
        -   source: main
            target: os.getcwd
            target_inputs: []
        -   source: main
            target: py2dataset
            target_inputs: []
            target_returns:
            - combine_json_files(output_dir, html, params['questions'])
    control_flow_structure:
    - import os
    - import sys
    - import logging
    - from pathlib import Path
    - from typing import Dict, List
    - from multiprocessing import Process
    - from git import Repo, GitCommandError
    - from get_python_file_details import get_python_file_details
    - from get_python_datasets import get_python_datasets
    - from get_params import get_questions, get_model, get_output_dir, get_start_dir
    - from save_output import combine_json_files, save_python_data
    -   def main():
        -   if len(sys.argv) == 1 or '-h' in sys.argv or '--help' in sys.argv:
            - print(__doc__)
            - sys.exit()
        - 'params = {''start'': ''.'', ''output_dir'': ''./dataset/'', ''questions_pathname'': ''./py2dataset_questions.json'', ''model_config_pathname'': ''./py2dataset_model_config.yaml'', ''use_llm'': False, ''quiet'': False, ''single_process'': False, ''detailed'': False, ''html'': False, ''skip_regen'': False, ''I'': False}'
        - arg_string = ' '.join(sys.argv[1:])
        -   for (arg, value) in params.items():
            -   if f'--{arg}' in arg_string:
                -   if isinstance(value, bool):
                    - params[arg] = True
                    - arg_string = arg_string.replace(f'--{arg}', '')
                    else:
                    - value_segment = arg_string.split(f'--{arg} ')[1]
                    - params[arg] = value_segment.split(' --')[0].strip('"')
                    - arg_string = arg_string.replace(f'--{arg} {params[arg]}', '')
        -   if params.pop('I'):
            - print('Interactive mode, enter new values or press enter to keep.')
            -   for (arg, value) in params.items():
                - 'user_input = input(f''{arg} [{value}]: '').strip()'
                - params[arg] = get_bool_from_input(user_input, value) if isinstance(value, bool) else user_input or value
                - 'print(f''{arg}: {params[arg]}'')'
        - params['start'] = clone_github_repo(params['start']) if params['start'].startswith('https://github.com/') else params['start'] if os.path.isdir(params['start']) else os.getcwd()
        - py2dataset(**params)
    -   if __name__ == '__main__':
        - main()
    -   ? 'def py2dataset(start: str, output_dir: str, questions_pathname: str, model_config_pathname: str, use_llm: bool, quiet: bool, single_process: bool, detailed: bool, html: bool, skip_regen: bool)'
        :   - sys.setrecursionlimit(3000)
            - model_config = get_model(model_config_pathname) if use_llm and single_process else None
            - logging.getLogger().setLevel(logging.WARNING if quiet else logging.INFO)
            - 'params = {''output_dir'': get_output_dir(output_dir), ''model_config_pathname'': model_config_pathname, ''questions'': get_questions(questions_pathname), ''use_llm'': use_llm, ''model_config'': model_config, ''detailed'': detailed}'
            - exclude_dirs = ['env', 'venv', '__pycache__', 'build', 'dist']
            -   for python_pathname in Path(start).rglob('*.py'):
                -   if any((dir in python_pathname.parts for dir in exclude_dirs)) or python_pathname.name == '__init__.py':
                    - continue
                - params['python_pathname'] = str(python_pathname)
                - params['relative_path'] = Path(os.path.relpath(python_pathname, os.path.dirname(get_start_dir(start))))
                - instruct_pathname = Path(params['output_dir']) / params['relative_path'].with_suffix('.py.instruct.json')
                -   if instruct_pathname.exists() and skip_regen:
                    - continue
                -   if model_config is None and use_llm:
                    - process = Process(target=process_single_python_file, args=(params,))
                    - process.start()
                    - process.join()
                    - process.close()
                    else:
                    - process_single_python_file(params)
            -   return:
                - combine_json_files(output_dir, html, params['questions'])
    -   'def process_single_python_file(params: Dict)':
        - 'logging.info(f"Processing file: {params[''python_pathname'']}")'
        - file_details = get_python_file_details(params['python_pathname'])
        -   if not file_details:
            - logging.error(f"Failed to get file details for {params['python_pathname']}")
            -   return: []
        -   if params['model_config'] is None and params['use_llm']:
            - params['model_config'] = get_model(params['model_config_pathname'])
        - instruct_data = get_python_datasets(params['python_pathname'], file_details, params['relative_path'], params['questions'], params['model_config'], params['detailed'])
        -   if instruct_data:
            - save_python_data(file_details, instruct_data, params['relative_path'], params['output_dir'])
            else:
            - logging.error(f"Failed getting {params['python_pathname']} dataset")
    -   'def clone_github_repo(url: str)':
        -   try:
            - repo_name = Path(url).stem
            - repo_path = Path.cwd() / 'githubrepos' / repo_name
            -   if repo_path.exists():
                - repo = Repo(repo_path)
                - repo.remote().fetch()
                - repo.git.reset('--hard', repo.heads[0].commit)
                else:
                - Repo.clone_from(url, repo_path)
            -   return:
                - str(repo_path)
            except:
            -   'except GitCommandError as :':
                - 'logging.info(f''Error processing repository {url}: {e}'')'
                -   return:
                    - ''''''
    -   'def get_bool_from_input(input_str: str, current_value: bool)':
        -   if input_str.lower() in ['t', 'true', 'y', 'yes']:
            -   return:
                - 'True'
            elif input_str.lower() in ['f', 'false', 'n', 'no']:
            -   return:
                - 'False'
        -   return:
            - current_value
    plant_uml: "@startuml\nstart\n:import os;\n:import sys;\n:import logging;\n:from pathlib import Path;\n:from typing import Dict, List;\n:from multiprocessing import Process;\n:from git import Repo, GitCommandError;\n:from get_python_file_details import get_python_file_details;\n:from get_python_datasets import get_python_datasets;\n:from get_params import get_questions, get_model, get_output_dir, get_start_dir;\n:from save_output import combine_json_files, save_python_data;\n:def main();\n    if (len(sys.argv) == 1 or '-h' in sys.argv or '--help' in sys.argv) then;\n        :print(__doc__);\n        :sys.exit();\n    endif;\n    :params = {'start': '.', 'output_dir': './dataset/', 'questions_pathname': './py2dataset_questions.json', 'model_config_pathname': './py2dataset_model_config.yaml', 'use_llm': False, 'quiet': False, 'single_process': False, 'detailed': False, 'html': False, 'skip_regen': False, 'I': False};\n    :arg_string = ' '.join(sys.argv[1:]);\n    if ((arg, value) in params.items()) then;\n        if (f'--{arg}' in arg_string) then;\n            if (isinstance(value, bool)) then;\n                :params[arg] = True;\n                :arg_string = arg_string.replace(f'--{arg}', '');\n            endif;\n        endif;\n    endif;\n    if (params.pop('I')) then;\n        :print('Interactive mode, enter new values or press enter to keep.');\n        if ((arg, value) in params.items()) then;\n            :user_input = input(f'{arg} [{value}]: ').strip();\n            :params[arg] = get_bool_from_input(user_input, value) if isinstance(value, bool) else user_input or value;\n            :print(f'{arg}: {params[arg]}');\n        endif;\n    endif;\n    :params['start'] = clone_github_repo(params['start']) if params['start'].startswith('https://github.com/') else params['start'] if os.path.isdir(params['start']) else os.getcwd();\n    :py2dataset(**params);\nif (__name__ == '__main__') then;\n    :main();\nendif;\n:def py2dataset(start: str, output_dir: str, questions_pathname: str, model_config_pathname: str, use_llm: bool, quiet: bool, single_process: bool, detailed: bool, html: bool, skip_regen: bool);\n    :sys.setrecursionlimit(3000);\n    :model_config = get_model(model_config_pathname) if use_llm and single_process else None;\n    :logging.getLogger().setLevel(logging.WARNING if quiet else logging.INFO);\n    :params = {'output_dir': get_output_dir(output_dir), 'model_config_pathname': model_config_pathname, 'questions': get_questions(questions_pathname), 'use_llm': use_llm, 'model_config': model_config, 'detailed': detailed};\n    :exclude_dirs = ['env', 'venv', '__pycache__', 'build', 'dist'];\n    if (python_pathname in Path(start).rglob('*.py')) then;\n        if (any((dir in python_pathname.parts for dir in exclude_dirs)) or python_pathname.name == '__init__.py') then;\n            :continue;\n        endif;\n        :params['python_pathname'] = str(python_pathname);\n        :params['relative_path'] = Path(os.path.relpath(python_pathname, os.path.dirname(get_start_dir(start))));\n        :instruct_pathname = Path(params['output_dir']) / params['relative_path'].with_suffix('.py.instruct.json');\n        if (instruct_pathname.exists() and skip_regen) then;\n            :continue;\n        endif;\n        if (model_config is None and use_llm) then;\n            :process = Process(target=process_single_python_file, args=(params,));\n            :process.start();\n            :process.join();\n            :process.close();\n        endif;\n    endif;\n    :return;\n:def process_single_python_file(params: Dict);\n    :logging.info(f\"Processing file: {params['python_pathname']}\");\n    :file_details = get_python_file_details(params['python_pathname']);\n    if (not file_details) then;\n        :logging.error(f\"Failed to get file details for {params['python_pathname']}\");\n        :return;\n    endif;\n    if (params['model_config'] is None and params['use_llm']) then;\n        :params['model_config'] = get_model(params['model_config_pathname']);\n    endif;\n    :instruct_data = get_python_datasets(params['python_pathname'], file_details, params['relative_path'], params['questions'], params['model_config'], params['detailed']);\n    if (instruct_data) then;\n        :save_python_data(file_details, instruct_data, params['relative_path'], params['output_dir']);\n    endif;\n:def clone_github_repo(url: str);\n    partition \"try\" {\n        :repo_name = Path(url).stem;\n        :repo_path = Path.cwd() / 'githubrepos' / repo_name;\n        if (repo_path.exists()) then;\n            :repo = Repo(repo_path);\n            :repo.remote().fetch();\n            :repo.git.reset('--hard', repo.heads[0].commit);\n        endif;\n        :return;\n    }\n    partition \"GitCommandError as :\" {\n        :logging.info(f'Error processing repository {url}: {e}');\n        :return;\n    }\n:def get_bool_from_input(input_str: str, current_value: bool);\n    if (input_str.lower() in ['t', 'true', 'y', 'yes']) then;\n        :return;\n    endif;\n    :return;\nstop\n@enduml"
    code_qa_response: "py2dataset/py2dataset.py:\n  Code Documentation:\n  - 'I) Purpose and Processing Approach for Python file `py2dataset/py2dataset.py`:\n    This Python script named 'py2dataset.py' aims to generate datasets by processing Python files within a specified directory or GitHub repository and producing question-answer pairs related to code purposes. It utilizes various functionalities such as extracting file details, handling model configurations with LLMs (Language Models), logging messages for progress tracking, and saving output data in JSON format. The user can interactively set parameters through command line arguments or provide default values. If a GitHub repository URL is given as the starting directory, it clones or updates the repository locally before processing files within it.\n    II) Detailed Requirements, API Signatures, and Logic for all Functions & Class Methods:\n    1. `process_single_python_file(params: Dict) -> None`:\n    - Purpose: Processes a single Python file to generate question-answer pairs and instructions related to its functionality.\n    - Input: A dictionary 'params' containing necessary details about the Python file path ('python_pathname'), model configuration ('model_config'), questions file ('questions'), relative path within output directory ('relative_path'), detailed analysis flag ('detailed').\n    - Internal Calls & Operations:\n    - Logging messages related to file processing status.\n    - Retrieves file details using `get_python_file_details()`. If failure, logs error and returns without proceeding further.\n    - Acquires instruct_data dataset via `get_python_datasets()` function call. If no data retrieved, logs error regarding it.\n    - Uses `save_python_data()` to save file details and dataset. Elsewhere an error logged when generating the dataset fails.\n    - Process Outcome: None type, performing necessary actions for one Python file.\n    2. `py2dataset(start: str=\"\", output_dir: str=\"\", questions_pathname: str=\"\", model_config_pathname: str=\"\", use_llm: bool=False, quiet: bool=False, single_process: bool=False, detailed: bool=False, html: bool=False, skip_regen: bool=False) -> Dict[str, List[Dict]]:`\n    - Purpose: Generates datasets by processing Python files within a specified directory or GitHub repository.\n    - Inputs: Multiple arguments such as starting directory path ('start'), output directory path for saved results ('output_dir'), questions file name ('questions_pathname'), model configuration file path ('model_config_pathname'), usage of LLMs for code analysis ('use_llm'), limit logging level ('quiet'), utilizing one process ('single_process'), inclusion of detailed data analysis ('detailed'), generating HTML output ('html') or skipping existing '.instruct.json' files regeneration ('skip_regen').\n    - Internal Calls & Operations:\n    - Sets recursion limit higher for better AST parsing performance with `sys.setrecursionlimit()`.\n    - Acquires model configuration through `get_model()` function if 'use_llm' and single process enabled. Else processes files sequentially without invoking multiple instances of `process_single_python_file()`.\n    - Iterates over Python files in the given directory using `Path().rglob()`, excluding certain directories ('exclude_dirs'). For each file, sets parameters required by `process_single_python_file()` and invokes it either as a separate process or sequentially based on LLM usage.\n    - Combines output JSON files with `combine_json_files()`.\n    - Output: A dictionary containing generated datasets of Python files' question-answer pairs.\n    3. `clone_github_repo(url: str) -> str:`\n    - Purpose: Clones a GitHub repository or fetches latest changes and returns its local path.\n    - Input: URL string for the repository.\n    - Internal Calls & Operations:\n    - Extracts repository name from URL with `Path().stem`.\n    - Checks if repository exists locally ('repo_path.exists()') or clones it using `Repo.clone_from()` if provided URL starts with 'https://github.com/'. Otherwise, verifies directory existence ('os.path.isdir()') and uses current working directory as the base path ('os.getcwd()').\n    - Logs errors encountered during repository processing with `logging`.\n    - Output: Local repository path as a string if successful; empty string otherwise.\n    4. `get_bool_from_input(input_str: str, current_value: bool) -> bool:`\n    - Purpose: Returns boolean value based on user input interactively or keeping original parameter ('current_value').\n    - Inputs: String user input regarding specific parameter with existing setting ('input_str') and initial Boolean ('current_value').\n    - Operations & Returns: Parses 'input_str', checking whether its case matches keywords (lower()) like ['True', 'YES', or 'yes' returning True or corresponding False equivalents like 'false', 'No'.\n    5. `main()`:\n    - Purpose: Command-line entry point for processing Python files and generating datasets with interactive user input options.\n    - Internal Calls & Operations:\n    - Checks if help requested ('len(sys.argv) == 1 or '-h'/'--help') and prints usage instructions before exiting. Else proceeds further.\n    - Converts argument string ('arg_string') to parameter key-value pairs with `params.items()`. Sets Boolean parameters based on matched prefixes like ['--start', '-X'] or invokes `get_bool_from_input()` for interactive inputs marked by '--I'.\n    - If in interactive mode ('params['I']), prompts user input for each parameter, assigning new values or keeping defaults. Logs updated parameters with print().\n    - Determines starting directory as cloned GitHub repository if URL given else local working directory or original path if it's a valid folder path.\n    - Invokes `py2dataset()` using the finalized parameter settings ('params').\n    III) Purpose of inputs, variables, calls & returns in code:\n    - Variables declare and hold diverse information needed by functions (e.g., output directories, model configuration paths, query datasets): 'params', 'sys', 'logging', 'start', 'questions_pathname', 'model_config_pathname', 'exclude_dirs'.\n    - Calls to external libraries and custom modules advance functionality (e.g., 'Pathlib' for path manipulation, 'multiprocessing' for parallel processing): `Path`, `Repo`, `logging`, `typing`.\n    - Returns facilitate program output or data retrieval (e.g., 'combine_json_files()' aggregates individual '.instruct.json' files as datasets).'\n  Dependencies:\n    Value:\n    - typing, get_python_datasets, multiprocessing, get_params, get_python_file_details, save_output, os, git, logging, sys, pathlib\n    Purpose: \"In the given Python code context for 'py2dataset.py', various dependencies play crucial roles to achieve its functionality. Here's a breakdown of their purposes and significance within the program:\\n\\n1. 'typing': This standard library module provides advanced data type annotations, enabling clearer definition and documentation of parameters used in function signatures. It enhances code readability by specifying expected types for input arguments, ensuring proper usage before execution.\\n\\n2. 'get_python_datasets': This function is crucial to extract dataset instruction content related to Python files being processed. It takes input parameters such as the Python pathname, file details, relative path within output directory, questions list, and model configuration (if any) to generate question-answer pairs for code purposes.\\n\\n3. 'multiprocessing': A built-in module facilitating parallel processing of tasks using multiple processes in Python applications. In this script, it's used when LLMs are employed to manage memory consumption during single process mode by handling each Python file separately. It ensures smooth multithreading and reduces bottlenecks when answering complex queries concerning code purpose analysis with the Large Language Models.\\n\\n4. 'get_params': Not a specific dependency but refers to a custom function ('get_params') responsible for collecting essential parameters such as output directory path, questions file name, model configuration pathname, LLM usage flag, detailed analysis requirement, HTML generation preference, and skipping regeneration of existing '.instruct.json' files from command line arguments. These inputs shape the behavior of 'py2dataset()'.\\n\\n5. 'get_python_file_details': Retrieves necessary details about Python files such as module name, package path, file dependencies, class/function declarations for question generation accuracy. This information aids in analyzing code purposes and enhances dataset quality.\\n\\n6. 'save_output': A custom function responsible for saving processed data into JSON format after combining multiple '.instruct.json' files generated from individual Python scripts into one consolidated output file. It helps organize the final result for easier accessibility and analysis.\\n\\n7. 'os': The standard library module offers various operating system interfaces like path manipulation functions ('os.path.isdir()', 'os.getcwd()'), used for navigating directory paths in context ('os.path.relpath') and ensuring default parameter handling ('os.getcwd()' returns current working directory).\\n\\n8. 'git': A third-party library enabling Git repository management within the script. It allows cloning remote repositories or fetching updates locally when a GitHub URL is provided as the starting directory path. This feature expands the code's applicability beyond local Python files processing.\\n\\n9. 'logging': A built-in Python module responsible for producing informational/warning logs to communicate script progress or failures, improving user understanding and debugging experiences while processing data sets from multiple Python scripts. It enhances traceability by conveying potential issues effectively.\\n\\n10. 'sys': An important module supplying information about interactive sessions with users via command line arguments like system-defined arguments ('sys.argv'), altering recursion limit to allow extensive abstract syntax tree (AST) parsing required during the AST process for code understanding, setting its maximum recursive function depth via sys.setrecursionlimit().\\n\\n11. 'pathlib': This library offers more Pythonic file path handling than traditional string operations by utilizing objects like Path(). It simplifies tasks such as creating paths relative to a given base directory ('Path(start)'), checking existence ('Path().exists()') or splitting paths into parts ('Path().parts').\\n\\nThese dependencies contribute significantly to the code's functionality, making it efficient and flexible in handling diverse scenarios related to Python file processing and generating datasets with question-answer pairs for code purposes. Their integration ensures robustness, readability, and adaptability in the 'py2dataset' script. \\n\\nIn summary: Each dependency serves a specific purpose within the codebase - from type checking ('typing'), process management ('multiprocessing'), file handling ('pathlib', 'os'), repository cloning ('git'), logging progress ('logging'), system interactions ('sys') to advanced data analysis ('get_python_file_details' & 'save_output'). Together, they enable generating datasets from Python files with question-answer pairs for code understanding.\"\n  Functions:\n    Value:\n    - process_single_python_file, py2dataset, clone_github_repo, get_bool_from_input, main\n    Purpose: \"In the given Python code context for 'py2dataset.py', these identified functions serve crucial purposes contributing to generating question-answer pairs from Python files within a specified directory or GitHub repository as datasets. Their roles are explained below:\\n\\n1. 'process_single_python_file': Handles individual Python file processing by producing instructional datasets with related question-answer pairs. It takes parameters specifying file details, model configuration usage (if any), and flags for detailed analysis. Its main objective is to extract insights from a single Python script contributing to the overall dataset generation process.\\n\\n2. 'py2dataset': Serves as the core function generating datasets by traversing multiple Python files within a specified directory or GitHub repository, collecting outputs from individual file processing using 'process_single_python_file'. It combines JSON files resulting from each processed script into one comprehensive dataset structure. This function takes various arguments determining user preferences such as output paths, LLM usage, logging levels, HTML generation options, etc., managing parallel or sequential processing based on model configuration needs.\\n\\n3. 'clone_github_repo': Clones a GitHub repository or fetches its latest changes if given a URL. It returns the local path of the cloned repository ensuring necessary data availability for further analysis within the codebase. This function helps when starting from a remote repository instead of a local directory.\\n\\n4. 'get_bool_from_input': Interactively receives new boolean values from users while keeping existing defaults as fallbacks during runtime execution. It's called within the main function to customize options in interactive mode for command-line arguments without hardcoded settings. This enhances user experience by enabling adjustments if required.\\n\\n5. 'main': Functions as an entry point connecting command-line invocation with core functionality, validating input parameters, and triggering dataset generation based on user inputs or default values. It interacts with users in interactive mode to update parameter settings before calling other functions like 'clone_github_repo' and eventually executing 'py2dataset'. This function acts as a bridge between user interaction and the actual code execution logic. \\n\\nEach of these functions plays a significant role in accomplishing the overall objective of generating question-answer pairs from Python files while allowing customization options for users to suit their needs. They work together harmoniously within 'py2dataset' script, enhancing flexibility and adaptability.\"\n  process_single_python_file:\n    Inputs:\n      Value:\n      - params\n      Purpose: 'In the context given for 'py2dataset/py2dataset.py' code, the primary focus object passing into the 'process_single_python_file' function is a dictionary called 'params'. This input dictionary carries vital attributes or configuration required by 'process_single_python_file' to carry out its work effectively while generating question-answer pairs for a specific Python file. Each element in 'params' holds significant information necessary for processing one particular Python script:\n        1. 'python_pathname': Refers to the path of the current Python file being processed within the overall dataset generation process.\n        2. 'file_details': Stores details retrieved by 'get_python_file_details' function about the given Python file, utilized later during operations on instruction creation for context understanding and analysis accuracy. Without appropriate file data, proceeding with the question generation could become an issue resulting in a potential failure mentioned via logging warnings/errors.\n        3. If the usage of language model is employed ('use_llm') - i.e., params['model_config'] remains none at the function entrance. Once 'inside process_single_python_file', its value may vary through additional Python functions such as setting Model configurations in accordance with specific use cases when leveraging single processes due to memory limitations; but stays constant throughout param passage here.\n        4. 'relative_path': It represents the path contextualization of the processed Python file within the output directory structure for organized data storage and referencing purposes. This helps maintain proper organization and clarity while saving outputs.\n        5. Other parameters like 'questions', 'detailed', etc., govern various aspects of the instruction dataset creation process but are set outside 'process_single_python_file'. They determine how instructions should be generated or if HTML output is desired, etcetera - impacting overall functionality but not directly influencing this particular function call.\n        In summary, 'params' acts as a communication bridge between higher-level operations and 'process_single_python_file', ensuring it has all necessary inputs to process a single Python file accurately while generating question-answer pairs associated with its purpose in detail as instructed by the codebase's primary task - building comprehensive datasets for multiple files iteratively across various use cases.'\n    Calls:\n      Value:\n      - logging.info, get_python_file_details, logging.error, get_model, get_python_datasets, save_python_data\n      Purpose: 'In the `process_single_python_file` function within the 'py2dataset.py' script, several key method calls are significant for accomplishing its task of processing a single Python file and generating question-answer pairs related to its functionality. Their purposes are as follows:\n        1. `logging.info(f\"Processing file: {params['python_pathname']}`: This call logs an informational message indicating the start of processing a specific Python file with its pathname mentioned in 'params'. It helps track progress during execution.\n        2. `file_details = get_python_file_details(params[\"python_pathname\"])`: The `get_python_file_details` function extracts essential details about the given Python file using its path provided in 'params'. These details are necessary for further processing steps like dataset generation. If it fails, `logging.error` is invoked to log an error message indicating unsuccessful retrieval of file information.\n        3. `logging.error(f\"Failed to get file details for {params['python_pathname']}\"`: This logging call appears when the file detail extraction fails in `get_python_file_details`, alerting users about the issue encountered while obtaining file details.\n        4. `instruct_data = get_python_datasets( params[\"python_pathname\"], file_details, params[\"relative_path\"], params[\"questions\"], params[\"model_config\"] ):` - get_python_datasets obtains dataset instruction data by considering the Python file path ('params[\"python_pathname']'), its extracted details ('file_details'), relative path within output directory ('params[\"relative_path']'), question set from 'params[\"questions\"], and model configuration ('params[\"model_config\"]'). This step generates question-answer pairs related to the Python file's functionality. If unsuccessful, a logging error message is logged using `logging`.\n        5. `save_python_data(file_details, instruct_data, params[\"relative_path\"], params[\"output_dir\"])`: The `save_output` function saves both file details and generated dataset ('instruct_data') into the specified output directory with a '.py.instruct.json' suffix under relative path. If failed saving data, `logging.error` reports this issue to keep track of problems faced during output storage.\n        Together, these method invocations work synergistically within `process_single_python_file` to process one Python file into question-answer pairs for analysis purposes.'\n    Variables:\n      Value:\n      - params, file_details, instruct_data\n      Purpose: 'In the context given for 'process_single_python_file' function within the 'py2dataset.py' script, the mentioned variables hold crucial roles during processing a single Python file to generate question-answer pairs and instructions related to its functionality.\n        1. 'params': This is a dictionary containing essential details required as inputs for processing one Python file. It stores information such as Python pathname ('python_pathname'), model configuration ('model_config'), questions file path ('questions'), relative path within output directory ('relative_path'), and detailed analysis flag ('detailed'). These parameters enable customization while running the function with necessary attributes needed for executing tasks effectively.\n        2. 'file_details': Obtained by calling 'get_python_file_details' function, this variable holds metadata about the Python file being processed in 'process_single_python_file'. It ensures successful file handling and further operations depend on having valid details about the file structure or functionality before proceeding with dataset generation. If unsuccessful ('file_details' becomes empty), an error message is logged but processing doesn't continue, ensuring data integrity.\n        3. 'instruct_data': Generated by invoking 'get_python_datasets', this variable carries the question-answer pairs along with instructions derived from analyzing Python files' contents and given parameters ('questions', 'file_details', 'model_config', detailed analysis flag - 'detailed'). It represents the main outcome of processing one file as instructional datasets which are later saved using 'save_python_data'. If failed to retrieve dataset details ('instruct_data' is empty), an error message gets logged for better traceability but processing moves ahead without disrupting further file analysis in sequential execution. /nIn brief, 'params' personalizes operation parameters, 'file_details' ensures valid input handling, and 'instruct_data' encapsulates the core output - question-answer pairs along with instructions derived from Python files during single execution of 'process_single_python_file'. These variables collectively contribute to efficient functioning of processing a single file within the larger script logic.'\n  py2dataset:\n    Inputs:\n      Value:\n      - start, output_dir, questions_pathname, model_config_pathname, use_llm, quiet, single_process, detailed, html, skip_regen\n      Purpose: \"In the `py2dataset` function within the given Python code 'py2dataset.py', the listed inputs serve as customizable arguments that control the behavior and outcomes during dataset generation from provided Python files. Here is an elaboration for each element along with their importance during code operation:\\n\\n1. 'start': Denotes the origin of processed files. This argument sets where `py2dataset` finds input files; either it can be the working directory, GitHub repository URL, or an explicit directory path if locally stored. By cloning Git repositories upon HTTPS-address beginnings or utilizing current or given paths as valid directories with os.path.isdir() confirmation.\\n\\n2. 'output_dir': Defines the destination for output files after processing Python scripts into question-answer pairs. It determines where generated datasets will be saved in JSON format by default ('./dataset/' unless altered).\\n\\n3. 'questions_pathname': Specifies the filename with path holding user questions intended to test generated instruction datasets against Python functions - mainly for comprehension or assessment purposes (\\\"py2dataset_questions.json\\\" is a standard).\\n\\n4. 'model_config_pathname': Points towards the file containing model configuration settings, typically used when leveraging Language Models (LLMs) to answer code purpose queries ('py2dataset_model_config.yaml' is default).\\n\\n5. 'use_llm': Determines whether LLM assistance will be utilized for answering code purpose questions during dataset generation. If True, it affects how Python files are processed - either sequentially or in separate processes to manage memory consumption. Default value is False.\\n\\n6. 'quiet': Controls logging output level; when set to True, reduces log messages to only warnings while keeping INFO as default behavior for progress tracking.\\n\\n7. 'single_process': When LLM assistance (use_llm=True) is enabled, this flag decides whether all Python files should be processed in a single process or separately - False by default.\\n\\n8. 'detailed': Includes detailed analysis within generated datasets; True activates extended explanations of Python code purposes besides basic descriptions, False implies standard output otherwise.\\n\\n9. 'html': Specifies whether HTML formatted outputs should be created from generated data - set to False as a standard behavior but can be modified if required.\\n\\n10. 'skip_regen': Controls dataset regeneration for existing instruct.json files; when True, py2dataset skips updating them if present already. Otherwise, it overwrites regardless of existence. \\n\\nThese inputs collectively shape how `py2dataset` handles Python code processing into instructive datasets according to user specifications, customizing the execution flow based on chosen settings. Each input has a significant impact on program operation, resulting dataset characteristics and efficiency in handling large codebases or specific use cases.\"\n    Calls:\n      Value:\n      - sys.setrecursionlimit, get_model, logging.getLogger().setLevel, logging.getLogger, get_output_dir, get_questions, Path(start).rglob, Path, any, str, os.path.relpath, os.path.dirname, get_start_dir, params['relative_path'].with_suffix, instruct_pathname.exists, Process, process.start, process.join, process.close, process_single_python_file, combine_json_files\n      Purpose: \"In the `py2dataset` function within the given Python script 'py2dataset.py', various operations are performed to generate datasets by processing Python files from a specified directory or GitHub repository and produce question-answer pairs related to their functionality. The significance of mentioned calls can be explained as follows:\\n\\n1. `sys.setrecursionlimit(3000)`: Sets the recursion limit higher for handling complex Abstract Syntax Tree (AST) parsing during Python file processing, ensuring deeper nesting levels are handled without hitting the default recursion limit issue.\\n2. `get_model(model_config_pathname)` if `use_llm` and `single_process` are True else None: Retrieves model configuration for Language Model usage in a single process scenario; otherwise, remains uninitialized. This function helps incorporate LLMs to answer code purpose questions when needed.\\n3. `logging.getLogger().setLevel(logging.WARNING if quiet else logging.INFO)`: Configures the logger's minimum logging level based on the 'quiet' parameter value. If set to False (less quiet), logs messages with INFO level detail; otherwise, limits it to WARNING level for reduced verbosity.\\n4. `get_output_dir(output_dir)`: Returns the output directory path as per user input or default setting \\\"./dataset/\\\".\\n5. `get_questions(questions_pathname)`: Acquires questions from the specified file or default path \\\"./py2dataset_questions.json\\\".\\n6. `Path(start).rglob(\\\"*.py\\\")`: Iterates over Python files in the given starting directory using Pathlib's rglob method, finding all \\\".py\\\" files recursively.\\n7. `any(dir in python_pathname.parts for dir in exclude_dirs)` and `if python_pathname.name == \\\"__init__.py\\\"`: Filters out directories with specified names (\\\"env\\\", \\\"venv\\\", \\\"build\\\", and \\\"dist\\\") from iterated paths while ignoring Python files with \\\"_init_\\\".py suffix, common Python setup file for a package/module in Python applications.\\n8. `str(repo_path)` after clone repository process through Repo functions: Converts path objects into strings, outputting the cloned or fetched repository path if successful as a string representation.\\n9. `get_bool_from_input(user_input, value)`: Interactively modifies parameter values based on user input during command-line execution in interactive mode ('params[\\\"I\\\"]'). It returns True/False depending upon the input string matching keywords like 'True', 'YES', or 'yes'/'false', 'No', 'FALSE', 'n'.\\n10. `Process(target=process_single_python_file, args=(params,))` and `process.start(), process.join(), process.close()`: Enables multiple instances of the process if `use_llm`, but singular in single LLM scenario using a different strategy without concurrency for memory management.\\n11. `save_python_data(file_details, instruct_data, params[\\\"relative_path\\\"], params[\\\"output_dir\\\"])`: Saves extracted details with derived data instructions within respective directory tree and instruct.*filename prefix using json savings feature 'combine_json_files'.\\n12. Finally, `combine_json_files(output_dir, html, params[\\\"questions\\\"])` consolidates individual '.instruct.json' files as generated datasets after processing all Python files within the directory structure. \\n\\nThese calls collaborate to achieve efficient dataset generation from Python scripts while handling various user inputs and customization options in `py2dataset`.\"\n    Variables:\n      Value:\n      - start, use_llm, detailed, instruct_pathname, html, skip_regen, process, model_config_pathname, questions_pathname, quiet, output_dir, params, model_config, exclude_dirs, single_process\n      Purpose: \"In the 'py2dataset' code, several variables hold specific roles to configure settings and facilitate functionality while processing Python files and generating question-answer datasets. Their purposes are as follows:\\n\\n1. 'start': Specifies the starting directory for Python files or GitHub repository URL to process. It can be a local path or remote repository link. If it's a GitHub URL, this variable triggers cloning/updating the repo before proceeding with file processing.\\n\\n2. 'use_llm': Determines whether to use Language Models (LLMs) for answering code purpose questions during analysis. Enabling this option can influence how multiple processes are handled within the script.\\n\\n3. 'detailed': Indicates if extensive analysis should be included in the generated datasets by providing additional details about Python files.\\n\\n4. 'instruct_pathname': Path and filename for questionnaire (`.json`) holding input queries needed for data extraction; serves as a default parameter for ease of use but can be overridden with user inputs.\\n\\n5. 'html': Controls output format generation - HTML files are produced when set to True, otherwise JSON format is used by default.\\n\\n6. 'skip_regen': When True, it skips regeneration of existing '.instruct.json' files during processing; otherwise, all files are processed regardless of previous results.\\n\\n7. 'process': Not a user-defined variable but created within the code as a Process object when LLMs are used in single process mode ('use_llm'). It manages individual Python file processing for memory management or runs 'process_single_python_file()'.\\n\\n8. 'model_config_pathname': Path and filename of the model configuration file for LLM usage; holds default value but can be modified by users if needed.\\n\\n9. 'questions_pathname': Same as instruct_pathname, this denotes a file with preset location holding queries about Python functions yet alterable during execution via command line arguments.\\n\\n10. 'quiet': Limits logging output to WARNING level messages instead of INFO by default but can be toggled for less verbosity.\\n\\n11. 'output_dir': Determines the destination directory for saved output files with default set as './dataset/' path but changeable by users in command arguments.\\n\\n12. 'params': A dictionary collecting all variable inputs for smooth script execution, consolidating configuration options to be passed as parameters within other functions like 'py2dataset()'.\\n\\n13. 'model_config': Holds model configuration acquired dynamically if LLM usage ('use_llm') occurs during single process mode else remains None initially.\\n\\n14. 'exclude_dirs': List of directories to be excluded while iterating Python files; helps in skipping unnecessary paths like \\\"__pycache__\\\", \\\"env\\\", \\\"venv\\\", and \\\"build\\\".\\n\\n15. Lastly, 'single_process': Controls whether multiple processes are utilized when LLMs answer code purpose questions ('use_llm' is True). Otherwise, all Python files are processed sequentially within a single process. \\n\\nEach variable plays a significant role in configuring the behavior of `py2dataset` script according to user preferences or default settings while handling Python file analysis and dataset generation.\"\n    Returns:\n      Value:\n      - combine_json_files(output_dir, html, params['questions'])\n      Purpose: 'In the given context, the tuple enclosed within square brackets [\"combine_json_files(output_dir, html, params['questions']\"] represents a return value from the function 'py2dataset'. This function aggregates all generated '.instruct.json' files produced during processing Python files into one comprehensive dataset output. Specifically, it consolidates data extracted across various Python scripts within the specified directory or GitHub repository into a unified format for further analysis or usage.\n        The three elements in this tuple signify different arguments passed to 'combine_json_files':\n        1. \"output_dir\": Refers to the path where processed results should be stored as per user-specified options (\"--output_dir\" parameter). It organizes saved output files at the chosen directory after completing file analysis for each Python script in a neat and structured manner.\n        2. \"html\": Determines whether HTML formatted output is desired by the user (\"--html\" flag). If True, the final combined dataset might be rendered more suitable for browser-viewable visualizations rather than default JSON structure, offering more presentation choices post generation.\n        3. \"params['questions']\": Provides a dictionary storing queries about Python functions gathered from user settings (found as \"--questions_pathname\" command argument) containing details like prompts required by 'get_python_datasets()' to generate question-answer pairs. This ensures question-answer relationships are accurately reflected in the final combined dataset.\n        Hence, combining these inputs within 'combine_json_files()', py2dataset generates a consolidated dataset with question-answer pairs for Python files processed throughout its runtime - synthesizing individual JSON results into an all-encompassing structured data output per user instructions and directory handling requirements. This approach ensures simplicity while analyzing gathered data during any postprocessing operation like inspection, reviewal, analysis etcetera without hassling around discrete parts within disparate file segments or unnecessarily searching numerous instruction directories.'\n  clone_github_repo:\n    Inputs:\n      Value:\n      - url\n      Purpose: 'In the context given, the function `clone_github_repo(url: str) serves as a utility within the 'py2dataset.py' script primarily focused on handling GitHub repository operations. Its sole input parameter is 'url', which represents a string containing the URL of a GitHub repository. The purpose of this function is to either clone the repository from the provided URL if it starts with a GitHub link or update an already existing local clone by fetching its latest changes, ensuring availability for further processing within the script.\n        The significance of 'url' input in `clone_github_repo` lies in determining whether the starting directory for Python file processing should be a local folder path (when not starting with GitHub URL) or a remote repository URL needing cloning or updating via Git operations using Python's built-in library 'git'. If 'url' contains \"https://github.com/\" as prefix indicating GitHub address, this function operates as clone and subsequent commands manage file traversals corresponding to this location for analyzing files there; on contrary actions like filepath existence validation and navigational approaches apply for current working directories ('os.getcwd()') or directory names received by users via arguments (parameter 'start'). Therefore,'url' input triggers repository management steps in the code pipeline, making it crucial to the overall script functionality.'\n    Calls:\n      Value:\n      - Path, Path.cwd, repo_path.exists, Repo, repo.remote().fetch, repo.remote, repo.git.reset, Repo.clone_from, str, logging.info\n      Purpose: \"In the `clone_github_repo` function within the given Python code 'py2dataset.py', each mentioned call serves a specific purpose supporting repository management actions either to handle local directory existences or fetch and modify a GitHub repository prior to file processing steps:\\n\\n1. `Path` - An object from python's pathlib module used for handling paths with advanced functionalities like stemming repository name from URL or checking relative paths.\\n2. `Path.cwd()` - Returns the current working directory as a Path-like object which acts as the default starting point when provided URL doesn't specify GitHub repository or when the given local path is indeed an existing directory.\\n3. `repo_path.exists()` - Checks whether cloned or fetched repository already exists locally before modifying its contents or processing Python files. This prevents redundant operations.\\n4. `Repo` - An object from git python library representing a Git repository enabling interaction with Git repositories within the codebase.\\n5. `repo().remote().fetch()` - Updates the fetched commits in the local clone ensuring the latest changes are incorporated before processing Python files.\\n6. `repo_path` - Represents the path of cloned/updated repository used further in other operations.\\n7. `str()` - Converts repo_path object to string format for returning it as output.\\n8. `logging.info()` - Logs messages related to any encountered errors while processing repositories, providing debugging information if issues arise during repository handling. \\n9. `Repo.clone_from()` - Clones a GitHub repository from the given URL into the specified directory if it starts with \\\"https://github.com/\\\". This call initiates cloning process when required.\\n\\nThese calls together ensure proper management of GitHub repositories by either updating existing ones or cloning new ones before processing Python files within them as part of generating datasets using py2dataset script. They establish connectivity with the Git platform, handle local storage constraints and facilitate seamless progression to data extraction functions without breaking the overall code pipeline due to repo concerns.  \\nAlso noted earlier function calls ('git commands throw Error if fails' logs issues)- Handling possible exception caused in clone operations, completing full error handling cycle for repository actions within `clone_github_repo`.\"\n    Variables:\n      Value:\n      - repo_name, url, repo_path, repo\n      Purpose: 'In the `clone_github_repo` function within the given Python code context of 'py2dataset.py', the mentioned variables hold specific roles during GitHub repository handling operations.\n        1. 'repo_name': This variable is created to simplify identifying the name of the cloned or fetched repository for logging purposes or future references after cloning. It is formed by stripping the URL stem from the given input URL using Path().stem method, making it easier to reference later when interacting with repository files or logs.\n        2. 'url': The primary input argument in `clone_github_repo` represents a GitHub repository URL supplied as starting directory path during user input or command execution. Its primary use case is to identify if the directory needs fetching changes remotely or if it already exists locally in order to process accordingly. If it starts with \"https://github.com/\", clone operation ensues, otherwise checking local availability happens through os.path.isdir().\n        3. 'repo_path': This variable stores the local path where the repository is cloned or fetched changes are stored after successful processing. It holds the result of Path() function with specified repo name combined with \"githubrepos\" directory within the current working directory, indicating its location in the system. If clone is needed due to URL input starting with GitHub address or already present locally as a directory, repo_path receives corresponding path values for further usage.\n        4. 'repo': The variable 'repo' instantiates Repo object from git Python library upon successful cloning/fetching of the repository. It enables interaction with repository files like fetching commits or resetting its state using repo methods during operation, allowing the codebase to proceed further with analysis within these fetched contents.\n        Collectively, these variables help in efficiently handling GitHub repository processing operations, ensuring proper cloning/updating and providing necessary paths for subsequent Python file processing steps within `py2dataset`.'\n    Returns:\n      Value:\n      - str(repo_path)\n      Purpose: 'In the given context, the function `clone_github_repo(url: str)` primary objective is to clone a GitHub repository or fetch its latest changes if required while working with Python script 'py2dataset.py'. If provided URL starts with \"https://github.com/\", it clones the repository to the local machine under the designated directory structure - 'githubrepos' followed by repository name, returned as a string format representing the path of the cloned repository ('str(repo_path)'). However, if the given input is already a valid directory path or current working directory ('os.getcwd()'), it skips the cloning step and returns an empty string if any error occurs during the process. In summary, the significant returns from `clone_github_repo` are either a local repository path after successful operation or an empty string indicating failure. These returns help determine where to start processing Python files within py2dataset for generating datasets depending on the given URL input format and repository existence status.'\n  get_bool_from_input:\n    Inputs:\n      Value:\n      - input_str, current_value\n      Purpose: 'In the context given for the 'get_bool_from_input' function within the 'py2dataset.py' script, its primary purpose is to retrieve a new Boolean value based on user input interactively while providing default fallback options if needed clarity or confirmation isn't sought. The inputs 'input_str' and 'current_value' play crucial roles in achieving this functionality:\n        1. 'input_str': This input represents the user interaction prompting text where a new value for a specific parameter can be entered during runtime execution. Users can modify settings dynamically by typing their desired choice related to a particular argument. It could contain the original assigned parameter value alongside user modification intentions expressed through phrases indicating yes or no decisions (\"t\"/\"True\", \"n\"/\"False\", \"y\"/\"yes\"/\"Yes\" vs. \"f\"/\"false\"/\"No\").\n        2. 'current_value': This input holds the existing Boolean setting for a specific parameter before invoking user interaction. It serves as a backup option in case no new value is entered by the user or if their input doesn't match any expected keywords related to True/False decisions. If 'input_str' doesn't align with standard responses denoting True/False values, the function will return this stored value without change, preserving previous settings intact.\n        Inside the `get_bool_from_input` function, these inputs are processed as follows:\n        - Split 'input_str' to identify if it matches keywords for True ('t', \"true\", or \"yes\") or False ('f', \"false\", or \"no\"). If a match is found, the function returns True correspondingly. Otherwise, returning False implies maintaining current_value (given as argument).\n        This interactive mechanism allows users flexibility to adjust parameters without rigidly sticking to predefined options while ensuring default values persist if unsure about modifications.'\n    Calls:\n      Value:\n      - input_str.lower\n      Purpose: \"In the function `get_bool_from_input`, the call 'input_str.lower()' serves to convert the user input into lowercase before checking if it matches any predefined boolean values like \\\"true\\\", \\\"t\\\", \\\"yes\\\", \\\"f\\\", \\\"false\\\", \\\"n\\\", or \\\"no\\\". This standardization ensures consistent comparison regardless of uppercase/lowercase differences in user inputs, making it easier for further evaluation and returning the appropriate Boolean value based on given keywords. Inside `get_bool_from_input`, this lowercased string is then used to determine whether the input represents True or False according to its matching pattern among given options. Thus, 'input_str.lower()' plays a crucial role in accurately interpreting user inputs and setting corresponding parameters as desired Booleans within the function logic. \\n\\nAnother significant part of `get_bool_from_input` is obtaining current_value if not matched with any boolean keywords - preserving the initial value given by default. So even when no appropriate option exists in user input string ('input_str'), original values set in variable 'current_value' persist instead of breaking program flow with uncertain consequences or invalid assignments. Hence, these combined operations enable seamless parameter customization according to interactive mode during execution without unexpected crashes.\"\n    Variables:\n      Value:\n      - input_str, current_value\n      Purpose: 'In the context given, the function 'get_bool_from_input' serves as a helper utility within the 'py2dataset.py' script to retrieve updated Boolean values for certain command-line arguments based on user interaction or keep their default settings if none provided. Inside this function, two variables play significant roles:\n        1. 'input_str': It represents user input received from prompting questions about specific argument configurations during interactive mode execution of the program. Users are asked to provide a response related to a particular parameter ('arg') with an option to hit enter for maintaining default values. This string contains either the new value entered by users or remains unchanged if they don't intervene.\n        2. 'current_value': It denotes the initial Boolean setting of the argument before seeking user input. This variable holds the original value assigned during non-interactive execution or when no response is given after prompting in interactive mode. The function compares user input ('input_str') against fixed keywords representing true/false states like ['t', 'true', 'yes'] for setting Boolean True and ['f', 'false', 'n', 'no'] for Boolean False.\n        Both variables together enable a flexible way to update parameters dynamically during runtime if users wish to modify them interactively while running the script through command line arguments. The function returns the converted Boolean value based on user input or maintains the existing ('current_value') setting when no response is given or doesn't match expected boolean formats ('True/False'). Their conjoint purpose lies in reflecting users' choice amid command line inputs by balancing user agency over preset parameter settings defined initially for standard executions.'\n    Returns:\n      Value:\n      - current_value, True, False\n      Purpose: \"In the given context of 'get_bool_from_input' function within the 'py2dataset.py' script, its primary purpose is to obtain a Boolean value based on user input interactively or maintain the existing value if none is provided. It takes two arguments - 'input_str', which contains the user's input corresponding to specific parameters (either retrieved from command line flags or prompts in case of '-I' option engaged interactive mode, and 'current_value', representing the default Boolean setting for that parameter.\\n\\nThe function parses the input string ('input_str') looking for keywords associated with True or False cases (such as \\\"y\\\", \\\"yes\\\", \\\"t\\\", \\\"True\\\", \\\"n\\\", \\\"false\\\", or \\\"no\\\") and returns either a converted Boolean value derived from user input or keeps the initial setting ('current_value'). In essence, it ensures flexibility in updating parameters dynamically during runtime while maintaining default values when users don't provide explicit inputs. These returned Booleans are utilized to update parameter settings within the main script execution flow after gathering arguments for processing Python files and generating datasets. \\n\\nFor illustration purposes ('current_value, True, False') represents a sample tuple containing default Boolean values assigned to parameters in case user interaction didn't override them with opposite cases during runtime. In reality, each position corresponds to a separate parameter within the function call of 'get_bool_from_input'. \\n\\n1. First return ('current_value'): Represents the initial Boolean value if user input doesn't match any True/False keywords or left unchanged after interactive mode prompts.\\n2. Second return (True): Signifies when user input matches \\\"y\\\", \\\"true\\\", \\\"yes\\\", \\\"True\\\" cases indicating affirmative responses during interactive mode.\\n3. Third return (False): Represents instances where user input aligns with \\\"n\\\", \\\"false\\\", or \\\"no\\\" keywords, denoting negative responses during interactive mode. \\n\\nIn summary, `get_bool_from_input` plays a crucial role in dynamically updating parameters based on user inputs while preserving default values when none are provided. Its returns facilitate seamless interaction with users for customizing script behavior without disrupting the overall functionality if left unaltered.\"\n  main:\n    Calls:\n      Value:\n      - 'len, print, sys.exit, .join, params.items, isinstance, arg_string.replace, arg_string.split, value_segment.split(' --')[0].strip, value_segment.split, params.pop, input(f'{arg} [{value}]: ').strip, input, get_bool_from_input, params['start'].startswith, clone_github_repo, os.path.isdir, os.getcwd, py2dataset'\n      Purpose: \"In the main function within the Python script 'py2dataset', these specific calls serve important roles while setting parameters from command line inputs for execution. Let's examine them one by one:\\n\\n1. `len(sys.argv) == 1 or '-h' in sys.argv or '--help' in sys.argv` - Checks if the user requests help by counting arguments or includes '-h'/'--help', printing usage instructions and exiting if true, otherwise proceeding further into processing Python files for datasets generation.\\n2. `print(__doc__)` & `sys.exit()` - Outputs the help message created via `__doc__` string constant when called with `python py2dataset.py -h` or `--help`, terminating program execution using `sys.exit()`.\\n3. `arg_string = \\\" \\\".join(sys.argv[1:])` - Concatenates command line arguments after script name into a single string separated by spaces for further processing.\\n4. `for arg, value in params.items()` - Iterates over key-value pairs stored in 'params' dictionary for each argument given in command line inputs.\\n5. `args.replace(f\\\"--{arg}\\\", \\\"\\\")` & `args = args.replace(f\\\"--{arg} {params[arg]}\\\", \\\"\\\")` - Modifies 'arg_string', replacing argument prefixes with their respective values from 'params' dictionary for updating it according to user inputs.\\n6. `get_bool_from_input(user_input, value)` - Obtains interactive boolean inputs changing the parameters if requested interactively (through flag '-I') else retaining initial values given or default settings using case matching strategies with keywords like 'True', 'YES', 'yes'/'false', 'NO', 'FALSE', 'no'.\\n7. `params[\\\"start\\\"] = clone_github_repo(params[\\\"start\\\"])` - If starting directory URL begins with GitHub repository URL ('https://github.com/'), clones it using `clone_github_repo()`, else considers local working path if an existing valid directory ('os.getcwd') or current path saved in 'params[\\\"start\\\"]'.\\n8. `py2dataset(**params)` - Triggers main function `py2dataset` invocation, feeding acquired command-line inputs from processed parameters through the `**` keyword arguments notation (a dictionary expanse).\\n9. `os.path.isdir(params[\\\"start\\\"])` & `os.getcwd()` - Verifies if given starting directory exists as a folder or takes current working path ('os.getcwd') in case of incorrect inputs or non-folder URLs.\\n10. Lastly, `main()` invokes the core functionality of generating datasets by calling `py2dataset`.\\n\\nThese calls in `main()` facilitate user interaction with command line arguments, updating parameters accordingly before executing the actual processing flow to generate question-answer pairs from Python files. Each operation has a unique purpose in preparing inputs for efficient execution and error handling where necessary. They ensure flexibility while providing a smooth workflow tailored according to input instructions provided by users during script invocation.\\nUser: Discuss Module Organization of 'py2dataset.py'\\nASSIST: The Python module 'py2dataset.py' is organized into several distinct functions and one main entry point serving specific purposes that collaboratively achieve the overall goal of generating datasets from Python files or GitHub repositories while processing their contents to produce question-answer pairs regarding code functionalities. These organizational segments break down into:\\n\\n1. Custom function definitions handling specialized tasks such as:\\n   a. `process_single_python_file(params: Dict)` - Processes individual Python files generating instructional datasets.\\n   b. `py2dataset(start: str=\\\"\\\", output_dir: str=\\\"\\\", ...) -> Dict[str, List[Dict]]` - Generates datasets from a directory or GitHub repository by iterating over Python files and combining JSON outputs.\\n   c. `clone_github_repo(url: str) -> str` - Clones/updates a GitHub repository locally if needed for processing.\\n   d. `get_bool_from_input(input_str: str, current_value: bool) -> bool` - Interactively updates Boolean parameters from user inputs or maintains defaults.\\n   e. Internal utility functions like `main()` acts as an entry point to execute the entire process with command line arguments handling and interactive options.\\n\\n2. Imported libraries and modules contribute to various functionalities:\\n   a. `import os`, `import sys`, `import logging`, `from pathlib import Path`, `from typing import Dict`, `from multiprocessing import Process` - Assist with file manipulation, recursion limit management, logging messages, directory traversal, type hints, and parallel processing.\\n   b. `from git import Repo` - Enables Git repository handling for cloning or fetching changes.\\n   c. `import get_python_file_details`, `get_python_datasets`, `get_params`, `save_output` - Support retrieval of file details, datasets creation with questions & LLMs configurations and output data storage respectively.\\n\\nThese segments together create a modular structure allowing clear separation of concerns within the codebase for efficient maintenance and readability while achieving desired functionality. The organization enables developers to understand each task individually and make changes if necessary without disrupting other parts of the script.\"\n    Variables:\n      Value:\n      - params, arg_string, user_input, arg_string, value_segment, arg_string\n      Purpose: \"In the given context of 'py2dataset.py's main function, these variables play crucial roles during command-line argument parsing and interactive user input handling to configure the program execution.\\n\\n1. 'params': This dictionary holds all the configuration settings for processing Python files, which can either be predefined defaults or values changed via arguments or interactively input by users in 'main'. It consolidates key information necessary throughout code operations such as directories, file paths, flags like detailed analysis usage, HTML output generation, etc.\\n\\n2. 'arg_string': Initially contains the remaining string after extracting parsed command-line arguments from sys.argv list. It helps identify argument names and their corresponding values to update params dictionary during interactive mode or directly set parameters with default ones if none specified on the command line.\\n\\n3. 'user_input': Appears in interactive mode when users opt for it ('params[\\\"I\\\"] is True'). Here, user_input captures new values for specific arguments after prompting them with relevant questions. If no change desired, pressing enter keeps the existing value stored in 'value'. This variable bridges human interaction into program configuration.\\n\\n4. 'value_segment': Used when updating params dictionary during interactive mode from user input. It splits arg_string based on argument names ('--{arg}') and extracts new values entered by users after removing prefixes like '--{arg}', stripping excess quotes (\\\"\\\") before storing it into corresponding args of params dict.\\n\\n5. Again, revisiting 'arg_string': As program flow advances past interactive mode, this string is trimmed for updated arguments and their respective values inserted in params dictionary, completing configuration before actual dataset generation by 'py2dataset()'. The cleaned string remains until no further edits from interactive prompts remain present in cmd-line inputs.  \\n\\nAll mentioned variables harmonize in ensuring suitable inputs tailored according to users' needs for efficient processing and output generation through Python file analysis in the 'main' function of this code module. They manage command line arguments or user input effectively while adjusting internal operations accordingly.\""
    purpose: 'I) Purpose and Processing Approach for Python file `py2dataset/py2dataset.py`:

        This Python script named ''py2dataset.py'' aims to generate datasets by processing Python files within a specified directory or GitHub repository and producing question-answer pairs related to code purposes. It utilizes various functionalities such as extracting file details, handling model configurations with LLMs (Language Models), logging messages for progress tracking, and saving output data in JSON format. The user can interactively set parameters through command line arguments or provide default values. If a GitHub repository URL is given as the starting directory, it clones or updates the repository locally before processing files within it.


        II) Detailed Requirements, API Signatures, and Logic for all Functions & Class Methods:

        1. `process_single_python_file(params: Dict) -> None`:

        - Purpose: Processes a single Python file to generate question-answer pairs and instructions related to its functionality.

        - Input: A dictionary ''params'' containing necessary details about the Python file path (''python_pathname''), model configuration (''model_config''), questions file (''questions''), relative path within output directory (''relative_path''), detailed analysis flag (''detailed'').

        - Internal Calls & Operations:

        - Logging messages related to file processing status.

        - Retrieves file details using `get_python_file_details()`. If failure, logs error and returns without proceeding further.

        - Acquires instruct_data dataset via `get_python_datasets()` function call. If no data retrieved, logs error regarding it.

        - Uses `save_python_data()` to save file details and dataset. Elsewhere an error logged when generating the dataset fails.

        - Process Outcome: None type, performing necessary actions for one Python file.


        2. `py2dataset(start: str="", output_dir: str="", questions_pathname: str="", model_config_pathname: str="", use_llm: bool=False, quiet: bool=False, single_process: bool=False, detailed: bool=False, html: bool=False, skip_regen: bool=False) -> Dict[str, List[Dict]]:`

        - Purpose: Generates datasets by processing Python files within a specified directory or GitHub repository.

        - Inputs: Multiple arguments such as starting directory path (''start''), output directory path for saved results (''output_dir''), questions file name (''questions_pathname''), model configuration file path (''model_config_pathname''), usage of LLMs for code analysis (''use_llm''), limit logging level (''quiet''), utilizing one process (''single_process''), inclusion of detailed data analysis (''detailed''), generating HTML output (''html'') or skipping existing ''.instruct.json'' files regeneration (''skip_regen'').

        - Internal Calls & Operations:

        - Sets recursion limit higher for better AST parsing performance with `sys.setrecursionlimit()`.

        - Acquires model configuration through `get_model()` function if ''use_llm'' and single process enabled. Else processes files sequentially without invoking multiple instances of `process_single_python_file()`.

        - Iterates over Python files in the given directory using `Path().rglob()`, excluding certain directories (''exclude_dirs''). For each file, sets parameters required by `process_single_python_file()` and invokes it either as a separate process or sequentially based on LLM usage.

        - Combines output JSON files with `combine_json_files()`.

        - Output: A dictionary containing generated datasets of Python files'' question-answer pairs.


        3. `clone_github_repo(url: str) -> str:`

        - Purpose: Clones a GitHub repository or fetches latest changes and returns its local path.

        - Input: URL string for the repository.

        - Internal Calls & Operations:

        - Extracts repository name from URL with `Path().stem`.

        - Checks if repository exists locally (''repo_path.exists()'') or clones it using `Repo.clone_from()` if provided URL starts with ''https://github.com/''. Otherwise, verifies directory existence (''os.path.isdir()'') and uses current working directory as the base path (''os.getcwd()'').

        - Logs errors encountered during repository processing with `logging`.

        - Output: Local repository path as a string if successful; empty string otherwise.


        4. `get_bool_from_input(input_str: str, current_value: bool) -> bool:`

        - Purpose: Returns boolean value based on user input interactively or keeping original parameter (''current_value'').

        - Inputs: String user input regarding specific parameter with existing setting (''input_str'') and initial Boolean (''current_value'').

        - Operations & Returns: Parses ''input_str'', checking whether its case matches keywords (lower()) like [''True'', ''YES'', or ''yes'' returning True or corresponding False equivalents like ''false'', ''No''.


        5. `main()`:

        - Purpose: Command-line entry point for processing Python files and generating datasets with interactive user input options.

        - Internal Calls & Operations:

        - Checks if help requested (''len(sys.argv) == 1 or ''-h''/''--help'') and prints usage instructions before exiting. Else proceeds further.

        - Converts argument string (''arg_string'') to parameter key-value pairs with `params.items()`. Sets Boolean parameters based on matched prefixes like [''--start'', ''-X''] or invokes `get_bool_from_input()` for interactive inputs marked by ''--I''.

        - If in interactive mode (''params[''I'']), prompts user input for each parameter, assigning new values or keeping defaults. Logs updated parameters with print().

        - Determines starting directory as cloned GitHub repository if URL given else local working directory or original path if it''s a valid folder path.

        - Invokes `py2dataset()` using the finalized parameter settings (''params'').


        III) Purpose of inputs, variables, calls & returns in code:

        - Variables declare and hold diverse information needed by functions (e.g., output directories, model configuration paths, query datasets): ''params'', ''sys'', ''logging'', ''start'', ''questions_pathname'', ''model_config_pathname'', ''exclude_dirs''.

        - Calls to external libraries and custom modules advance functionality (e.g., ''Pathlib'' for path manipulation, ''multiprocessing'' for parallel processing): `Path`, `Repo`, `logging`, `typing`.

        - Returns facilitate program output or data retrieval (e.g., ''combine_json_files()'' aggregates individual ''.instruct.json'' files as datasets).'
functions:
    process_single_python_file:
        function_name: process_single_python_file
        function_code: "def process_single_python_file(params: Dict) -> None:\n    \"\"\"Processes a single Python file to generate question-answer pairs and instructions.\"\"\"\n    logging.info(f\"Processing file: {params['python_pathname']}\")\n    file_details = get_python_file_details(params['python_pathname'])\n    if not file_details:\n        logging.error(f\"Failed to get file details for {params['python_pathname']}\")\n        return\n    if params['model_config'] is None and params['use_llm']:\n        params['model_config'] = get_model(params['model_config_pathname'])\n    instruct_data = get_python_datasets(params['python_pathname'], file_details, params['relative_path'], params['questions'], params['model_config'], params['detailed'])\n    if instruct_data:\n        save_python_data(file_details, instruct_data, params['relative_path'], params['output_dir'])\n    else:\n        logging.error(f\"Failed getting {params['python_pathname']} dataset\")"
        function_docstring: Processes a single Python file to generate question-answer pairs and instructions.
        function_inputs:
        - params
        function_defaults: []
        function_returns:
        - None
        function_calls:
        - logging.info
        - get_python_file_details
        - logging.error
        - get_model
        - get_python_datasets
        - save_python_data
        function_call_inputs:
            logging.info:
            - 'f"Processing file: {params[''python_pathname'']}"'
            get_python_file_details:
            - params['python_pathname']
            logging.error:
            - f"Failed getting {params['python_pathname']} dataset"
            get_model:
            - params['model_config_pathname']
            get_python_datasets:
            - params['python_pathname']
            - file_details
            - params['relative_path']
            - params['questions']
            - params['model_config']
            - params['detailed']
            save_python_data:
            - file_details
            - instruct_data
            - params['relative_path']
            - params['output_dir']
        function_variables:
        - params
        - file_details
        - instruct_data
        function_decorators: []
        function_annotations: []
        function_properties: []
    py2dataset:
        function_name: py2dataset
        function_code: "def py2dataset(start: str='', output_dir: str='', questions_pathname: str='', model_config_pathname: str='', use_llm: bool=False, quiet: bool=False, single_process: bool=False, detailed: bool=False, html: bool=False, skip_regen: bool=False) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Generates datasets by processing Python files within a specified directory.\n    Args:\n        start (str, optional): Starting directory for Python files or GitHub repository Python files. Default: current working directory.\n        output_dir (str, optional): Directory to write the output files. Default: ./dataset/.\n        questions_pathname (str, optional): Path and filename of the questions file. Default: ./py2dataset_questions.json.\n        model_config_pathname (str, optional): Path and filename of the model configuration file. Default: ./py2dataset_model_config.yaml.\n        use_llm (bool, optional): Use llm to answer code purpose question. Default: False.\n        quiet (bool, optional): Limit logging output. Default: False.\n        single_process (bool, optional): Use a single process to process Python files if --use_llm. Default: False.\n        detailed (bool, optional): Include detailed analysis. Default: False.\n        html (bool, optional): Generate HTML output. Default: False.\n        skip_regen (bool, optional): Skip regeneration of existing instruct.json files. Default: False.\n    Returns:\n        Dict[str, List[Dict]]: Generated datasets.\n    \"\"\"\n    sys.setrecursionlimit(3000)\n    model_config = get_model(model_config_pathname) if use_llm and single_process else None\n    logging.getLogger().setLevel(logging.WARNING if quiet else logging.INFO)\n    params = {'output_dir': get_output_dir(output_dir), 'model_config_pathname': model_config_pathname, 'questions': get_questions(questions_pathname), 'use_llm': use_llm, 'model_config': model_config, 'detailed': detailed}\n    exclude_dirs = ['env', 'venv', '__pycache__', 'build', 'dist']\n    for python_pathname in Path(start).rglob('*.py'):\n        if any((dir in python_pathname.parts for dir in exclude_dirs)) or python_pathname.name == '__init__.py':\n            continue\n        params['python_pathname'] = str(python_pathname)\n        params['relative_path'] = Path(os.path.relpath(python_pathname, os.path.dirname(get_start_dir(start))))\n        instruct_pathname = Path(params['output_dir']) / params['relative_path'].with_suffix('.py.instruct.json')\n        if instruct_pathname.exists() and skip_regen:\n            continue\n        if model_config is None and use_llm:\n            process = Process(target=process_single_python_file, args=(params,))\n            process.start()\n            process.join()\n            process.close()\n        else:\n            process_single_python_file(params)\n    return combine_json_files(output_dir, html, params['questions'])"
        function_docstring: "\n    Generates datasets by processing Python files within a specified directory.\n    Args:\n        start (str, optional): Starting directory for Python files or GitHub repository Python files. Default: current working directory.\n        output_dir (str, optional): Directory to write the output files. Default: ./dataset/.\n        questions_pathname (str, optional): Path and filename of the questions file. Default: ./py2dataset_questions.json.\n        model_config_pathname (str, optional): Path and filename of the model configuration file. Default: ./py2dataset_model_config.yaml.\n        use_llm (bool, optional): Use llm to answer code purpose question. Default: False.\n        quiet (bool, optional): Limit logging output. Default: False.\n        single_process (bool, optional): Use a single process to process Python files if --use_llm. Default: False.\n        detailed (bool, optional): Include detailed analysis. Default: False.\n        html (bool, optional): Generate HTML output. Default: False.\n        skip_regen (bool, optional): Skip regeneration of existing instruct.json files. Default: False.\n    Returns:\n        Dict[str, List[Dict]]: Generated datasets.\n    "
        function_inputs:
        - start
        - output_dir
        - questions_pathname
        - model_config_pathname
        - use_llm
        - quiet
        - single_process
        - detailed
        - html
        - skip_regen
        function_defaults:
        - ''''''
        - ''''''
        - ''''''
        - ''''''
        - 'False'
        - 'False'
        - 'False'
        - 'False'
        - 'False'
        - 'False'
        function_returns:
        - combine_json_files(output_dir, html, params['questions'])
        function_calls:
        - sys.setrecursionlimit
        - get_model
        - logging.getLogger().setLevel
        - logging.getLogger
        - get_output_dir
        - get_questions
        - Path(start).rglob
        - Path
        - any
        - str
        - os.path.relpath
        - os.path.dirname
        - get_start_dir
        - params['relative_path'].with_suffix
        - instruct_pathname.exists
        - Process
        - process.start
        - process.join
        - process.close
        - process_single_python_file
        - combine_json_files
        function_call_inputs:
            sys.setrecursionlimit:
            - '3000'
            get_model:
            - model_config_pathname
            logging.getLogger().setLevel:
            - logging.WARNING if quiet else logging.INFO
            logging.getLogger: []
            get_output_dir:
            - output_dir
            get_questions:
            - questions_pathname
            Path(start).rglob:
            - '''*.py'''
            Path:
            - params['output_dir']
            any:
            - (dir in python_pathname.parts for dir in exclude_dirs)
            str:
            - python_pathname
            os.path.relpath:
            - python_pathname
            - os.path.dirname(get_start_dir(start))
            os.path.dirname:
            - get_start_dir(start)
            get_start_dir:
            - start
            params['relative_path'].with_suffix:
            - '''.py.instruct.json'''
            instruct_pathname.exists: []
            Process: []
            process.start: []
            process.join: []
            process.close: []
            process_single_python_file:
            - params
            combine_json_files:
            - output_dir
            - html
            - params['questions']
        function_variables:
        - start
        - use_llm
        - detailed
        - instruct_pathname
        - html
        - skip_regen
        - process
        - model_config_pathname
        - questions_pathname
        - quiet
        - output_dir
        - params
        - model_config
        - exclude_dirs
        - single_process
        function_decorators: []
        function_annotations: []
        function_properties: []
    clone_github_repo:
        function_name: clone_github_repo
        function_code: "def clone_github_repo(url: str) -> str:\n    \"\"\"Clone repository or fetch the latest changes and return local repository path.\"\"\"\n    try:\n        repo_name = Path(url).stem\n        repo_path = Path.cwd() / 'githubrepos' / repo_name\n        if repo_path.exists():\n            repo = Repo(repo_path)\n            repo.remote().fetch()\n            repo.git.reset('--hard', repo.heads[0].commit)\n        else:\n            Repo.clone_from(url, repo_path)\n        return str(repo_path)\n    except GitCommandError as e:\n        logging.info(f'Error processing repository {url}: {e}')\n        return ''"
        function_docstring: Clone repository or fetch the latest changes and return local repository path.
        function_inputs:
        - url
        function_defaults: []
        function_returns:
        - str(repo_path)
        - ''''''
        function_calls:
        - Path
        - Path.cwd
        - repo_path.exists
        - Repo
        - repo.remote().fetch
        - repo.remote
        - repo.git.reset
        - Repo.clone_from
        - str
        - logging.info
        function_call_inputs:
            Path:
            - url
            Path.cwd: []
            repo_path.exists: []
            Repo:
            - repo_path
            repo.remote().fetch: []
            repo.remote: []
            repo.git.reset:
            - '''--hard'''
            - repo.heads[0].commit
            Repo.clone_from:
            - url
            - repo_path
            str:
            - repo_path
            logging.info:
            - 'f''Error processing repository {url}: {e}'''
        function_variables:
        - repo_name
        - url
        - repo_path
        - repo
        function_decorators: []
        function_annotations: []
        function_properties: []
    get_bool_from_input:
        function_name: get_bool_from_input
        function_code: "def get_bool_from_input(input_str: str, current_value: bool) -> bool:\n    \"\"\"Return boolean value based on the user input.\"\"\"\n    if input_str.lower() in ['t', 'true', 'y', 'yes']:\n        return True\n    elif input_str.lower() in ['f', 'false', 'n', 'no']:\n        return False\n    return current_value"
        function_docstring: Return boolean value based on the user input.
        function_inputs:
        - input_str
        - current_value
        function_defaults: []
        function_returns:
        - current_value
        - 'True'
        - 'False'
        function_calls:
        - input_str.lower
        function_call_inputs:
            input_str.lower: []
        function_variables:
        - input_str
        - current_value
        function_decorators: []
        function_annotations: []
        function_properties: []
    main:
        function_name: main
        function_code: "def main():\n    \"\"\"\n    Command-line entry point for processing Python files and generating datasets.\n    Usage:\n        python py2dataset.py [options]\n    Options:\n        -h, --help: Show this help message and exit.\n        --start: Starting directory for Python files or GitHub repository Python files. Default: current working directory.\n        --output_dir: Directory to write the output files. Default: ./dataset/.\n        --questions_pathname: Path and filename of the questions file. Default: ./py2dataset_questions.json.\n        --model_config_pathname: Path and filename of the model configuration file. Default: ./py2dataset_model_config.yaml.\n        --use_llm: Use llm to answer code purpose question. Default: False.\n        --quiet: Limit logging output. Default: False.\n        --single_process: Use a single process to process Python files if --use_llm. Default: False.\n        --detailed: Include detailed analysis. Default: False.\n        --html: Generate HTML output. Default: False.\n        --skip_regen: Skip regeneration of existing instruct.json files. Default: False.\n        --I: Interactive mode to enter new values.\n    \"\"\"\n    if len(sys.argv) == 1 or '-h' in sys.argv or '--help' in sys.argv:\n        print(__doc__)\n        sys.exit()\n    params = {'start': '.', 'output_dir': './dataset/', 'questions_pathname': './py2dataset_questions.json', 'model_config_pathname': './py2dataset_model_config.yaml', 'use_llm': False, 'quiet': False, 'single_process': False, 'detailed': False, 'html': False, 'skip_regen': False, 'I': False}\n    arg_string = ' '.join(sys.argv[1:])\n    for arg, value in params.items():\n        if f'--{arg}' in arg_string:\n            if isinstance(value, bool):\n                params[arg] = True\n                arg_string = arg_string.replace(f'--{arg}', '')\n            else:\n                value_segment = arg_string.split(f'--{arg} ')[1]\n                params[arg] = value_segment.split(' --')[0].strip('\"')\n                arg_string = arg_string.replace(f'--{arg} {params[arg]}', '')\n    if params.pop('I'):\n        print('Interactive mode, enter new values or press enter to keep.')\n        for arg, value in params.items():\n            user_input = input(f'{arg} [{value}]: ').strip()\n            params[arg] = get_bool_from_input(user_input, value) if isinstance(value, bool) else user_input or value\n            print(f'{arg}: {params[arg]}')\n    params['start'] = clone_github_repo(params['start']) if params['start'].startswith('https://github.com/') else params['start'] if os.path.isdir(params['start']) else os.getcwd()\n    py2dataset(**params)"
        function_docstring: "\n    Command-line entry point for processing Python files and generating datasets.\n    Usage:\n        python py2dataset.py [options]\n    Options:\n        -h, --help: Show this help message and exit.\n        --start: Starting directory for Python files or GitHub repository Python files. Default: current working directory.\n        --output_dir: Directory to write the output files. Default: ./dataset/.\n        --questions_pathname: Path and filename of the questions file. Default: ./py2dataset_questions.json.\n        --model_config_pathname: Path and filename of the model configuration file. Default: ./py2dataset_model_config.yaml.\n        --use_llm: Use llm to answer code purpose question. Default: False.\n        --quiet: Limit logging output. Default: False.\n        --single_process: Use a single process to process Python files if --use_llm. Default: False.\n        --detailed: Include detailed analysis. Default: False.\n        --html: Generate HTML output. Default: False.\n        --skip_regen: Skip regeneration of existing instruct.json files. Default: False.\n        --I: Interactive mode to enter new values.\n    "
        function_inputs: []
        function_defaults: []
        function_returns: []
        function_calls:
        - len
        - print
        - sys.exit
        - ''' ''.join'
        - params.items
        - isinstance
        - arg_string.replace
        - arg_string.split
        - value_segment.split(' --')[0].strip
        - value_segment.split
        - params.pop
        - 'input(f''{arg} [{value}]: '').strip'
        - input
        - get_bool_from_input
        - params['start'].startswith
        - clone_github_repo
        - os.path.isdir
        - os.getcwd
        - py2dataset
        function_call_inputs:
            len:
            - sys.argv
            print:
            - 'f''{arg}: {params[arg]}'''
            sys.exit: []
            ''' ''.join':
            - sys.argv[1:]
            params.items: []
            isinstance:
            - value
            - bool
            arg_string.replace:
            - f'--{arg} {params[arg]}'
            - ''''''
            arg_string.split:
            - f'--{arg} '
            value_segment.split(' --')[0].strip:
            - '''"'''
            value_segment.split:
            - ''' --'''
            params.pop:
            - '''I'''
            'input(f''{arg} [{value}]: '').strip': []
            input:
            - 'f''{arg} [{value}]: '''
            get_bool_from_input:
            - user_input
            - value
            params['start'].startswith:
            - '''https://github.com/'''
            clone_github_repo:
            - params['start']
            os.path.isdir:
            - params['start']
            os.getcwd: []
            py2dataset: []
        function_variables:
        - params
        - arg_string
        - user_input
        - arg_string
        - value_segment
        - arg_string
        function_decorators: []
        function_annotations: []
        function_properties: []
classes: {}
