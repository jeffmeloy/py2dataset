file_info:
    file_code: "\"\"\"\nGenerates JSON format question-answer pairs and instructions for a Python file\nRequirements:\n[req01] The `DatasetGenerator` class shall:\n        a. Accept Python file path, file details, base name, list of questions,\n        and model configuration as input during instantiation.\n        b. Initialize and store Python file path, file details, base name,\n        question list, llm, and use_llm flag as class attributes.\n        d. Provide `get_response_from_llm` method to retrieve llm response.\n        e. Provide `get_detailed_response` method to generate detailed responses.\n        f. Provide `get_code_qa` method to get code responses from instruct_list.\n        g. Provide `process_question` method to process each question, generate\n        corresponding responses, and add to the instruct_list.\n        h. Provide `process_question_type` method to process questions\n        related to the file, functions, classes, and methods.\n        i. Provide `generate` method to generate responses for all questions\n        and return the instruct_list.\n        j. Internally manage question mapping to file details.\n[req02] The `get_python_datasets` function shall:\n        a. Accept a Python file path, file details, base name, questions list,\n        and model config as input.\n        b. Instantiate `DatasetGenerator` class using the provided input.\n        c. Generate instruct_list using `DatasetGenerator` `generate` method.\n        d. Return the generated `instruct_list`.\n[req03] The `get_unique_elements` function shall:\n        a. Clean an input string (str) and return a string of unique elements.\n\"\"\"\n\nimport logging\nimport re\nimport math\nimport yaml\n\n\ndef get_unique_elements(input_str: str) -> str:\n    \"\"\"\n    Clean an input string (str) and return a string of unique elements.\n    Args:\n        input_str (str): Input string.\n    Returns:\n        str: String of unique elements.\n    \"\"\"\n    def element_generator(input_str):\n        start, brace_level = 0, 0\n        for i, char in enumerate(input_str):\n            if char in \"{}\":\n                brace_level += 1 if char == \"{\" else -1\n            elif char == \",\" and brace_level == 0:\n                yield input_str[start:i].strip(\"'\\\" \")\n                start = i + 1\n        yield input_str[start:].strip(\"'\\\" \")\n    input_str = input_str.strip(\"[]'\\\"\")\n    cleaned_elements = [element for element in element_generator(input_str) if element]\n    return \", \".join(cleaned_elements)\n\n\nclass DatasetGenerator:\n    \"\"\"\n    Generate JSON formatted dictionary outputs for a Python file.\n    Args:\n        file_path (str): Path to the Python file.\n        file_details (dict): Dictionary containing details of the Python file.\n        base_name (str): Base name of the Python file.\n        questions (list): List of questions to be answered for the Python file.\n        model_config (dict): Dictionary containing the model configuration.\n        detailed (bool): Flag to generate detailed responses.\n    Attributes:\n        file_path (str): Path to the Python file.\n        file_details (dict): Dictionary containing details of the Python file.\n        base_name (str): Base name of the Python file.\n        questions (list): List of questions to be answered for the Python file.\n        model_config (dict): Dictionary containing the model configuration.\n        llm (obj): Language model object.\n        use_llm (bool): Flag to use language model.\n        detailed (bool): Flag to generate detailed responses.\n        instruct_list (list): List of instructions and responses.\n        question_mapping (dict): Mapping of question types to file details.\n        code_qa_list (list): List of code questions and responses.\n        code_qa_response (str): Response for the code_qa_dict.\n        code_qa_dict (dict): Dictionary containing code questions and responses.\n    \"\"\"\n\n    def __init__(\n        self,\n        file_path: str,\n        file_details: dict,\n        base_name: str,\n        questions: list[dict],\n        model_config: dict,\n        detailed: bool,\n    ) -> None:\n        \"\"\"Initialize the DatasetGenerator class.\"\"\"\n        self.file_path = file_path\n        self.file_details = file_details\n        self.base_name = base_name\n        self.questions = questions\n        self.model_config = model_config\n        self.llm = model_config[\"model\"] if model_config else None\n        self.use_llm = bool(model_config)\n        self.detailed = detailed if self.use_llm else False\n        self.instruct_list = []\n        self.question_mapping = {\n            \"file\": \"file\",\n            \"function\": \"functions\",\n            \"class\": \"classes\",\n            \"method\": \"classes\",\n        }\n        self.code_qa_list = []\n        self.code_qa_response = \"\"\n\n    def format_response(self):\n        \"\"\"Format the response for the code_qa_dict.\"\"\"\n        self.code_qa_response = (\n            re.sub(\n                r\"\\n\\s*\\n\",\n                \"\\n\",\n                yaml.dump(\n                    self.code_qa_dict,\n                    Dumper=yaml.SafeDumper,\n                    width=float(\"inf\"),\n                    sort_keys=False,\n                    default_flow_style=False,\n                    indent=2,\n                ),\n            )\n            .replace(\"''\", \"'\")\n            .strip('\"')\n            .strip(\"'\")\n            .strip()\n        )\n        self.file_details[\"file_info\"][\"code_qa_response\"] = self.code_qa_response\n\n    def get_response_from_llm(self, query: str, context: str) -> str:\n        \"\"\"\n        Get language model response to query for given context.\n        Args:\n            query (str): Query to be answered.\n            context (str): Context for the query.\n        Returns:\n            str: Language model response.\n        \"\"\"\n        context_strategies = [\n            lambda: str(context),\n            lambda: f\"```python\\n{self.file_details['file_info']['file_code_simplified']}\\n```\",\n            lambda: f\"```python\\n{self.get_info_string(self.file_details['file_info'], 'file_summary')}\\n```\",\n            lambda: \"\",\n        ]\n\n        max_context_length = self.model_config[\"inference_model\"][\"model_params\"][\n            \"context_length\"\n        ]\n        prompt_template = self.model_config[\"prompt_template\"].format(\n            system_prompt=self.model_config[\"system_prompt\"],\n            instruction_prompt=self.model_config[\"instruction_prompt\"],\n        )\n\n        for strategy in context_strategies:\n            context = strategy()\n            prompt = prompt_template.format(\n                context=context, query=query, code_objects=self.code_qa_response\n            )\n            context_size = len(self.llm.tokenize(prompt))\n            logging.info(f\"***Context Size: {context_size}\")\n            if context_size <= 0.70 * max_context_length:\n                break\n        else:\n            err_msg = \"Model response failed, increase py2dataset_model_config.yaml context_length\"\n            logging.error(f\"{err_msg} > {math.ceil(context_size/0.70)}\")\n            return \"\"\n\n        context = self.code_qa_list if context == \"\" else context\n        response = \"\"\n\n        try:\n            response = (\n                re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", self.llm(prompt))\n                .replace(\"<|im_end|>\", \"\")\n                .replace(\"</|im_end|>\", \"\")\n            )\n            response = \"\\n\".join(line.lstrip() for line in response.split(\"\\n\"))\n            logging.info(f\"***Overall Response: {response}\")\n        except Exception as error:\n            logging.error(f\"Failed to generate model response: {error}\")\n\n        if self.detailed:\n            self.get_detailed_response(context, response)\n\n        basename = str(self.base_name).replace(\"\\\\\", \"/\")\n        self.code_qa_dict = {basename: self.code_qa_dict}\n        self.code_qa_dict[basename] = {\n            \"Code Documentation\": [response],\n            **self.code_qa_dict[basename],\n        }\n        self.format_response()\n        self.file_details[\"file_info\"][\"purpose\"] = response\n        return response\n\n    def get_detailed_response(self, context: str, response: str) -> None:\n        \"\"\"\n        Generate detailed responses for code objects.\n        Args:\n            context (str): Context for the response.\n            response (str): Response from the model.\n        \"\"\"\n        for item in self.code_qa_list:\n            try:\n                instruct_key = list(item.keys())[0]\n                instruct_value = list(item.values())[0]\n                query = f\"Describe the Purpose and Significance of these {instruct_key}: [{instruct_value}] and Explain what each of these {instruct_key} does in the code.\"\n                prompt = (\n                    self.model_config[\"prompt_template\"]\n                    .format(\n                        system_prompt=self.model_config[\"system_prompt\"],\n                        instruction_prompt=self.model_config[\"instruction_prompt\"],\n                    )\n                    .format(\n                        context=f\"{context}/nCode Summary:/n{response}\",\n                        query=f\"{query}\",\n                        code_objects=f\"{instruct_value}\",\n                    )\n                )\n                item_response = (\n                    re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", self.llm(prompt))\n                    .replace(\"<|im_end|>\", \"\")\n                    .replace(\"</|im_end|>\", \"\")\n                )\n                logging.info(f\"\\n***Itemized Response: {query}\\n{item_response}\")\n                for item in self.instruct_list:\n                    if item[\"instruction\"].startswith(instruct_key):\n                        output = f\"\\n\\nPurpose and Significance:\\n{item_response}\"\n                        item[\"output\"] += output\n\n                if \"`\" in instruct_key:\n                    dict_key1 = instruct_key.split(\"`\")[1]\n                    dict_key2 = instruct_key.split()[0]\n                else:\n                    dict_key1 = instruct_key\n                    dict_key2 = \"\"\n\n                purpose_dict = {\"Purpose\": item_response.strip()}\n                if dict_key1 not in self.code_qa_dict:\n                    self.code_qa_dict[dict_key1] = {}\n                elif not isinstance(self.code_qa_dict[dict_key1], dict):\n                    value = self.code_qa_dict[dict_key1]\n                    self.code_qa_dict[dict_key1] = {\"Value\": value}\n                if dict_key2:\n                    if not isinstance(self.code_qa_dict[dict_key1], dict):\n                        self.code_qa_dict[dict_key1] = {}\n                    if dict_key2 not in self.code_qa_dict[dict_key1] or not isinstance(\n                        self.code_qa_dict[dict_key1][dict_key2], dict\n                    ):\n                        value = self.code_qa_dict[dict_key1].get(dict_key2, \"\")\n                        self.code_qa_dict[dict_key1][dict_key2] = {\"Value\": value}\n                    self.code_qa_dict[dict_key1][dict_key2].update(purpose_dict)\n                else:\n                    self.code_qa_dict[dict_key1].update(purpose_dict)\n            except Exception as error:\n                logging.error(f\"Failed to generate detailed response: {error}\")\n\n    def get_code_qa(self):\n        \"\"\"Get code responses from the instruct_list and update.\"\"\"\n        excluded = {\"Call code graph\", \"Docstring\"}\n        self.code_qa_list = []\n        responses = {}\n        for item in self.instruct_list:\n            instruction, output = (\n                item[\"instruction\"].split(\" in Python file:\")[0],\n                item[\"output\"],\n            )\n            if not any(instruction.startswith(prefix) for prefix in excluded):\n                self.code_qa_list.append({instruction: output})\n                if \"`\" in instruction:\n                    code_object, code_type = (\n                        instruction.split(\"`\")[1],\n                        instruction.split()[0],\n                    )\n                    responses.setdefault(code_object, []).append((code_type, output))\n                else:\n                    responses.setdefault(instruction, []).append((instruction, output))\n\n        self.code_qa_dict = {}\n        for code_object, type_responses in responses.items():\n            self.code_qa_dict[code_object] = {}\n            for code_type, response in type_responses:\n                if code_object == code_type:\n                    self.code_qa_dict[code_object] = response\n                else:\n                    self.code_qa_dict.setdefault(code_object, {})[code_type] = response\n\n        if not self.use_llm:\n            basename = str(self.base_name).replace(\"\\\\\", \"/\")\n            self.code_qa_dict = {basename: self.code_qa_dict}\n\n        self.format_response()\n\n    def process_question(\n        self, question_id: str, query: str, context: str, info: dict\n    ) -> None:\n        \"\"\"Process question and add the generated response to the instruct_list.\"\"\"\n        if question_id.endswith((\"code_graph\", \"docstring\")):\n            response = info.get(question_id, {})\n        elif question_id.endswith(\"file_purpose\"):  # file_purpose is last question\n            self.get_code_qa()\n            response = (\n                self.get_response_from_llm(query, context) if self.use_llm else \"\"\n            )\n        else:\n            response = get_unique_elements(str(info.get(question_id, \"\")))\n\n        if response and response != \"None\":\n            response = str(response).strip()\n            self.instruct_list.append(\n                {\"instruction\": query, \"input\": context, \"output\": response}\n            )\n\n    @staticmethod\n    def get_info_string(info: dict, item_type: str) -> str:\n        \"\"\"Get string from info dictionary.\"\"\"\n        return \", \".join(\n            [item.strip() for item in str(info.get(item_type, \"\")).split(\",\") if item]\n        )\n\n    def process_question_type(\n        self, question_type: str, question_id: str, question_text: str\n    ) -> None:\n        \"\"\"\n        Process questions related to a file, function, class, or method.\n        Args:\n            question_type (str): Type of question.\n            question_id (str): Question ID.\n            question_text (str): Question text.\n        \"\"\"\n        if question_type == \"file\":\n            query = question_text.format(filename=self.base_name)\n            info = self.file_details[\"file_info\"]\n            context = f\"```python\\n{self.file_details['file_info']['file_code']}\\n```\"\n            self.process_question(question_id, query, context, info)\n        elif question_type == \"method\":\n            for class_name, class_info in self.file_details[\"classes\"].items():\n                for key, method_info in class_info.items():\n                    if key.startswith(\"class_method_\"):\n                        method_name = f\"{class_name}.{key[len('class_method_'):]}\"\n                        context = f\"```python\\n{method_info['method_code']}\\n```\"\n                        mapping = {\"class_name\": class_name, \"method_name\": method_name}\n                        query = question_text.format(filename=self.base_name, **mapping)\n                        self.process_question(question_id, query, context, method_info)\n        else:  # question_type is 'function' or 'class'\n            for name, info in self.file_details[\n                self.question_mapping[question_type]\n            ].items():\n                context = f\"```python\\n{info[f'{question_type}_code']}\\n```\"\n                mapping = {f\"{question_type}_name\": name}\n                if question_id == f\"{question_type}_purpose\" and self.use_llm:\n                    variables = self.get_info_string(info, f\"{question_type}_variables\")\n                    inputs = self.get_info_string(info, f\"{question_type}_inputs\")\n                    combined = \", \".join(filter(None, [variables, inputs]))\n                    mapping[f\"{question_type}_variables\"] = get_unique_elements(\n                        combined\n                    )\n                    if question_type == \"class\":\n                        mapping[f\"{question_type}_methods\"] = self.get_info_string(\n                            info, f\"{question_type}_methods\"\n                        )\n                query = question_text.format(filename=self.base_name, **mapping)\n                self.process_question(question_id, query, context, info)\n\n    def generate(self) -> tuple[list[dict], list[dict]]:\n        \"\"\"Generate responses for all the questions and returns the instruct_list.\"\"\"\n        for question in self.questions:\n            self.process_question_type(\n                question[\"type\"], question[\"id\"], question[\"text\"]\n            )\n        self.instruct_list.sort(key=lambda x: len(x[\"input\"]), reverse=True)\n        return self.instruct_list\n\n\ndef get_python_datasets(\n    file_path: str,\n    file_details: dict,\n    base_name: str,\n    questions: list[dict],\n    model_config: dict,\n    detailed: bool,\n) -> tuple[list[dict], list[dict]]:\n    \"\"\"\n    Extract information from a Python file and return it in JSON format.\n    Args:\n        file_path (str): Path to the Python file.\n        file_details (dict): Dictionary containing details of the Python file.\n        base_name (str): Base name of the Python file.\n        questions (list): List of questions to be answered for the Python file.\n        model_config (dict): Dictionary containing the model configuration.\n        detailed (bool): Flag to generate detailed responses.\n    Returns:\n        tuple: Tuple containing the instruct_list and code_qa_dict.\n    \"\"\"\n    return DatasetGenerator(\n        file_path, file_details, base_name, questions, model_config, detailed\n    ).generate()\n"
    file_dependencies:
    - re
    - yaml
    - logging
    - math
    file_functions:
    - get_unique_elements
    - element_generator
    - get_python_datasets
    file_classes:
    - DatasetGenerator
    file_constants: []
    file_summary: '{dependencies: [re, yaml, logging, math], function_defs: [{get_unique_elements: {inputs: [input_str], calls: [enumerate, input_str[start:i].strip, input_str[start:].strip, input_str.strip, element_generator, '', ''.join], call_inputs: {enumerate: [input_str], input_str[start:i].strip: [''\\''\ ''], input_str[start:].strip: [''\\''\ ''], input_str.strip: [''[]\\''\''], element_generator: [input_str], '', ''.join: [cleaned_elements]}, returns: ['', ''.join(cleaned_elements)]}}, {element_generator: {inputs: [input_str], calls: [enumerate, input_str[start:i].strip, input_str[start:].strip], call_inputs: {enumerate: [input_str], input_str[start:i].strip: [''\\''\ ''], input_str[start:].strip: [''\\''\ '']}, returns: []}}, {get_python_datasets: {inputs: [file_path, file_details, base_name, questions, model_config, detailed], calls: [DatasetGenerator(file_path, file_details, base_name, questions, model_config, detailed).generate, DatasetGenerator], call_inputs: {DatasetGenerator(file_path, file_details, base_name, questions, model_config, detailed).generate: [], DatasetGenerator: [file_path, file_details, base_name, questions, model_config, detailed]}, returns: [DatasetGenerator(file_path, file_details, base_name, questions, model_config, detailed).generate()]}}], class_defs: [{DatasetGenerator: {method_defs: {__init__: {inputs: [self, file_path, file_details, base_name, questions, model_config, detailed], calls: [bool], call_inputs: {bool: [model_config]}, returns: []}, format_response: {inputs: [self], calls: [re.sub(''\\\\n\\\\s*\\\\n'', ''\\n'', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float(''inf''), sort_keys=False, default_flow_style=False, indent=2)).replace(\''''\, \''\).strip(''\'').strip(\''\).strip, re.sub(''\\\\n\\\\s*\\\\n'', ''\\n'', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float(''inf''), sort_keys=False, default_flow_style=False, indent=2)).replace(\''''\, \''\).strip(''\'').strip, re.sub(''\\\\n\\\\s*\\\\n'', ''\\n'', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float(''inf''), sort_keys=False, default_flow_style=False, indent=2)).replace(\''''\, \''\).strip, re.sub(''\\\\n\\\\s*\\\\n'', ''\\n'', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float(''inf''), sort_keys=False, default_flow_style=False, indent=2)).replace, re.sub, yaml.dump, float], call_inputs: {re.sub(''\\\\n\\\\s*\\\\n'', ''\\n'', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float(''inf''), sort_keys=False, default_flow_style=False, indent=2)).replace(\''''\, \''\).strip(''\'').strip(\''\).strip: [], re.sub(''\\\\n\\\\s*\\\\n'', ''\\n'', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float(''inf''), sort_keys=False, default_flow_style=False, indent=2)).replace(\''''\, \''\).strip(''\'').strip: [\''\], re.sub(''\\\\n\\\\s*\\\\n'', ''\\n'', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float(''inf''), sort_keys=False, default_flow_style=False, indent=2)).replace(\''''\, \''\).strip: [''\''], re.sub(''\\\\n\\\\s*\\\\n'', ''\\n'', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float(''inf''), sort_keys=False, default_flow_style=False, indent=2)).replace: [\''''\, \''\], re.sub: [''\\\\n\\\\s*\\\\n'', ''\\n'', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float(''inf''), sort_keys=False, default_flow_style=False, indent=2)], yaml.dump: [self.code_qa_dict], float: [''inf'']}, returns: []}, get_response_from_llm: {inputs: [self, query, context], calls: [str, self.get_info_string, self.model_config[''prompt_template''].format, strategy, prompt_template.format, len, self.llm.tokenize, logging.info, logging.error, math.ceil, re.sub(''\\\\n\\\\s*\\\\n'', ''\\n\\n'', self.llm(prompt)).replace(''<|im_end|>'', '''').replace, re.sub(''\\\\n\\\\s*\\\\n'', ''\\n\\n'', self.llm(prompt)).replace, re.sub, self.llm, ''\\n''.join, line.lstrip, response.split, self.get_detailed_response, str(self.base_name).replace, self.format_response], call_inputs: {str: [context, self.base_name], self.get_info_string: [self.file_details[''file_info''], ''file_summary''], self.model_config[''prompt_template''].format: [], strategy: [], prompt_template.format: [], len: [self.llm.tokenize(prompt)], self.llm.tokenize: [prompt], logging.info: [f''***Context Size: {context_size}'', f''***Overall Response: {response}''], logging.error: [f''{err_msg} > {math.ceil(context_size / 0.7)}'', f''Failed to generate model response: {error}''], math.ceil: [context_size / 0.7], re.sub(''\\\\n\\\\s*\\\\n'', ''\\n\\n'', self.llm(prompt)).replace(''<|im_end|>'', '''').replace: [''</|im_end|>'', ''''], re.sub(''\\\\n\\\\s*\\\\n'', ''\\n\\n'', self.llm(prompt)).replace: [''<|im_end|>'', ''''], re.sub: [''\\\\n\\\\s*\\\\n'', ''\\n\\n'', self.llm(prompt)], self.llm: [prompt], ''\\n''.join: [(line.lstrip() for line in response.split(''\\n''))], line.lstrip: [], response.split: [''\\n''], self.get_detailed_response: [context, response], str(self.base_name).replace: [''\\\\'', ''/''], self.format_response: []}, returns: [response, '''']}, get_detailed_response: {inputs: [self, context, response], calls: [list, item.keys, item.values, self.model_config[''prompt_template''].format(system_prompt=self.model_config[''system_prompt''], instruction_prompt=self.model_config[''instruction_prompt'']).format, self.model_config[''prompt_template''].format, re.sub(''\\\\n\\\\s*\\\\n'', ''\\n\\n'', self.llm(prompt)).replace(''<|im_end|>'', '''').replace, re.sub(''\\\\n\\\\s*\\\\n'', ''\\n\\n'', self.llm(prompt)).replace, re.sub, self.llm, logging.info, item[''instruction''].startswith, instruct_key.split, item_response.strip, isinstance, self.code_qa_dict[dict_key1].get, self.code_qa_dict[dict_key1][dict_key2].update, self.code_qa_dict[dict_key1].update, logging.error], call_inputs: {list: [item.keys(), item.values()], item.keys: [], item.values: [], self.model_config[''prompt_template''].format(system_prompt=self.model_config[''system_prompt''], instruction_prompt=self.model_config[''instruction_prompt'']).format: [], self.model_config[''prompt_template''].format: [], re.sub(''\\\\n\\\\s*\\\\n'', ''\\n\\n'', self.llm(prompt)).replace(''<|im_end|>'', '''').replace: [''</|im_end|>'', ''''], re.sub(''\\\\n\\\\s*\\\\n'', ''\\n\\n'', self.llm(prompt)).replace: [''<|im_end|>'', ''''], re.sub: [''\\\\n\\\\s*\\\\n'', ''\\n\\n'', self.llm(prompt)], self.llm: [prompt], logging.info: [f''\\n***Itemized Response: {query}\\n{item_response}''], item[''instruction''].startswith: [instruct_key], instruct_key.split: [''`''], item_response.strip: [], isinstance: [self.code_qa_dict[dict_key1], dict, self.code_qa_dict[dict_key1], dict, self.code_qa_dict[dict_key1][dict_key2], dict], self.code_qa_dict[dict_key1].get: [dict_key2, ''''], self.code_qa_dict[dict_key1][dict_key2].update: [purpose_dict], self.code_qa_dict[dict_key1].update: [purpose_dict], logging.error: [f''Failed to generate detailed response: {error}'']}, returns: []}, get_code_qa: {inputs: [self], calls: [item[''instruction''].split, any, instruction.startswith, self.code_qa_list.append, instruction.split, responses.setdefault(code_object, []).append, responses.setdefault, responses.setdefault(instruction, []).append, responses.items, self.code_qa_dict.setdefault, str(self.base_name).replace, str, self.format_response], call_inputs: {item[''instruction''].split: ['' in Python file:''], any: [(instruction.startswith(prefix) for prefix in excluded)], instruction.startswith: [prefix], self.code_qa_list.append: [{instruction: output}], instruction.split: [''`''], responses.setdefault(code_object, []).append: [(code_type, output)], responses.setdefault: [code_object, [], instruction, []], responses.setdefault(instruction, []).append: [(instruction, output)], responses.items: [], self.code_qa_dict.setdefault: [code_object, {}], str(self.base_name).replace: [''\\\\'', ''/''], str: [self.base_name], self.format_response: []}, returns: []}, process_question: {inputs: [self, question_id, query, context, info], calls: [question_id.endswith, info.get, self.get_code_qa, self.get_response_from_llm, get_unique_elements, str, str(response).strip, self.instruct_list.append], call_inputs: {question_id.endswith: [(''code_graph'', ''docstring''), ''file_purpose''], info.get: [question_id, {}, question_id, ''''], self.get_code_qa: [], self.get_response_from_llm: [query, context], get_unique_elements: [str(info.get(question_id, ''''))], str: [info.get(question_id, ''''), response], str(response).strip: [], self.instruct_list.append: [{''instruction'': query, ''input'': context, ''output'': response}]}, returns: []}, get_info_string: {inputs: [info, item_type], calls: ['', ''.join, item.strip, str(info.get(item_type, '''')).split, str, info.get], call_inputs: {'', ''.join: [[item.strip() for item in str(info.get(item_type, '''')).split('','') if item]], item.strip: [], str(info.get(item_type, '''')).split: ['',''], str: [info.get(item_type, '''')], info.get: [item_type, '''']}, returns: ['', ''.join([item.strip() for item in str(info.get(item_type, '''')).split('','') if item])]}, process_question_type: {inputs: [self, question_type, question_id, question_text], calls: [question_text.format, self.process_question, self.file_details[''classes''].items, class_info.items, key.startswith, len, self.file_details[self.question_mapping[question_type]].items, self.get_info_string, '', ''.join, filter, get_unique_elements], call_inputs: {question_text.format: [], self.process_question: [question_id, query, context, info, question_id, query, context, method_info, question_id, query, context, info], self.file_details[''classes''].items: [], class_info.items: [], key.startswith: [''class_method_''], len: [''class_method_''], self.file_details[self.question_mapping[question_type]].items: [], self.get_info_string: [info, f''{question_type}_variables'', info, f''{question_type}_inputs'', info, f''{question_type}_methods''], '', ''.join: [filter(None, [variables, inputs])], filter: [None, [variables, inputs]], get_unique_elements: [combined]}, returns: []}, generate: {inputs: [self], calls: [self.process_question_type, self.instruct_list.sort, len], call_inputs: {self.process_question_type: [question[''type''], question[''id''], question[''text'']], self.instruct_list.sort: [], len: [x[''input'']]}, returns: [self.instruct_list]}}}}]}'
    file_code_simplified: "import logging\nimport re\nimport math\nimport yaml\n\ndef get_unique_elements(input_str: str) -> str:\n\n    def element_generator(input_str):\n        start, brace_level = (0, 0)\n        for i, char in enumerate(input_str):\n            if char in '{}':\n                brace_level += 1 if char == '{' else -1\n            elif char == ',' and brace_level == 0:\n                yield input_str[start:i].strip('\\'\" ')\n                start = i + 1\n        yield input_str[start:].strip('\\'\" ')\n    input_str = input_str.strip('[]\\'\"')\n    cleaned_elements = [element for element in element_generator(input_str) if element]\n    return ', '.join(cleaned_elements)\n\nclass DatasetGenerator:\n\n    def __init__(self, file_path: str, file_details: dict, base_name: str, questions: list[dict], model_config: dict, detailed: bool) -> None:\n        self.file_path = file_path\n        self.file_details = file_details\n        self.base_name = base_name\n        self.questions = questions\n        self.model_config = model_config\n        self.llm = model_config['model'] if model_config else None\n        self.use_llm = bool(model_config)\n        self.detailed = detailed if self.use_llm else False\n        self.instruct_list = []\n        self.question_mapping = {'file': 'file', 'function': 'functions', 'class': 'classes', 'method': 'classes'}\n        self.code_qa_list = []\n        self.code_qa_response = ''\n\n    def format_response(self):\n        self.code_qa_response = re.sub('\\\\n\\\\s*\\\\n', '\\n', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False, indent=2)).replace(\"''\", \"'\").strip('\"').strip(\"'\").strip()\n        self.file_details['file_info']['code_qa_response'] = self.code_qa_response\n\n    def get_response_from_llm(self, query: str, context: str) -> str:\n        context_strategies = [lambda: str(context), lambda: f\"```python\\n{self.file_details['file_info']['file_code_simplified']}\\n```\", lambda: f\"```python\\n{self.get_info_string(self.file_details['file_info'], 'file_summary')}\\n```\", lambda: '']\n        max_context_length = self.model_config['inference_model']['model_params']['context_length']\n        prompt_template = self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'], instruction_prompt=self.model_config['instruction_prompt'])\n        for strategy in context_strategies:\n            context = strategy()\n            prompt = prompt_template.format(context=context, query=query, code_objects=self.code_qa_response)\n            context_size = len(self.llm.tokenize(prompt))\n            logging.info(f'***Context Size: {context_size}')\n            if context_size <= 0.7 * max_context_length:\n                break\n        else:\n            err_msg = 'Model response failed, increase py2dataset_model_config.yaml context_length'\n            logging.error(f'{err_msg} > {math.ceil(context_size / 0.7)}')\n            return ''\n        context = self.code_qa_list if context == '' else context\n        response = ''\n        try:\n            response = re.sub('\\\\n\\\\s*\\\\n', '\\n\\n', self.llm(prompt)).replace('<|im_end|>', '').replace('</|im_end|>', '')\n            response = '\\n'.join((line.lstrip() for line in response.split('\\n')))\n            logging.info(f'***Overall Response: {response}')\n        except Exception as error:\n            logging.error(f'Failed to generate model response: {error}')\n        if self.detailed:\n            self.get_detailed_response(context, response)\n        basename = str(self.base_name).replace('\\\\', '/')\n        self.code_qa_dict = {basename: self.code_qa_dict}\n        self.code_qa_dict[basename] = {'Code Documentation': [response], **self.code_qa_dict[basename]}\n        self.format_response()\n        self.file_details['file_info']['purpose'] = response\n        return response\n\n    def get_detailed_response(self, context: str, response: str) -> None:\n        for item in self.code_qa_list:\n            try:\n                instruct_key = list(item.keys())[0]\n                instruct_value = list(item.values())[0]\n                query = f'Describe the Purpose and Significance of these {instruct_key}: [{instruct_value}] and Explain what each of these {instruct_key} does in the code.'\n                prompt = self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'], instruction_prompt=self.model_config['instruction_prompt']).format(context=f'{context}/nCode Summary:/n{response}', query=f'{query}', code_objects=f'{instruct_value}')\n                item_response = re.sub('\\\\n\\\\s*\\\\n', '\\n\\n', self.llm(prompt)).replace('<|im_end|>', '').replace('</|im_end|>', '')\n                logging.info(f'\\n***Itemized Response: {query}\\n{item_response}')\n                for item in self.instruct_list:\n                    if item['instruction'].startswith(instruct_key):\n                        output = f'\\n\\nPurpose and Significance:\\n{item_response}'\n                        item['output'] += output\n                if '`' in instruct_key:\n                    dict_key1 = instruct_key.split('`')[1]\n                    dict_key2 = instruct_key.split()[0]\n                else:\n                    dict_key1 = instruct_key\n                    dict_key2 = ''\n                purpose_dict = {'Purpose': item_response.strip()}\n                if dict_key1 not in self.code_qa_dict:\n                    self.code_qa_dict[dict_key1] = {}\n                elif not isinstance(self.code_qa_dict[dict_key1], dict):\n                    value = self.code_qa_dict[dict_key1]\n                    self.code_qa_dict[dict_key1] = {'Value': value}\n                if dict_key2:\n                    if not isinstance(self.code_qa_dict[dict_key1], dict):\n                        self.code_qa_dict[dict_key1] = {}\n                    if dict_key2 not in self.code_qa_dict[dict_key1] or not isinstance(self.code_qa_dict[dict_key1][dict_key2], dict):\n                        value = self.code_qa_dict[dict_key1].get(dict_key2, '')\n                        self.code_qa_dict[dict_key1][dict_key2] = {'Value': value}\n                    self.code_qa_dict[dict_key1][dict_key2].update(purpose_dict)\n                else:\n                    self.code_qa_dict[dict_key1].update(purpose_dict)\n            except Exception as error:\n                logging.error(f'Failed to generate detailed response: {error}')\n\n    def get_code_qa(self):\n        excluded = {'Call code graph', 'Docstring'}\n        self.code_qa_list = []\n        responses = {}\n        for item in self.instruct_list:\n            instruction, output = (item['instruction'].split(' in Python file:')[0], item['output'])\n            if not any((instruction.startswith(prefix) for prefix in excluded)):\n                self.code_qa_list.append({instruction: output})\n                if '`' in instruction:\n                    code_object, code_type = (instruction.split('`')[1], instruction.split()[0])\n                    responses.setdefault(code_object, []).append((code_type, output))\n                else:\n                    responses.setdefault(instruction, []).append((instruction, output))\n        self.code_qa_dict = {}\n        for code_object, type_responses in responses.items():\n            self.code_qa_dict[code_object] = {}\n            for code_type, response in type_responses:\n                if code_object == code_type:\n                    self.code_qa_dict[code_object] = response\n                else:\n                    self.code_qa_dict.setdefault(code_object, {})[code_type] = response\n        if not self.use_llm:\n            basename = str(self.base_name).replace('\\\\', '/')\n            self.code_qa_dict = {basename: self.code_qa_dict}\n        self.format_response()\n\n    def process_question(self, question_id: str, query: str, context: str, info: dict) -> None:\n        if question_id.endswith(('code_graph', 'docstring')):\n            response = info.get(question_id, {})\n        elif question_id.endswith('file_purpose'):\n            self.get_code_qa()\n            response = self.get_response_from_llm(query, context) if self.use_llm else ''\n        else:\n            response = get_unique_elements(str(info.get(question_id, '')))\n        if response and response != 'None':\n            response = str(response).strip()\n            self.instruct_list.append({'instruction': query, 'input': context, 'output': response})\n\n    @staticmethod\n    def get_info_string(info: dict, item_type: str) -> str:\n        return ', '.join([item.strip() for item in str(info.get(item_type, '')).split(',') if item])\n\n    def process_question_type(self, question_type: str, question_id: str, question_text: str) -> None:\n        if question_type == 'file':\n            query = question_text.format(filename=self.base_name)\n            info = self.file_details['file_info']\n            context = f\"```python\\n{self.file_details['file_info']['file_code']}\\n```\"\n            self.process_question(question_id, query, context, info)\n        elif question_type == 'method':\n            for class_name, class_info in self.file_details['classes'].items():\n                for key, method_info in class_info.items():\n                    if key.startswith('class_method_'):\n                        method_name = f\"{class_name}.{key[len('class_method_'):]}\"\n                        context = f\"```python\\n{method_info['method_code']}\\n```\"\n                        mapping = {'class_name': class_name, 'method_name': method_name}\n                        query = question_text.format(filename=self.base_name, **mapping)\n                        self.process_question(question_id, query, context, method_info)\n        else:\n            for name, info in self.file_details[self.question_mapping[question_type]].items():\n                context = f\"```python\\n{info[f'{question_type}_code']}\\n```\"\n                mapping = {f'{question_type}_name': name}\n                if question_id == f'{question_type}_purpose' and self.use_llm:\n                    variables = self.get_info_string(info, f'{question_type}_variables')\n                    inputs = self.get_info_string(info, f'{question_type}_inputs')\n                    combined = ', '.join(filter(None, [variables, inputs]))\n                    mapping[f'{question_type}_variables'] = get_unique_elements(combined)\n                    if question_type == 'class':\n                        mapping[f'{question_type}_methods'] = self.get_info_string(info, f'{question_type}_methods')\n                query = question_text.format(filename=self.base_name, **mapping)\n                self.process_question(question_id, query, context, info)\n\n    def generate(self) -> tuple[list[dict], list[dict]]:\n        for question in self.questions:\n            self.process_question_type(question['type'], question['id'], question['text'])\n        self.instruct_list.sort(key=lambda x: len(x['input']), reverse=True)\n        return self.instruct_list\n\ndef get_python_datasets(file_path: str, file_details: dict, base_name: str, questions: list[dict], model_config: dict, detailed: bool) -> tuple[list[dict], list[dict]]:\n    return DatasetGenerator(file_path, file_details, base_name, questions, model_config, detailed).generate()"
    entire_code_graph:
        nodes:
        - DatasetGenerator
        - DatasetGenerator.__init__
        - DatasetGenerator.format_response
        - DatasetGenerator.get_response_from_llm
        - DatasetGenerator.get_detailed_response
        - DatasetGenerator.get_code_qa
        - DatasetGenerator.process_question
        - DatasetGenerator.get_info_string
        - DatasetGenerator.process_question_type
        - DatasetGenerator.generate
        - get_unique_elements
        - element_generator
        - get_python_datasets
        - enumerate
        - input_str[start:i].strip
        - input_str[start:].strip
        - input_str.strip
        - ''', ''.join'
        - DatasetGenerator(file_path, file_details, base_name, questions, model_config, detailed).generate
        - bool
        - re.sub('\\n\\s*\\n', '\n', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False, indent=2)).replace("''", "'").strip('"').strip("'").strip
        - re.sub('\\n\\s*\\n', '\n', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False, indent=2)).replace("''", "'").strip('"').strip
        - re.sub('\\n\\s*\\n', '\n', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False, indent=2)).replace("''", "'").strip
        - re.sub('\\n\\s*\\n', '\n', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False, indent=2)).replace
        - re.sub
        - yaml.dump
        - float
        - str
        - self.model_config['prompt_template'].format
        - strategy
        - prompt_template.format
        - len
        - self.llm.tokenize
        - logging.info
        - logging.error
        - math.ceil
        - re.sub('\\n\\s*\\n', '\n\n', self.llm(prompt)).replace('<|im_end|>', '').replace
        - re.sub('\\n\\s*\\n', '\n\n', self.llm(prompt)).replace
        - self.llm
        - '''\n''.join'
        - line.lstrip
        - response.split
        - str(self.base_name).replace
        - list
        - item.keys
        - item.values
        - self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'], instruction_prompt=self.model_config['instruction_prompt']).format
        - item['instruction'].startswith
        - instruct_key.split
        - item_response.strip
        - isinstance
        - self.code_qa_dict[dict_key1].get
        - self.code_qa_dict[dict_key1][dict_key2].update
        - self.code_qa_dict[dict_key1].update
        - item['instruction'].split
        - any
        - instruction.startswith
        - self.code_qa_list.append
        - instruction.split
        - responses.setdefault(code_object, []).append
        - responses.setdefault
        - responses.setdefault(instruction, []).append
        - responses.items
        - self.code_qa_dict.setdefault
        - question_id.endswith
        - info.get
        - str(response).strip
        - self.instruct_list.append
        - item.strip
        - str(info.get(item_type, '')).split
        - question_text.format
        - self.file_details['classes'].items
        - class_info.items
        - key.startswith
        - self.file_details[self.question_mapping[question_type]].items
        - filter
        - self.instruct_list.sort
        edges:
        -   source: DatasetGenerator
            target: DatasetGenerator.__init__
            target_inputs:
            - self
            - file_path
            - file_details
            - base_name
            - questions
            - model_config
            - detailed
            target_returns: []
        -   source: DatasetGenerator
            target: DatasetGenerator.format_response
            target_inputs:
            - self
            target_returns: []
        -   source: DatasetGenerator
            target: DatasetGenerator.get_response_from_llm
            target_inputs:
            - self
            - query
            - context
            target_returns:
            - ''''''
            - response
        -   source: DatasetGenerator
            target: DatasetGenerator.get_detailed_response
            target_inputs:
            - self
            - context
            - response
            target_returns: []
        -   source: DatasetGenerator
            target: DatasetGenerator.get_code_qa
            target_inputs:
            - self
            target_returns: []
        -   source: DatasetGenerator
            target: DatasetGenerator.process_question
            target_inputs:
            - self
            - question_id
            - query
            - context
            - info
            target_returns: []
        -   source: DatasetGenerator
            target: DatasetGenerator.get_info_string
            target_inputs:
            - info
            - item_type
            target_returns:
            - ''', ''.join([item.strip() for item in str(info.get(item_type, '''')).split('','') if item])'
        -   source: DatasetGenerator
            target: DatasetGenerator.process_question_type
            target_inputs:
            - self
            - question_type
            - question_id
            - question_text
            target_returns: []
        -   source: DatasetGenerator
            target: DatasetGenerator.generate
            target_inputs:
            - self
            target_returns:
            - self.instruct_list
        -   source: DatasetGenerator.__init__
            target: bool
            target_inputs:
            - model_config
        -   source: DatasetGenerator.format_response
            target: re.sub('\\n\\s*\\n', '\n', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False, indent=2)).replace("''", "'").strip('"').strip("'").strip
            target_inputs: []
        -   source: DatasetGenerator.format_response
            target: re.sub('\\n\\s*\\n', '\n', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False, indent=2)).replace("''", "'").strip('"').strip
            target_inputs:
            - '"''"'
        -   source: DatasetGenerator.format_response
            target: re.sub('\\n\\s*\\n', '\n', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False, indent=2)).replace("''", "'").strip
            target_inputs:
            - '''"'''
        -   source: DatasetGenerator.format_response
            target: re.sub('\\n\\s*\\n', '\n', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False, indent=2)).replace
            target_inputs:
            - '"''''"'
            - '"''"'
        -   source: DatasetGenerator.format_response
            target: re.sub
            target_inputs:
            - '''\\n\\s*\\n'''
            - '''\n'''
            - yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False, indent=2)
        -   source: DatasetGenerator.format_response
            target: yaml.dump
            target_inputs:
            - self.code_qa_dict
        -   source: DatasetGenerator.format_response
            target: float
            target_inputs:
            - '''inf'''
        -   source: DatasetGenerator.get_response_from_llm
            target: str
            target_inputs:
            - context
            - self.base_name
        -   source: DatasetGenerator.get_response_from_llm
            target: DatasetGenerator.get_info_string
            target_inputs:
            - info
            - item_type
            target_returns:
            - ''', ''.join([item.strip() for item in str(info.get(item_type, '''')).split('','') if item])'
        -   source: DatasetGenerator.get_response_from_llm
            target: self.model_config['prompt_template'].format
            target_inputs: []
        -   source: DatasetGenerator.get_response_from_llm
            target: strategy
            target_inputs: []
        -   source: DatasetGenerator.get_response_from_llm
            target: prompt_template.format
            target_inputs: []
        -   source: DatasetGenerator.get_response_from_llm
            target: len
            target_inputs:
            - self.llm.tokenize(prompt)
        -   source: DatasetGenerator.get_response_from_llm
            target: self.llm.tokenize
            target_inputs:
            - prompt
        -   source: DatasetGenerator.get_response_from_llm
            target: logging.info
            target_inputs:
            - 'f''***Context Size: {context_size}'''
            - 'f''***Overall Response: {response}'''
        -   source: DatasetGenerator.get_response_from_llm
            target: logging.error
            target_inputs:
            - f'{err_msg} > {math.ceil(context_size / 0.7)}'
            - 'f''Failed to generate model response: {error}'''
        -   source: DatasetGenerator.get_response_from_llm
            target: math.ceil
            target_inputs:
            - context_size / 0.7
        -   source: DatasetGenerator.get_response_from_llm
            target: re.sub('\\n\\s*\\n', '\n\n', self.llm(prompt)).replace('<|im_end|>', '').replace
            target_inputs:
            - '''</|im_end|>'''
            - ''''''
        -   source: DatasetGenerator.get_response_from_llm
            target: re.sub('\\n\\s*\\n', '\n\n', self.llm(prompt)).replace
            target_inputs:
            - '''<|im_end|>'''
            - ''''''
        -   source: DatasetGenerator.get_response_from_llm
            target: re.sub
            target_inputs:
            - '''\\n\\s*\\n'''
            - '''\n\n'''
            - self.llm(prompt)
        -   source: DatasetGenerator.get_response_from_llm
            target: self.llm
            target_inputs:
            - prompt
        -   source: DatasetGenerator.get_response_from_llm
            target: '''\n''.join'
            target_inputs:
            - (line.lstrip() for line in response.split('\n'))
        -   source: DatasetGenerator.get_response_from_llm
            target: line.lstrip
            target_inputs: []
        -   source: DatasetGenerator.get_response_from_llm
            target: response.split
            target_inputs:
            - '''\n'''
        -   source: DatasetGenerator.get_response_from_llm
            target: DatasetGenerator.get_detailed_response
            target_inputs:
            - self
            - context
            - response
            target_returns: []
        -   source: DatasetGenerator.get_response_from_llm
            target: str(self.base_name).replace
            target_inputs:
            - '''\\'''
            - '''/'''
        -   source: DatasetGenerator.get_response_from_llm
            target: DatasetGenerator.format_response
            target_inputs:
            - self
            target_returns: []
        -   source: DatasetGenerator.get_detailed_response
            target: list
            target_inputs:
            - item.keys()
            - item.values()
        -   source: DatasetGenerator.get_detailed_response
            target: item.keys
            target_inputs: []
        -   source: DatasetGenerator.get_detailed_response
            target: item.values
            target_inputs: []
        -   source: DatasetGenerator.get_detailed_response
            target: self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'], instruction_prompt=self.model_config['instruction_prompt']).format
            target_inputs: []
        -   source: DatasetGenerator.get_detailed_response
            target: self.model_config['prompt_template'].format
            target_inputs: []
        -   source: DatasetGenerator.get_detailed_response
            target: re.sub('\\n\\s*\\n', '\n\n', self.llm(prompt)).replace('<|im_end|>', '').replace
            target_inputs:
            - '''</|im_end|>'''
            - ''''''
        -   source: DatasetGenerator.get_detailed_response
            target: re.sub('\\n\\s*\\n', '\n\n', self.llm(prompt)).replace
            target_inputs:
            - '''<|im_end|>'''
            - ''''''
        -   source: DatasetGenerator.get_detailed_response
            target: re.sub
            target_inputs:
            - '''\\n\\s*\\n'''
            - '''\n\n'''
            - self.llm(prompt)
        -   source: DatasetGenerator.get_detailed_response
            target: self.llm
            target_inputs:
            - prompt
        -   source: DatasetGenerator.get_detailed_response
            target: logging.info
            target_inputs:
            - 'f''\n***Itemized Response: {query}\n{item_response}'''
        -   source: DatasetGenerator.get_detailed_response
            target: item['instruction'].startswith
            target_inputs:
            - instruct_key
        -   source: DatasetGenerator.get_detailed_response
            target: instruct_key.split
            target_inputs:
            - '''`'''
        -   source: DatasetGenerator.get_detailed_response
            target: item_response.strip
            target_inputs: []
        -   source: DatasetGenerator.get_detailed_response
            target: isinstance
            target_inputs:
            - self.code_qa_dict[dict_key1]
            - dict
            - self.code_qa_dict[dict_key1]
            - dict
            - self.code_qa_dict[dict_key1][dict_key2]
            - dict
        -   source: DatasetGenerator.get_detailed_response
            target: self.code_qa_dict[dict_key1].get
            target_inputs:
            - dict_key2
            - ''''''
        -   source: DatasetGenerator.get_detailed_response
            target: self.code_qa_dict[dict_key1][dict_key2].update
            target_inputs:
            - purpose_dict
        -   source: DatasetGenerator.get_detailed_response
            target: self.code_qa_dict[dict_key1].update
            target_inputs:
            - purpose_dict
        -   source: DatasetGenerator.get_detailed_response
            target: logging.error
            target_inputs:
            - 'f''Failed to generate detailed response: {error}'''
        -   source: DatasetGenerator.get_code_qa
            target: item['instruction'].split
            target_inputs:
            - ''' in Python file:'''
        -   source: DatasetGenerator.get_code_qa
            target: any
            target_inputs:
            - (instruction.startswith(prefix) for prefix in excluded)
        -   source: DatasetGenerator.get_code_qa
            target: instruction.startswith
            target_inputs:
            - prefix
        -   source: DatasetGenerator.get_code_qa
            target: self.code_qa_list.append
            target_inputs:
            - '{instruction: output}'
        -   source: DatasetGenerator.get_code_qa
            target: instruction.split
            target_inputs:
            - '''`'''
        -   source: DatasetGenerator.get_code_qa
            target: responses.setdefault(code_object, []).append
            target_inputs:
            - (code_type, output)
        -   source: DatasetGenerator.get_code_qa
            target: responses.setdefault
            target_inputs:
            - code_object
            - '[]'
            - instruction
            - '[]'
        -   source: DatasetGenerator.get_code_qa
            target: responses.setdefault(instruction, []).append
            target_inputs:
            - (instruction, output)
        -   source: DatasetGenerator.get_code_qa
            target: responses.items
            target_inputs: []
        -   source: DatasetGenerator.get_code_qa
            target: self.code_qa_dict.setdefault
            target_inputs:
            - code_object
            - '{}'
        -   source: DatasetGenerator.get_code_qa
            target: str(self.base_name).replace
            target_inputs:
            - '''\\'''
            - '''/'''
        -   source: DatasetGenerator.get_code_qa
            target: str
            target_inputs:
            - self.base_name
        -   source: DatasetGenerator.get_code_qa
            target: DatasetGenerator.format_response
            target_inputs:
            - self
            target_returns: []
        -   source: DatasetGenerator.process_question
            target: question_id.endswith
            target_inputs:
            - ('code_graph', 'docstring')
            - '''file_purpose'''
        -   source: DatasetGenerator.process_question
            target: info.get
            target_inputs:
            - question_id
            - '{}'
            - question_id
            - ''''''
        -   source: DatasetGenerator.process_question
            target: DatasetGenerator.get_code_qa
            target_inputs:
            - self
            target_returns: []
        -   source: DatasetGenerator.process_question
            target: DatasetGenerator.get_response_from_llm
            target_inputs:
            - self
            - query
            - context
            target_returns:
            - ''''''
            - response
        -   source: DatasetGenerator.process_question
            target: get_unique_elements
            target_inputs:
            - str(info.get(question_id, ''))
            target_returns:
            - ''', ''.join(cleaned_elements)'
        -   source: DatasetGenerator.process_question
            target: str
            target_inputs:
            - info.get(question_id, '')
            - response
        -   source: DatasetGenerator.process_question
            target: str(response).strip
            target_inputs: []
        -   source: DatasetGenerator.process_question
            target: self.instruct_list.append
            target_inputs:
            - '{''instruction'': query, ''input'': context, ''output'': response}'
        -   source: DatasetGenerator.get_info_string
            target: ''', ''.join'
            target_inputs:
            - '[item.strip() for item in str(info.get(item_type, '''')).split('','') if item]'
        -   source: DatasetGenerator.get_info_string
            target: item.strip
            target_inputs: []
        -   source: DatasetGenerator.get_info_string
            target: str(info.get(item_type, '')).split
            target_inputs:
            - ''','''
        -   source: DatasetGenerator.get_info_string
            target: str
            target_inputs:
            - info.get(item_type, '')
        -   source: DatasetGenerator.get_info_string
            target: info.get
            target_inputs:
            - item_type
            - ''''''
        -   source: DatasetGenerator.process_question_type
            target: question_text.format
            target_inputs: []
        -   source: DatasetGenerator.process_question_type
            target: DatasetGenerator.process_question
            target_inputs:
            - self
            - question_id
            - query
            - context
            - info
            target_returns: []
        -   source: DatasetGenerator.process_question_type
            target: self.file_details['classes'].items
            target_inputs: []
        -   source: DatasetGenerator.process_question_type
            target: class_info.items
            target_inputs: []
        -   source: DatasetGenerator.process_question_type
            target: key.startswith
            target_inputs:
            - '''class_method_'''
        -   source: DatasetGenerator.process_question_type
            target: len
            target_inputs:
            - '''class_method_'''
        -   source: DatasetGenerator.process_question_type
            target: self.file_details[self.question_mapping[question_type]].items
            target_inputs: []
        -   source: DatasetGenerator.process_question_type
            target: DatasetGenerator.get_info_string
            target_inputs:
            - info
            - item_type
            target_returns:
            - ''', ''.join([item.strip() for item in str(info.get(item_type, '''')).split('','') if item])'
        -   source: DatasetGenerator.process_question_type
            target: ''', ''.join'
            target_inputs:
            - filter(None, [variables, inputs])
        -   source: DatasetGenerator.process_question_type
            target: filter
            target_inputs:
            - None
            - '[variables, inputs]'
        -   source: DatasetGenerator.process_question_type
            target: get_unique_elements
            target_inputs:
            - combined
            target_returns:
            - ''', ''.join(cleaned_elements)'
        -   source: DatasetGenerator.generate
            target: DatasetGenerator.process_question_type
            target_inputs:
            - self
            - question_type
            - question_id
            - question_text
            target_returns: []
        -   source: DatasetGenerator.generate
            target: self.instruct_list.sort
            target_inputs: []
        -   source: DatasetGenerator.generate
            target: len
            target_inputs:
            - x['input']
        -   source: get_unique_elements
            target: enumerate
            target_inputs:
            - input_str
        -   source: get_unique_elements
            target: input_str[start:i].strip
            target_inputs:
            - '''\''" '''
        -   source: get_unique_elements
            target: input_str[start:].strip
            target_inputs:
            - '''\''" '''
        -   source: get_unique_elements
            target: input_str.strip
            target_inputs:
            - '''[]\''"'''
        -   source: get_unique_elements
            target: element_generator
            target_inputs:
            - input_str
            target_returns: []
        -   source: get_unique_elements
            target: ''', ''.join'
            target_inputs:
            - cleaned_elements
        -   source: element_generator
            target: enumerate
            target_inputs:
            - input_str
        -   source: element_generator
            target: input_str[start:i].strip
            target_inputs:
            - '''\''" '''
        -   source: element_generator
            target: input_str[start:].strip
            target_inputs:
            - '''\''" '''
        -   source: get_python_datasets
            target: DatasetGenerator(file_path, file_details, base_name, questions, model_config, detailed).generate
            target_inputs: []
        -   source: get_python_datasets
            target: DatasetGenerator
            target_inputs:
            - file_path
            - file_details
            - base_name
            - questions
            - model_config
            - detailed
            target_returns: []
    control_flow_structure:
    -   ? 'def get_python_datasets(file_path: str, file_details: dict, base_name: str, questions: list[dict], model_config: dict, detailed: bool)'
        :   -   return:
                - DatasetGenerator(file_path, file_details, base_name, questions, model_config, detailed).generate()
    - import logging
    - import re
    - import math
    - import yaml
    -   'def get_unique_elements(input_str: str)':
        -   def element_generator(input_str):
            - start, brace_level = (0, 0)
            -   for (i, char) in enumerate(input_str):
                -   if char in '{}':
                    - brace_level += 1 if char == '{' else -1
                    elif char == ',' and brace_level == 0:
                    - yield input_str[start:i].strip('\'" ')
                    - start = i + 1
            - yield input_str[start:].strip('\'" ')
        - input_str = input_str.strip('[]\'"')
        - cleaned_elements = [element for element in element_generator(input_str) if element]
        -   return:
            - ''', ''.join(cleaned_elements)'
    -   class DatasetGenerator:
        -   ? 'def __init__(self, file_path: str, file_details: dict, base_name: str, questions: list[dict], model_config: dict, detailed: bool)'
            :   - self.file_path = file_path
                - self.file_details = file_details
                - self.base_name = base_name
                - self.questions = questions
                - self.model_config = model_config
                - self.llm = model_config['model'] if model_config else None
                - self.use_llm = bool(model_config)
                - self.detailed = detailed if self.use_llm else False
                - self.instruct_list = []
                - 'self.question_mapping = {''file'': ''file'', ''function'': ''functions'', ''class'': ''classes'', ''method'': ''classes''}'
                - self.code_qa_list = []
                - self.code_qa_response = ''
        -   def format_response(self):
            - self.code_qa_response = re.sub('\\n\\s*\\n', '\n', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False, indent=2)).replace("''", "'").strip('"').strip("'").strip()
            - self.file_details['file_info']['code_qa_response'] = self.code_qa_response
        -   'def get_response_from_llm(self, query: str, context: str)':
            - 'context_strategies = [lambda: str(context), lambda: f"```python\n{self.file_details[''file_info''][''file_code_simplified'']}\n```", lambda: f"```python\n{self.get_info_string(self.file_details[''file_info''], ''file_summary'')}\n```", lambda: '''']'
            - max_context_length = self.model_config['inference_model']['model_params']['context_length']
            - prompt_template = self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'], instruction_prompt=self.model_config['instruction_prompt'])
            -   for strategy in context_strategies:
                - context = strategy()
                - prompt = prompt_template.format(context=context, query=query, code_objects=self.code_qa_response)
                - context_size = len(self.llm.tokenize(prompt))
                - 'logging.info(f''***Context Size: {context_size}'')'
                -   if context_size <= 0.7 * max_context_length:
                    - break
            - context = self.code_qa_list if context == '' else context
            - response = ''
            -   try:
                - response = re.sub('\\n\\s*\\n', '\n\n', self.llm(prompt)).replace('<|im_end|>', '').replace('</|im_end|>', '')
                - response = '\n'.join((line.lstrip() for line in response.split('\n')))
                - 'logging.info(f''***Overall Response: {response}'')'
                except:
                -   'except Exception as :':
                    - 'logging.error(f''Failed to generate model response: {error}'')'
            -   if self.detailed:
                - self.get_detailed_response(context, response)
            - basename = str(self.base_name).replace('\\', '/')
            - 'self.code_qa_dict = {basename: self.code_qa_dict}'
            - 'self.code_qa_dict[basename] = {''Code Documentation'': [response], **self.code_qa_dict[basename]}'
            - self.format_response()
            - self.file_details['file_info']['purpose'] = response
            -   return:
                - response
        -   'def get_detailed_response(self, context: str, response: str)':
            -   for item in self.code_qa_list:
                -   try:
                    - instruct_key = list(item.keys())[0]
                    - instruct_value = list(item.values())[0]
                    - 'query = f''Describe the Purpose and Significance of these {instruct_key}: [{instruct_value}] and Explain what each of these {instruct_key} does in the code.'''
                    - prompt = self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'], instruction_prompt=self.model_config['instruction_prompt']).format(context=f'{context}/nCode Summary:/n{response}', query=f'{query}', code_objects=f'{instruct_value}')
                    - item_response = re.sub('\\n\\s*\\n', '\n\n', self.llm(prompt)).replace('<|im_end|>', '').replace('</|im_end|>', '')
                    - 'logging.info(f''\n***Itemized Response: {query}\n{item_response}'')'
                    -   for item in self.instruct_list:
                        -   if item['instruction'].startswith(instruct_key):
                            - output = f'\n\nPurpose and Significance:\n{item_response}'
                            - item['output'] += output
                    -   if '`' in instruct_key:
                        - dict_key1 = instruct_key.split('`')[1]
                        - dict_key2 = instruct_key.split()[0]
                        else:
                        - dict_key1 = instruct_key
                        - dict_key2 = ''
                    - 'purpose_dict = {''Purpose'': item_response.strip()}'
                    -   if dict_key1 not in self.code_qa_dict:
                        - self.code_qa_dict[dict_key1] = {}
                        elif not isinstance(self.code_qa_dict[dict_key1], dict):
                        - value = self.code_qa_dict[dict_key1]
                        - 'self.code_qa_dict[dict_key1] = {''Value'': value}'
                    -   if dict_key2:
                        -   if not isinstance(self.code_qa_dict[dict_key1], dict):
                            - self.code_qa_dict[dict_key1] = {}
                        -   if dict_key2 not in self.code_qa_dict[dict_key1] or not isinstance(self.code_qa_dict[dict_key1][dict_key2], dict):
                            - value = self.code_qa_dict[dict_key1].get(dict_key2, '')
                            - 'self.code_qa_dict[dict_key1][dict_key2] = {''Value'': value}'
                        - self.code_qa_dict[dict_key1][dict_key2].update(purpose_dict)
                        else:
                        - self.code_qa_dict[dict_key1].update(purpose_dict)
                    except:
                    -   'except Exception as :':
                        - 'logging.error(f''Failed to generate detailed response: {error}'')'
        -   def get_code_qa(self):
            - excluded = {'Call code graph', 'Docstring'}
            - self.code_qa_list = []
            - responses = {}
            -   for item in self.instruct_list:
                - instruction, output = (item['instruction'].split(' in Python file:')[0], item['output'])
                -   if not any((instruction.startswith(prefix) for prefix in excluded)):
                    - 'self.code_qa_list.append({instruction: output})'
                    -   if '`' in instruction:
                        - code_object, code_type = (instruction.split('`')[1], instruction.split()[0])
                        - responses.setdefault(code_object, []).append((code_type, output))
                        else:
                        - responses.setdefault(instruction, []).append((instruction, output))
            - self.code_qa_dict = {}
            -   for (code_object, type_responses) in responses.items():
                - self.code_qa_dict[code_object] = {}
                -   for (code_type, response) in type_responses:
                    -   if code_object == code_type:
                        - self.code_qa_dict[code_object] = response
                        else:
                        - self.code_qa_dict.setdefault(code_object, {})[code_type] = response
            -   if not self.use_llm:
                - basename = str(self.base_name).replace('\\', '/')
                - 'self.code_qa_dict = {basename: self.code_qa_dict}'
            - self.format_response()
        -   'def process_question(self, question_id: str, query: str, context: str, info: dict)':
            -   if question_id.endswith(('code_graph', 'docstring')):
                - response = info.get(question_id, {})
                elif question_id.endswith('file_purpose'):
                - self.get_code_qa()
                - response = self.get_response_from_llm(query, context) if self.use_llm else ''
                else:
                - response = get_unique_elements(str(info.get(question_id, '')))
            -   if response and response != 'None':
                - response = str(response).strip()
                - 'self.instruct_list.append({''instruction'': query, ''input'': context, ''output'': response})'
        -   'def get_info_string(info: dict, item_type: str)':
            -   return:
                - ''', ''.join([item.strip() for item in str(info.get(item_type, '''')).split('','') if item])'
        -   'def process_question_type(self, question_type: str, question_id: str, question_text: str)':
            -   if question_type == 'file':
                - query = question_text.format(filename=self.base_name)
                - info = self.file_details['file_info']
                - context = f"```python\n{self.file_details['file_info']['file_code']}\n```"
                - self.process_question(question_id, query, context, info)
                elif question_type == 'method':
                -   for (class_name, class_info) in self.file_details['classes'].items():
                    -   for (key, method_info) in class_info.items():
                        -   if key.startswith('class_method_'):
                            - method_name = f"{class_name}.{key[len('class_method_'):]}"
                            - context = f"```python\n{method_info['method_code']}\n```"
                            - 'mapping = {''class_name'': class_name, ''method_name'': method_name}'
                            - query = question_text.format(filename=self.base_name, **mapping)
                            - self.process_question(question_id, query, context, method_info)
                else:
                -   for (name, info) in self.file_details[self.question_mapping[question_type]].items():
                    - context = f"```python\n{info[f'{question_type}_code']}\n```"
                    - 'mapping = {f''{question_type}_name'': name}'
                    -   if question_id == f'{question_type}_purpose' and self.use_llm:
                        - variables = self.get_info_string(info, f'{question_type}_variables')
                        - inputs = self.get_info_string(info, f'{question_type}_inputs')
                        - combined = ', '.join(filter(None, [variables, inputs]))
                        - mapping[f'{question_type}_variables'] = get_unique_elements(combined)
                        -   if question_type == 'class':
                            - mapping[f'{question_type}_methods'] = self.get_info_string(info, f'{question_type}_methods')
                    - query = question_text.format(filename=self.base_name, **mapping)
                    - self.process_question(question_id, query, context, info)
        -   def generate(self):
            -   for question in self.questions:
                - self.process_question_type(question['type'], question['id'], question['text'])
            - 'self.instruct_list.sort(key=lambda x: len(x[''input'']), reverse=True)'
            -   return:
                - self.instruct_list
    plant_uml: "@startuml\n  : def get_python_datasets(file_path: str, file_details: dict, base_name: str, questions: list[dict], model_config: dict, detailed: bool);\n      : return;\n  : import logging;\n  : import re;\n  : import math;\n  : import yaml;\n  : def get_unique_elements(input_str: str);\n      : def element_generator(input_str);\n          : start, brace_level = (0, 0);\n          if ((i, char) in enumerate(input_str)) then (yes)\n              if (char in '{}') then (yes)\n                  : brace_level += 1 if char == '{' else -1;\n              endif\n          endif\n          : yield input_str[start:].strip('\\'\" ');\n      : input_str = input_str.strip('[]\\'\"');\n      : cleaned_elements = [element for element in element_generator(input_str) if element];\n      : return;\n  : class DatasetGenerator;\n      : def __init__(self, file_path: str, file_details: dict, base_name: str, questions: list[dict], model_config: dict, detailed: bool);\n          : self.file_path = file_path;\n          : self.file_details = file_details;\n          : self.base_name = base_name;\n          : self.questions = questions;\n          : self.model_config = model_config;\n          : self.llm = model_config['model'] if model_config else None;\n          : self.use_llm = bool(model_config);\n          : self.detailed = detailed if self.use_llm else False;\n          : self.instruct_list = [];\n          : self.question_mapping = {'file': 'file', 'function': 'functions', 'class': 'classes', 'method': 'classes'};\n          : self.code_qa_list = [];\n          : self.code_qa_response = '';\n      : def format_response(self);\n          : self.code_qa_response = re.sub('\\\\n\\\\s*\\\\n', '\\n', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False, indent=2)).replace(\"''\", \"'\").strip('\"').strip(\"'\").strip();\n          : self.file_details['file_info']['code_qa_response'] = self.code_qa_response;\n      : def get_response_from_llm(self, query: str, context: str);\n          : context_strategies = [lambda: str(context), lambda: f\"```python\\n{self.file_details['file_info']['file_code_simplified']}\\n```\", lambda: f\"```python\\n{self.get_info_string(self.file_details['file_info'], 'file_summary')}\\n```\", lambda: ''];\n          : max_context_length = self.model_config['inference_model']['model_params']['context_length'];\n          : prompt_template = self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'], instruction_prompt=self.model_config['instruction_prompt']);\n          if (strategy in context_strategies) then (yes)\n              : context = strategy();\n              : prompt = prompt_template.format(context=context, query=query, code_objects=self.code_qa_response);\n              : context_size = len(self.llm.tokenize(prompt));\n              : logging.info(f'***Context Size: {context_size}');\n              if (context_size <= 0.7 * max_context_length) then (yes)\n                  : break;\n              endif\n          endif\n          : context = self.code_qa_list if context == '' else context;\n          : response = '';\n          partition \"try\" {\n              : response = re.sub('\\\\n\\\\s*\\\\n', '\\n\\n', self.llm(prompt)).replace('<|im_end|>', '').replace('</|im_end|>', '');\n              : response = '\\n'.join((line.lstrip() for line in response.split('\\n')));\n              : logging.info(f'***Overall Response: {response}');\n          }\n          if (self.detailed) then (yes)\n              : self.get_detailed_response(context, response);\n          endif\n          : basename = str(self.base_name).replace('\\\\', '/');\n          : self.code_qa_dict = {basename: self.code_qa_dict};\n          : self.code_qa_dict[basename] = {'Code Documentation': [response], **self.code_qa_dict[basename]};\n          : self.format_response();\n          : self.file_details['file_info']['purpose'] = response;\n          : return;\n      : def get_detailed_response(self, context: str, response: str);\n          if (item in self.code_qa_list) then (yes)\n              partition \"try\" {\n                  : instruct_key = list(item.keys())[0];\n                  : instruct_value = list(item.values())[0];\n                  : query = f'Describe the Purpose and Significance of these {instruct_key}: [{instruct_value}] and Explain what each of these {instruct_key} does in the code.';\n                  : prompt = self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'], instruction_prompt=self.model_config['instruction_prompt']).format(context=f'{context}/nCode Summary:/n{response}', query=f'{query}', code_objects=f'{instruct_value}');\n                  : item_response = re.sub('\\\\n\\\\s*\\\\n', '\\n\\n', self.llm(prompt)).replace('<|im_end|>', '').replace('</|im_end|>', '');\n                  : logging.info(f'\\n***Itemized Response: {query}\\n{item_response}');\n                  if (item in self.instruct_list) then (yes)\n                      if (item['instruction'].startswith(instruct_key)) then (yes)\n                          : output = f'\\n\\nPurpose and Significance:\\n{item_response}';\n                          : item['output'] += output;\n                      endif\n                  endif\n                  if ('`' in instruct_key) then (yes)\n                      : dict_key1 = instruct_key.split('`')[1];\n                      : dict_key2 = instruct_key.split()[0];\n                  endif\n                  : purpose_dict = {'Purpose': item_response.strip()};\n                  if (dict_key1 not in self.code_qa_dict) then (yes)\n                      : self.code_qa_dict[dict_key1] = {};\n                  endif\n                  if (dict_key2) then (yes)\n                      if (not isinstance(self.code_qa_dict[dict_key1], dict)) then (yes)\n                          : self.code_qa_dict[dict_key1] = {};\n                      endif\n                      if (dict_key2 not in self.code_qa_dict[dict_key1] or not isinstance(self.code_qa_dict[dict_key1][dict_key2], dict)) then (yes)\n                          : value = self.code_qa_dict[dict_key1].get(dict_key2, '');\n                          : self.code_qa_dict[dict_key1][dict_key2] = {'Value': value};\n                      endif\n                      : self.code_qa_dict[dict_key1][dict_key2].update(purpose_dict);\n                  endif\n              }\n          endif\n      : def get_code_qa(self);\n          : excluded = {'Call code graph', 'Docstring'};\n          : self.code_qa_list = [];\n          : responses = {};\n          if (item in self.instruct_list) then (yes)\n              : instruction, output = (item['instruction'].split(' in Python file:')[0], item['output']);\n              if (not any((instruction.startswith(prefix) prefix in excluded))) then (yes)\n                  : self.code_qa_list.append({instruction: output});\n                  if ('`' in instruction) then (yes)\n                      : code_object, code_type = (instruction.split('`')[1], instruction.split()[0]);\n                      : responses.setdefault(code_object, []).append((code_type, output));\n                  endif\n              endif\n          endif\n          : self.code_qa_dict = {};\n          if ((code_object, type_responses) in responses.items()) then (yes)\n              : self.code_qa_dict[code_object] = {};\n              if ((code_type, response) in type_responses) then (yes)\n                  if (code_object == code_type) then (yes)\n                      : self.code_qa_dict[code_object] = response;\n                  endif\n              endif\n          endif\n          if (not self.use_llm) then (yes)\n              : basename = str(self.base_name).replace('\\\\', '/');\n              : self.code_qa_dict = {basename: self.code_qa_dict};\n          endif\n          : self.format_response();\n      : def process_question(self, question_id: str, query: str, context: str, info: dict);\n          if (question_id.endswith(('code_graph', 'docstring'))) then (yes)\n              : response = info.get(question_id, {});\n          endif\n          if (response and response != 'None') then (yes)\n              : response = str(response).strip();\n              : self.instruct_list.append({'instruction': query, 'input': context, 'output': response});\n          endif\n      : def get_info_string(info: dict, item_type: str);\n          : return;\n      : def process_question_type(self, question_type: str, question_id: str, question_text: str);\n          if (question_type == 'file') then (yes)\n              : query = question_text.format(filename=self.base_name);\n              : info = self.file_details['file_info'];\n              : context = f\"```python\\n{self.file_details['file_info']['file_code']}\\n```\";\n              : self.process_question(question_id, query, context, info);\n          endif\n      : def generate(self);\n          if (question in self.questions) then (yes)\n              : self.process_question_type(question['type'], question['id'], question['text']);\n          endif\n          : self.instruct_list.sort(key=lambda x: len(x['input']), reverse=True);\n          : return;\nend\n@enduml"
    code_qa_response: "py2dataset/get_python_datasets.py:\n  Code Documentation:\n  - 'I) Python file `py2dataset\\get_python_datasets.py` serves as a central component to extract information from a given Python file along with associated questions pertaining to it and generate structured outputs in JSON format containing question-answer pairs. This dataset generation involves multiple processes, namely file processing through Language Models for generating detailed explanations and an extensive series of methods for question classification, data organization, response generation, etc. Let's delve deeper into each aspect:\n    1. Dependencies: The script relies on four external libraries - `re`, `yaml`, `logging`, and `math`. These libraries enable regular expression handling (`re`), managing complex data structures like dictionaries (`yaml`), logging errors or debugging information (`logging`), and mathematical operations (`math`) respectively.\n    2. Functions: There are three user-defined functions in the script - `get_unique_elements`, `element_generator`, and `get_python_datasets`.\n    a. `get_unique_elements(input_str)` cleans an input string by removing duplicates and returns a string containing unique elements separated by commas. It uses `enumerate`, `input_str[start:i].strip()`, `input_str[start:].strip()`, `input_str.strip(\"[]'\\\"\")`, and `getelement generator`. Its application streamlines readability of complex data structures.\n    b. `element_generator(input_str)` generates elements from an input string while maintaining contextual integrity by tracking braces ('{', '}'). It uses `enumerate`, `input_str[start:i].strip()`, `input_str[start:].strip()`, `yield`, and `lambda`.\n    c. `get_python_datasets(file_path, file_details, base_name, questions, model_config, detailed)` instantiates a `DatasetGenerator` class using given inputs and invokes its `generate()` method to retrieve instruct_list and code_qa_dict as outputs - crucial data structures for JSON formatted question-answer pairs. It uses the `DatasetGenerator` class for extensive functionality described below.\n    3. Classes: There is one major user-defined class named `DatasetGenerator`. It encapsulates multiple methods and attributes to achieve its purpose.\n    a. `__init__(self, file_path, file_details, base_name, questions, model_config, detailed)` initializes the DatasetGenerator object by setting input arguments as respective instance variables while creating internal attributes for code_qa_response storage (to hold generated responses for code questions), detailed response flag based on LLM usage, and instruct_list to store question-answer pairs. It also defines question mapping between file details and question types ('file', 'function', 'class', 'method').\n    b. `format_response()` formats the code_qa_dict output by applying YAML formatting techniques for readability. This ensures that generated JSON data remains organized.\n    c. `get_response_from_llm(self, query, context)` queries language models using specified prompt template (derived from configuration). It attempts different context strategies and manages LLM response processing while handling errors and detailed response generation if necessary. This method relies heavily on external LLM services through the `llm` attribute.\n    d. `get_detailed_response(context, response)` generates detailed responses for code objects by querying LLM again with additional context and question-specific information. It updates relevant data structures accordingly.\n    e. `get_code_qa()` identifies code responses from instruct_list and updates internal attributes like code_qa_dict to store these responses in a structured manner.\n    f. `process_question(self, question_id, query, context, info)` processes questions related to different file entities ('file', 'function', 'class', or 'method') by invoking other methods and appending relevant outputs to instruct_list based on the given question IDs.\n    g. `get_info_string(info, item_type)` returns a string of comma-separated distinct values from given dictionary 'info' associated with the provided key (item_type).\n    h. `process_question_type(self, question_type, question_id, question_text)` dispatches further question processing as per query types such as handling function-related or class-related questions by invoking `process_question()`. It also collects additional information like variables and methods for 'class' type questions.\n    i. `generate(self)` processes all given questions and generates instruct_list with answers for JSON output. It calls multiple internal methods to accomplish this task.\n    III. Key Inputs include Python file path, file details (dictionary), base name of the Python file, list of questions, model configuration (dict), and detailed flag indicating LLM usage. Outputs are instruct_list containing question-answer pairs and code_qa_dict with code responses from instruct_list. Variables like llm, use_llm, detailed, instruct_list, question_mapping, code_qa_list, etc., assist internal data manipulation for better readability. Functions/methods invoke each other forming a complex but coordinated system for comprehensive dataset generation according to specified requirements in the codebase.\n    '\n  Dependencies:\n    Value: re, yaml, logging, math\n    Purpose: 'The given dependencies - 're', 'yaml', 'logging', and 'math' play crucial roles in ensuring smooth functioning of the Python script 'get_python_datasets.py'. Each library serves a unique purpose contributing to different aspects of the codebase:\n      1. 're': This is a standard Python module for regular expression processing. Regular expressions allow pattern matching over strings, making it easier to extract specific information or manipulate text data in complex ways. In this script, 're' helps with cleansing input strings and managing iterations within 'element_generator'.\n      2. 'yaml': It is a data serialization library which provides support for parsing and constructing YAML documents - a human-readable data format commonly used to store configuration details or data structures. In the given codebase, 'yaml' helps format output in JSON format while maintaining readability for question-answer pairs.\n      3. 'logging': This module enables logging debugging information during runtime. It allows developers to track errors and monitor program behavior efficiently. In this context, 'logging' is used for error handling in the script as well as logging information about LLM responses generation progress or issues.\n      4. 'math': Being a Python standard library, it offers various mathematical operations required in computational processes like finding maximum values, floor division etc. Though not directly evident in this specific code snippet, its inclusion may be for potential future use cases involving mathematical calculations related to context lengths or other measurements.\n      In summary, these dependencies - 're', 'yaml', 'logging', and 'math' contribute significantly to the functionality of 'get_python_datasets.py', ensuring efficient data handling, proper logging of debugging details and flexibility for varied data formatting while solving a particular task: generating Python dataset JSON structures using queries about source codes and AI language model assistance if enabled by detailed flag configuration in 'model_config'.'\n  Functions:\n    Value: get_unique_elements, element_generator, get_python_datasets\n    Purpose: 'The given instructions point towards understanding the role of three distinct functions within the Python script - 'get_unique_elements', 'element_generator', and 'get_python_datasets'. These functions play significant roles in facilitating data manipulation and extraction required for generating question-answer pairs from a Python file.\n      1. 'get_unique_elements(input_str)' aims to clean an input string by eliminating duplicate elements and return them as a string separated with commas. It primarily improves the presentation of complex information sets obtained through diverse inputs like Python output formats (dict, lists), reducing potential noise within response content while keeping semantically crucial pieces intact for analysis or representation. This functionality benefits both human readability and subsequent data processing steps in the script.\n      2. 'element_generator(input_str)' works by managing input strings with nested braces ({, }). It iterates through characters in an input string and identifies opening/closing brace levels to accurately yield elements between these boundaries while skipping comma-separated portions not encased within curly brackets. This method serves as a helper function for other processes needing such element extraction without disrupting the original context.\n      3. 'get_python_datasets(file_path, file_details, base_name, questions, model_config, detailed)' is a higher-level function that utilizes the DatasetGenerator class to extract information from Python files along with associated questions and generates structured JSON outputs containing question-answer pairs. It instantiates the DatasetGenerator object using provided inputs and invokes its 'generate()' method to retrieve instruct_list and code_qa_dict as outputs - crucial data structures for question-answer pairs formation. This function serves as a primary entry point to initiate dataset generation within the Python script while encapsulating other components in a user-friendly manner.\n      The combined utilities of these three functions support better dataset generation with refined processing techniques tailored for code-specific complex data extraction, resulting in a coherent analysis experience with readable question responses from a Python file.'\n  Classes:\n    Value: DatasetGenerator\n    Purpose: \"In the given context, we need to understand the \\\"DatasetGenerator\\\" class within the provided Python script thoroughly. The DatasetGenerator serves as a central component for generating JSON formatted question-answer pairs from a Python file along with associated queries. Its primary purpose is to extract information from the input Python file and organize it into structured outputs containing detailed explanations when using Language Models (LLM).\\n\\nTo generate responses explaining the \\\"Purpose and Significance\\\" of classes in DatasetGenerator, we would follow these steps:\\n1. Invoke get_python_datasets() function with appropriate inputs like Python file path, file details dictionary, base name of the Python file, list of questions containing the specific query, model configuration (including LLM settings), and detailed flag set to True for generating elaborate responses. This call will instantiate a DatasetGenerator object and execute its generate() method to obtain instruct_list and code_qa_dict outputs.\\n2. In the instruct_list obtained from step 1, look for instances where the instruction matches \\\"Class\\\" question type (since we are focusing on classes). For each such instance:\\n   a. Retrieve relevant context (Python code segment associated with those classes) using self.file_details[\\\"classes\\\"] dictionary from DatasetGenerator object. This may contain information related to Python files, methods, functions, and classes.\\n   b. Pass the query (\\\"Describe the Purpose and Significance of these Classes: [DatasetGenerator] and Explain what each of these Classes does in the code.\\\") along with extracted context into get_response_from_llm() method within DatasetGenerator instance. This will interact with LLM to generate detailed responses considering the provided query context.\\n   c. Capture both inputs \\\"class name\\\" (\\\"DatasetGenerator\\\" as derived from '{}').\\\" format operation within original user input - however only classes need mention since \\\"dataset_generator\\\" encapsulates all functionalities) and \\\"method name\\\" (which is not applicable here). The generated response will include explanations for the DatasetGenerator class's purpose, significance, and its various functionalities.\\n3. Assemble these detailed responses into a JSON format as per the code structure defined in the script using YAML formatting techniques within format_response() method of DatasetGenerator instance. This step ensures well-organized JSON data for further utilization.\\n4. The resulting code_qa_dict output will contain the required explanation of \\\"DatasetGenerator\\\" class with Purpose and Significance details along with other relevant information extracted from Python file processing.\"\n  get_unique_elements:\n    Inputs:\n      Value: input_str\n      Purpose: In the given context, we need to explain the purpose and significance of the \"input_str\" parameter within the get_unique_elements function along with its role in the overall code. The `get_unique_elements` function is responsible for cleaning an input string by removing duplicate elements and returning a string composed solely of these distinct parts, joined using commas. In terms of functioning inside this larger structure (codebase), 'input_str' refers to incoming complex text containing variables potentially with bracketing characters like curly braces ({}) which `get_unique_elements` handles efficiently by managing brace levels through the `element_generator` function. This cleaning process enhances readability when dealing with intricate data structures, making it easier for users to understand and analyze the underlying information.\n    Calls:\n      Value: enumerate, input_str[start:i].strip, input_str[start:].strip, input_str.strip, element_generator, .join\n      Purpose: 'In the given context, we need to elaborate on the functions used within 'get_unique_elements' along with their roles in that function. This method aims to clean an input string by removing duplicates and returning a string containing unique elements separated by commas. The mentioned calls contribute significantly to achieving this objective:\n        1. `enumerate`: This built-in Python function creates an enumerated sequence of pairs where each pair contains a counter (from start index 0) alongside corresponding items from the input iterable ('input_str'). It helps to process each element considering both position and actual string contents later used with indices like 'start', resulting in fine granular manipulations in handling text portions within strings.\n        2. `input_str[start:i].strip()`: This slices a portion of the input string ('input_str') starting from index 'start' till 'i'. The '.strip()' method removes leading and trailing whitespaces, making sure the extracted string fragment is clean before further processing.\n        3. `input_str[start:].strip()`: Similar to above but extracts the remaining part of the input string after the previous slice ('start') till the end while removing any leading or trailing whitespaces.\n        4. `input_str.strip(\"[]'\\\"\")`: This strips specific characters from the input string ('input_str'), namely '[' (left square bracket), ']' (right square bracket), single quote ('\\'') and double quotes (\"\") ensuring cleaner strings to process in the following stages of `get_unique_elements`.\n        5. `element_generator(input_str)`: A generator function producing unique elements by scanning through 'input_str', iteratively yielding them with contextual integrity as determined by the count and positions of '{', '}', ',' within it (i.e., balancing braces while handling commas). This method ensures no data loss due to syntactical complexity in strings.\n        6. `join`: The string method '.join()' concatenates an iterable of elements into a single string separated by the specified delimiter ('', comma in this case), resulting in a readable output list converted into a single string for final return from 'get_unique_elements'.\n        These calls together help clean input strings and extract unique elements to enhance readability while maintaining contextual integrity within complex data structures. They are utilized effectively in `get_unique_elements` method within their roles outlined above.'\n    Variables:\n      Value: cleaned_elements, start, input_str\n      Purpose: 'In the `get_unique_elements` function within the given Python script, three variables - cleaned_elements, start, and input_str play significant roles. Their purpose and significance can be explained as follows:\n        1. cleaned_elements: This variable represents a generated list obtained by cleaning an input string using the `get_element_generator()`. It returns unique elements separated by commas from the given input string after removing unnecessary characters like square brackets ('[]'), single or double quotes, and trailing spaces. The cleaned version of the string is returned as output when invoked within `get_unique_elements()`.\n        2. start: This variable acts as an index pointer used during the iteration over input_str in `element_generator()`, which is a generator function called within `get_unique_elements()`. It helps identify starting positions for generating elements from input_str while skipping braces ('{', '}') and commas followed by whitespaces when they indicate list or dictionary boundaries.\n        3. input_str: This variable refers to the original string passed as an argument into `get_unique_elements()`. It represents a complex string possibly containing lists, dictionaries, and nested blocks represented as \"{...}\", \"'content...'\", \"...\", and \"...'\". The function aims to extract unique elements from this input string while maintaining contextual integrity by tracking braces ('{', '}') using start and brace_level variables in `element_generator()`.\n        Each of these three variables assists the user-defined `get_unique_elements()` method with unique segmentations (separated tokens/contents), locators within an iterable process, and original input string handling respectively, contributing to its functionality of returning a cleaned list of unique elements from a given input string.'\n    Returns:\n      Value: .join(cleaned_elements)\n      Purpose: \"In the given context, we need to understand two main aspects - Purpose and Significance of 'Returns from get_unique_elements' along with explaining their functionality within the code. Let's break down each part:\\n\\n1. 'Returns from `get_unique_elements`': This refers to the output generated by the function `get_unique_elements`. It takes an input string, removes duplicate elements, and returns a string containing unique elements separated by commas. Its primary purpose is simplifying complex data structures for better readability in various situations across the codebase.\\n\\n2. Explanation of what each 'Returns from get_unique_elements' does in the code:\\n   a. Cleaning input string: `get_unique_elements` iterates through the input string using `element_generator`. It keeps track of braces ('{', '}') to maintain contextual integrity while yielding elements as per occurrence and skipping commas before brackets followed by whitespaces or ending symbols like ', ', \\\", \\\", \\\"'\\\", or '\\\"'. This cleaning ensures better data handling.\\n   b. Returning unique elements: After iterating through the string, `get_unique_elements` creates a list of cleaned elements using a generator expression and returns them as a comma-separated string using '.join(cleaned_elements)'. This simplifies complex data structures for easier comprehension in different parts of the codebase.\\n\\nIn summary, 'Returns from get_unique_elements' helps improve readability by extracting unique elements from input strings while maintaining contextual information and presenting them in a concise format within the Python script.\"\n  element_generator:\n    Inputs:\n      Value: input_str\n      Purpose: 'In the given context, we need to elaborate on the purpose and significance of \"input_str\" within the scope of the `element_generator` function. This function is part of the `get_unique_elements` utility function defined in the provided Python script. Its primary role is to parse a complex input string containing multiple elements separated by various delimiters while eliminating duplicate occurrences before returning them as unique values in a string format.\n        To describe its purpose, consider a situation where an input string holds several pieces of information surrounded by curly braces ('{}'). These may be keys or values in Python dictionaries, list items separated by commas, strings wrapped in single or double quotes, etc. `element_generator` breaks down this string into individual elements while maintaining contextual integrity using brace level tracking. This helps ensure that elements extracted from within nested structures are correctly identified and processed one after another.\n        The function essentially performs data cleansing by removing extra characters such as brackets (['[]']), quotes (\"\"), and newlines at the start or end before iterating through its generated element yield expression with a 'next' operation over `input_str`. These steps allow the original information to be efficiently condensed into distinct items represented by a comma-separated string list once processed entirely. Thus, it significantly improves data clarity for subsequent operations working with complex data structures encountered within programming contexts like Python code parsing or text analytics applications.'\n    Calls:\n      Value: enumerate, input_str[start:i].strip, input_str[start:].strip\n      Purpose: 'In `element_generator`, enumerate, input_str[start:i].strip, and input_str[start:].strip are used to generate unique elements from an input string while maintaining contextual integrity. They contribute to efficient data handling within the function by managing iteration through the input string and stripping leading/trailing whitespaces as required.\n        enumerate helps iterate over characters in the input string 'input_str' along with keeping track of index 'i'. When char becomes a brace ({ or }), the respective brace_level changes (if adding else increments otherwise decrements it). Thus, bracket nesting information is available via 'brace_level', making it easier to identify start and end points for elements.\n        input_str[start:i].strip() extracts substrings from 'input_str' starting at index 'start' till the current iteration position 'i'. This ensures removing leading whitespaces before each element while maintaining contextual integrity.\n        input_str[start:].strip() yields the remaining string after reaching the end of a nested block (indicated by a closing brace). It strips trailing whitespaces to ensure clean elements are returned.\n        Together, these three calls in `element_generator` function collectively help parse an input string into distinct unique elements with consistent context handling while keeping readability high by stripping extra whitespaces at relevant points during the process.'\n    Variables:\n      Value: start, input_str\n      Purpose: 'In the context given above related to function 'element_generator', we need to describe the purpose and significance of variables 'start' and 'input_str'. These two variables play crucial roles within the `element_generator` function.\n        The 'start' variable keeps track of index position where a new element in the input string begins before yielding it using the generator expression. It helps maintain contextual integrity while iterating through the input string by skipping characters until encountering a comma followed by whitespace or end of string ('\\n'). This ensures that each unique element is extracted without including nested structures like braces ('{}') within strings.\n        On the other hand, 'input_str' represents the entire user-provided string which goes through 'element_generator'. It contains all data necessary for extracting unique elements after filtering out unnecessary characters such as '[', ']', quotes ('\"'), single quotes ('\\''), brackets ('[]'), and newlines ('\\n'). This variable serves as the primary source of information for generating a string containing only unique elements separated by commas.\n        In summary, 'start' helps manage context while iterating through input_str to extract unique elements without disrupting nested structures, whereas 'input_str' holds the original user-supplied string from which these elements are extracted. Both variables contribute significantly to the functionality of `element_generator`.'\n  get_python_datasets:\n    Inputs:\n      Value: file_path, file_details, base_name, questions, model_config, detailed\n      Purpose: 'To comprehend the role of the mentioned inputs in the `get_python_datasets` function, we first break them down individually and highlight their usages throughout the associated script:\n        1. `file_path (str)`: Represents the path to the Python file from which information needs to be extracted for generating question-answer pairs. It serves as an entry point into the dataset generation process by allowing access to the codebase contents.\n        2. `file_details (dict)`: A dictionary containing comprehensive details about the Python file, including its filename, function definitions ('functions'), classes, and associated class methods ('classes'). These structured data act as the source of facts and answers to posed queries about a given file's specific entities (variables in classes or inputs/outputs for functions).\n        3. `base_name (str)`: Refers to the base name of the Python file without any extensions or path information. It is primarily used to organize generated JSON outputs by adding it as a prefix to code objects within the `code_qa_dict`. This helps maintain contextual relevance in the final dataset.\n        4. `questions (list[dict])`: Represents a list of question definitions structured as dictionaries, each containing attributes such as 'type', 'id', and 'text'. These questions are processed by the DatasetGenerator instance to generate relevant responses and form instruct_list containing question-answer pairs.\n        5. `model_config (dict)`: Configures the language model behavior in dataset generation. It contains essential parameters like prompt template, system prompt, instruction prompt, context length for LLM queries, and whether to use an external Language Model ('llm') or not. In simpler words, this object steers AI interactions throughout dataset preparation and defines detailed response modes as specified by `detailed` attribute's presence (based on LLM configuration).\n        6. `detailed (bool)`: Defines the extensiveness of LLM output, driving how profound answers shall be delivered regarding questions connected with classes and code structures (\"File\" excluded), ensuring well-explained documentation if `True`. This flag influences `get_response_from_llm()`, `get_detailed_response()`, and `process_question_type()` methods.\n        In summary, these inputs collectively enable `get_python_datasets` to extract information from a Python file, process associated questions, interact with Language Models (if configured), generate structured JSON data as instruct_list, code_qa_dict outputs and facilitate overall dataset generation adhering to given requirements. They create an efficient communication bridge between different stages of this script.'\n    Calls:\n      Value: DatasetGenerator(file_path, file_details, base_name, questions, model_config, detailed).generate, DatasetGenerator\n      Purpose: 'In the context given, we need to elaborate on the purpose and significance of two primary call instances within `get_python_datasets`, which involve the DatasetGenerator class - namely `DatasetGenerator(file_path, file_details, base_name, questions, model_config, detailed).generate` and its encompassing class itself, DatasetGenerator.\n        Firstly, let's consider the specific function call: `DatasetGenerator(file_path, file_details, base_name, questions, model_config, detailed).generate`. This invocation initiates an object from the DatasetGenerator class by passing relevant parameters and then executes its `generate()` method. The primary intention is to process the provided Python file along with associated questions and generate structured outputs in JSON format containing question-answer pairs. It handles various tasks such as parsing the input file, querying language models for detailed responses if necessary, organizing data into instruct_list (question-answer pairs), and creating code_qa_dict with code responses from instruct_list. This function call is crucial to achieve the main objective of `get_python_datasets`.\n        Secondly, we have DatasetGenerator as a class itself. It encapsulates multiple methods and attributes to accomplish its purpose - generating JSON formatted question-answer pairs using python file details combined with the set of queries asked against those files. Important instances from this class are:\n        - Initializer `__init__(...)`, initializing internal data members along with mapping questions to file details.\n        - Methods like `format_response()` and `get_code_qa()` that format output data structures for better readability and identify code responses respectively.\n        - Other methods such as `get_response_from_llm()`, `get_detailed_response()`, `process_question()`, `get_info_string()`, and `process_question_type()` that perform question processing, language model interaction, response generation, string manipulation for distinct values extraction, and handling different types of questions related to files, functions, classes, or methods.\n        - The `generate()` method, which is the core functionality of this class, processes all given questions and generates instruct_list containing question-answer pairs as JSON output along with code_qa_dict having code responses from instruct_list.\n        In summary, within `get_python_datasets`, the DatasetGenerator call instance triggers comprehensive dataset generation through its generate() method, while the DatasetGenerator class itself provides various functionalities to achieve this goal effectively by managing question processing and response generation in a structured manner. These calls are significant as they form the backbone of the entire Python script's functionality for generating JSON formatted datasets from given Python files with associated questions.'\n    Variables:\n      Value: detailed, file_details, questions, file_path, model_config, base_name\n      Purpose: 'In the context given for explaining the purpose and significance of variables within `get_python_datasets`, we need to elaborate on their roles individually while keeping the focus on this specific function.\n        1. `detailed`: This boolean flag determines whether detailed responses will be generated using a Language Model (LLM) in `DatasetGenerator`. When `True`, it triggers extra steps like generating elaborate explanations for code objects through LLM queries, which are not included when set to `False`. Its purpose is to provide an optional layer of insight beyond simple text-based answers.\n        2. `file_details`: This dictionary stores information about the Python file analyzed by `get_python_datasets`. It includes key-value pairs representing essential file properties like filename details ('base name'), file content segments ('file_info', 'classes', 'functions'), variable names ('variables'), input parameters ('inputs'), and method descriptions ('methods'). The `DatasetGenerator` instance utilizes this data structure extensively to generate relevant responses.\n        3. `questions`: This list contains user-defined questions related to the Python file processed by `get_python_datasets`. Each question is represented as a dictionary with 'type', 'id', and 'text' attributes representing the type of inquiry, identifier, and query itself respectively. The generated instruct_list consists of responses to these queries alongside the original questions for better understanding.\n        4. `file_path`: It specifies the path to the Python file from which data will be extracted by `get_python_datasets`. This string input directs the function towards analyzing relevant source code and correlating it with the given question list.\n        5. `model_config`: A dictionary containing model configuration settings required for LLM operations in `DatasetGenerator`. It encompasses prompts, system instructions, context lengths, etc., essential to interface with a language model like OpenAI's GPT or any other compatible API. This input enables customization of the LLM interaction according to user preferences.\n        6. `base_name`: The base name of the Python file under analysis by `get_python_datasets`. It simplifies referencing the file in output JSON structures and context strings throughout the codebase.\n        Each variable plays a significant role in accomplishing the primary task - generating structured question-answer pairs from a given Python file using `DatasetGenerator` functionality while considering user queries and model configuration settings. Their combined usage ensures comprehensive dataset generation tailored to specific requirements.'\n    Returns:\n      Value: DatasetGenerator(file_path, file_details, base_name, questions, model_config, detailed).generate()\n      Purpose: 'The given instruction asks for explaining the purpose and significance of the returns obtained from `get_python_datasets()` function focusing on its invocation with `DatasetGenerator(file_path, file_details, base_name, questions, model_config, detailed).generate()`. Alongside, it also requests an explanation about each of these returns' roles within the overall code context.\n        To elaborate on the primary `get_python_datasets` function, it acts as a wrapper around the `DatasetGenerator` class and its `generate()` method. Its purpose is to extract information from a Python file considering provided details, questions list, model configuration settings, and detailed response flag. It returns a tuple containing instruct_list and code_qa_dict after processing all given queries through DatasetGenerator's functionality.\n        Now let's break down the two significant outputs of `get_python_datasets()` function call - instruct_list and code_qa_dict:\n        1. instruct_list: This list holds question-answer pairs generated from various questions related to a Python file after processing them through the DatasetGenerator class. It's sorted by input string length in descending order to ensure efficient access while rendering structured outputs as JSON formatted dictionary later on.\n        2. code_qa_dict: It stores responses related to code objects (functions, classes, methods) extracted from Python files as a dictionary format. Each key represents a unique entity name, and its corresponding value contains detailed information about the code object's purpose and significance. This dictionary helps in generating comprehensive documentation for Python files by providing insights into various code elements.\n        In summary, `get_python_datasets()` is a high-level interface to facilitate structured data extraction from Python files alongside associated question answers, contributing immensely towards constructing well-organized JSON format datasets that enable easy comprehension of Python source codes with their corresponding explanations. Both instruct_list and code_qa_dict are critical components extracted by this function call, enriching the overall dataset generation process.'\n  DatasetGenerator:\n    Methods:\n      Value: __init__, format_response, get_response_from_llm, get_detailed_response, get_code_qa, process_question, get_info_string, process_question_type, generate\n      Purpose: 'In the DatasetGenerator class within the given Python script 'py2dataset\\get_python_datasets.py', these methods serve significant purposes contributing to generating structured JSON format question-answer pairs from a Python file. Let's elaborate on each method:\n        1. __init__(self, file_path, file_details, base_name, questions, model_config, detailed): This constructor initializes the DatasetGenerator object by assigning input arguments as respective instance variables. It also defines attributes for internal storage and question mapping between file details and question types ('file', 'function', 'class', 'method'). Additionally, it instantiates an LLM object if provided in model_config to generate detailed responses.\n        2. format_response(): This method formats the code_qa_dict output by applying YAML formatting techniques ensuring organized JSON data representation. It makes generated responses easy to read and comprehend when working with Python files.\n        3. get_response_from_llm(self, query, context): The method queries a language model using specified prompt templates derived from configuration. It attempts different context strategies to manage LLM response processing while handling errors if any. If detailed responses are enabled (based on model_config), it generates additional explanations for code objects within the context provided.\n        4. get_detailed_response(context, response): This method generates detailed responses for code objects by querying LLM again with extended context and question-specific information. It updates relevant data structures accordingly, ensuring richer documentation about code purposes and functionalities.\n        5. get_code_qa(): Identifies code responses from instruct_list and updates internal attributes like code_qa_dict to store these responses in a structured manner for easier retrieval later on. This helps separate the extracted information pertaining only to code fragments in Python files.\n        6. process_question(self, question_id, query, context, info): Processes questions related to different file entities ('file', 'function', 'class', or 'method') by invoking other methods based on the given question IDs. It appends relevant outputs to instruct_list as per requirements.\n        7. get_info_string(info, item_type): Returns a string of comma-separated distinct values from given dictionary 'info' associated with the provided key (item_type). This method helps extract specific information related to classes, methods, variables, etc., for better question processing.\n        8. process_question_type(self, question_type, question_id, question_text): Dispatches further question processing according to query types like handling function-related or class-related questions. It also collects additional information such as variables and methods for 'class' type questions when required.\n        9. generate(self): This final method processes all given questions, generates instruct_list with answers for JSON output by calling multiple internal methods synchronously. Instruct_list contains question-answer pairs forming the dataset from Python files based on specified requirements in the codebase.\n        Each of these methods works collaboratively to ensure comprehensive dataset generation through extensive data manipulation within the DatasetGenerator class. They provide an organized way to extract meaningful insights from Python files using language models when necessary, making it easier for developers to understand complex code structures and functionalities.'\n    Attributes:\n      Value: file_path, file_details, base_name, questions, model_config, llm, use_llm, detailed, instruct_list, question_mapping, code_qa_list, code_qa_response, code_qa_response, code_qa_dict, code_qa_list, code_qa_dict, code_qa_dict\n      Purpose: 'In the context of the 'DatasetGenerator' class in given Python code, various attributes serve distinct purposes that collectively enable generating JSON format question-answer pairs related to a Python file. Here are explanations for each mentioned attribute:\n        1. `file_path`: It represents the path to the input Python file where information extraction will occur.\n        2. `file_details`: A dictionary holding crucial details of the given Python file which facilitates easy processing according to various query types (like file-related details).\n        3. `base_name`: This string variable contains the base name of the input Python file without any extensions, mainly used for organization in generated JSON outputs.\n        4. `questions`: A list containing multiple dictionaries representing different questions related to the Python file that need answering during dataset generation.\n        5. `model_config`: A dictionary storing configuration details about the language model (LLM) usage like prompt template, system prompt, instruction prompt, etc., required for generating detailed responses if needed.\n        6. `llm`: An object instantiated from the provided 'model_config' dictionary if LLM configuration is present; otherwise, it remains None. This object helps interact with the language model for answering complex queries related to code explanations.\n        7. `use_llm`: A boolean flag indicating whether Language Model (LLM) usage is enabled or not based on 'model_config' presence in input arguments. It determines if detailed responses will be generated using LLM.\n        8. `detailed`: Another boolean flag that forces generating detailed responses even when LLM is disabled due to improper configuration, but limited to queries that don't need file interactions (i.e., mostly self-explanatory questions).\n        9. `instruct_list`: This list stores the processed question-answer pairs in JSON format as instruction objects with keys like 'instruction', 'input', and 'output'. It forms the central data structure to represent final generated responses after processing all queries.\n        10. `question_mapping`: A dictionary that maps different query types ('file', 'function', 'class', and 'method') to respective entities (e.g., file mapping to file details) within Python code, ensuring organized response extraction according to questions.\n        11. `code_qa_list`: Initially empty but grows while processing questions related to functions or methods in the codebase. It contains dictionaries with keys representing question types and corresponding outputs extracted from the code as values. Used later for updating `code_qa_dict`.\n        12. `code_qa_response` & `code_qa_response` (duplicate name): Initially empty strings but get formatted JSON responses for code questions during processing. The former is used internally while the latter becomes part of file details in JSON output.\n        13. `code_qa_dict`: An initially empty dictionary that stores structured data related to code objects' explanations. It grows while processing 'file', 'function', or 'class' queries with code responses. Eventually, it is converted into a JSON formatted string as part of the final output.\n        14. `code_qa_list`: A temporary list used during question processing to collect code response pairs before structuring them in `code_qa_dict`. It gets cleared after generating `code_qa_dict`.\n        These attributes work together within the 'DatasetGenerator' class methods to extract relevant information from a Python file and generate structured JSON outputs containing question-answer pairs related to various aspects of the codebase. Their coordinated usage ensures efficient handling of different query types and organization of generated responses in an easily readable format.'\n  DatasetGenerator.__init__:\n    Inputs:\n      Value: self, file_path, file_details, base_name, questions, model_config, detailed\n      Purpose: 'In the context of `DatasetGenerator.__init__`, the given inputs serve as essential arguments for initializing the class object and setting up its internal attributes. Let's elaborate on each input:\n        1. `self`: This refers to the instance of the `DatasetGenerator` class being created during object instantiation. It acts as a reference to access other methods within the class and manipulate its attributes.\n        2. `file_path`: Represents the path to the Python file for which dataset generation is required. This input helps extract file code content when preparing context information while addressing the provided queries during `generate()` process or by accessing through attribute self.file_path after object instantiation.\n        3. `file_details`: Contains vital data about the Python file such as functions, classes, methods, variables, inputs, etc., which are necessary to generate accurate responses for various question types. This dictionary is stored in self.file_details attribute during initialization and used extensively throughout class operations.\n        4. `base_name`: Represents the base name of the Python file being processed. It assists in structuring JSON output format by identifying specific file details when creating code_qa_dict using this variable as key and providing context while addressing questions related to that particular file instance through attribute self.base_name.\n        5. `questions`: Refers to a list of dictionaries containing various question IDs, types, texts, etc., which the DatasetGenerator class will process to generate question-answer pairs in instruct_list format. This input is utilized by invoking process_question_type() method during object initialization and stored as self.questions attribute.\n        6. `model_config`: A dictionary containing model configuration details like prompt template, system prompt, instruction prompt, tokenizer setup (like max_context_length and model params), whether LLM will be employed (by setting self.use_llm), detailed responses usage by tracking the detailed flag, etc. This input is used in multiple methods like get_response_from_llm(), format_response(), etc., after being stored as self.model_config attribute during initialization.\n        7. `detailed`: A boolean flag indicating whether to generate detailed responses using LLM or not. It helps decide the response generation strategy by setting self.detailed attribute during object creation and influences methods like get_response_from_llm() or get_detailed_response().'\n    Calls:\n      Value: bool\n      Purpose: 'In the context given, we need to describe the purpose and significance of calls happening within the \"__init__\" method of the DatasetGenerator class. This constructor initializes an instance of the DatasetGenerator class with various inputs such as file path, file details dictionary, base name of the Python file, a list containing multiple question structures as \"dict,\" along with the configuration data associated with LLM use, i.e., 'model_config,' and the optional parameter specifying detailed response generation flag called 'detailed.'\n        Let's break down each call within \"__init__\":\n        1. Setting instance attributes like file_path, file_details, base_name, questions, model_config, llm (Language Model object if present), use_llm (True if model_config exists), detailed (detailed responses flag depending on LLM usage), instruct_list (to store question-answer pairs), question_mapping (dictionary mapping question types to file details), code_qa_list (for holding code questions and their responses initially empty), and code_qa_response (empty string).\n        2. Defining format_response() - a method that formats the code_qa_dict output later in the class for better readability using YAML formatting techniques.\n        3. The constructor also sets up internal attributes related to generating JSON outputs from Python files: question mapping between file details and question types ('file', 'function', 'class', 'method'), get_response_from_llm() method invocation for querying language models (using provided prompt template), get_detailed_response() to generate detailed responses for code objects, get_code_qa() handling extraction of code responses and update associated structures. Lastly process_question() invokes to parse user queries relevant to diverse Python file aspects and finally instantiating `getpython_datasets`.\n        The called processes ensure thorough structuring, extracting important insights, parsing question formats (according to entity type like files or objects) into instruct_list and code_qa_dict for JSON formatted outputs. These organized data structures help generate comprehensive question-answer pairs related to Python files with the potential of detailed explanations if LLM is utilized.'\n    Variables:\n      Value: self, detailed, file_details, questions, file_path, model_config, base_name\n      Purpose: 'In the context of `DatasetGenerator.__init__`, the given variables play significant roles to initialize and manage the functionality of the class. Let's elaborate on each one:\n        1. self: This refers to the current instance of the DatasetGenerator class itself when methods are called within its scope. It provides access to all attributes and methods defined in the class.\n        2. detailed: A boolean flag denoting whether detailed responses generated by a Language Model should be included or not. If the model configuration is provided, detailed responses will be generated; otherwise, defaulting to false. This variable helps control the level of explanation within answers during response generation.\n        3. file_details: It's a dictionary containing all essential information related to the Python file under consideration such as details about functions, classes, methods, variables, inputs, etc. This data is crucial for processing questions related to the file and generating appropriate responses.\n        4. questions: A list of dictionaries representing various question IDs along with their respective types and texts. These questions will be processed by the DatasetGenerator instance to generate structured JSON outputs containing question-answer pairs.\n        5. file_path: The path to the Python file from which information needs to be extracted for generating dataset responses. This string is used during initialization to set up necessary attributes related to the file being analyzed.\n        6. model_config: A dictionary holding configuration details about the Language Model (LLM) integration if required. It includes parameters like prompt template, system prompt, instruction prompt, context length, etc., which influence how LLM responses are handled within `DatasetGenerator`. If this dictionary is empty or not provided, LLM usage will be disabled.\n        7. base_name: The base name of the Python file without any extension. This string simplifies referencing the file throughout processing steps and helps organize generated outputs in a structured manner.\n        These variables together form the foundation for DatasetGenerator to function effectively by managing file-specific information, controlling detailed responses, storing data about Python files and their respective attributes, defining question categories along with queries associated with them, as well as setting up parameters related to Language Model integration when needed. They facilitate a comprehensive approach towards dataset generation in response to given questions pertaining to the analyzed Python file.'\n  DatasetGenerator.format_response:\n    Inputs:\n      Value: self\n      Purpose: 'In the given context related to 'DatasetGenerator' class, the inputs referred to as '[self]' are primarily associated with its instance attributes and methods that contribute towards generating JSON format question-answer pairs from a Python file. The 'DatasetGenerator.format_response()' method plays a crucial role in formatting the code_qa_dict output into a readable YAML structure. Its purpose is to ensure organized presentation of generated responses for code questions within the dataset.\n        The format_response() function carries out the following tasks:\n        1. It invokes the dumper class from yaml library with specific settings (width=float(\"inf\"), sort_keys=False, default_flow_style=False, indent=2) to serialize Python objects into YAML strings while maintaining original data structure and readability.\n        2. Updates file_details['file_info']['code_qa_response'] with the serialized code_qa_dict for later retrieval or processing in other parts of the codebase. This helps in creating JSON format output combining file details with code explanations as required by user queries.\n        3. It ensures that string representations are cleaned up before final formatting to maintain consistency throughout the dataset generation process.\n        The inputs mentioned in the query are essential components within 'DatasetGenerator' class responsible for managing responses related to code questions:\n        A. 'self.code_qa_dict': A dictionary storing detailed information about code objects and their associated functions or classes along with relevant explanations obtained through LLM responses (if configured). This structure serves as the primary data source for JSON output generation.\n        B. 'self.file_details[\"file_info\"][\"purpose\"]': Contains an overall purpose description of the Python file derived from language model response if detailed explanation is enabled or direct string information otherwise. It represents a summary context that can be referenced by other parts of the codebase to understand the primary objective of the Python script being processed.\n        The method format_response() formats these inputs into a structured JSON output combining file details with generated responses for code questions, making it easier to consume and analyze dataset information.'\n    Calls:\n      Value: re.sub(\\'\\\\\\\\n\\\\\\\\s*\\\\\\\\n\\, \\'\\\\n\\, yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float(\\'inf\\'), sort_keys=False, default_flow_style=False, indent=2)).replace(\"\\'\\, \\'\").strip(\\'\"\\').strip(\"\\'\").strip, re.sub(\\'\\\\\\\\n\\\\\\\\s*\\\\\\\\n\\, \\'\\\\n\\, yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float(\\'inf\\'), sort_keys=False, default_flow_style=False, indent=2)).replace(\"\\'\\, \\'\").strip(\\'\"\\').strip, re.sub(\\'\\\\\\\\n\\\\\\\\s*\\\\\\\\n\\, \\'\\\\n\\, yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float(\\'inf\\'), sort_keys=False, default_flow_style=False, indent=2)).replace(\"\\'\\, \\'\").strip, re.sub('\\\\\\\\n\\\\\\\\s*\\\\\\\\n, \\\\n, yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False, indent=2)).replace, re.sub, yaml.dump, float\n      Purpose: 'In 'DatasetGenerator.format_response', the primary purpose is to format the response stored in self.code_qa_response (a string containing detailed responses for code questions and their respective objects) into a structured YAML document conforming to specific presentation guidelines - indented by two spaces, sorted keys alphabetically, no leading quotes for scalar values but preserving object identifiers' quotation marks while discarding unwanted newline characters or duplicate ones within content. It carries out the reformatting via YAML dumper which enables organizing responses more concisely before final storage in file_details[\"file_info\"][\"code_qa_response\"].\n        The mentioned regular expressions (re.sub instances) are used to clean up newlines and whitespaces from various string manipulations throughout the codebase for better readability when formatting output responses. They ensure a neat appearance by replacing multiple consecutive newlines with single ones or removing them entirely as per context requirements.\n        Now, let's break down each Call made in `DatasetGenerator.format_response`:\n        1. re.sub('\\\\n\\\\s*\\\\n\\, '\\\\n\\, yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float(\\'inf\\'), sort_keys=False, default_flow_style=False, indent=2)).replace(\"\\'\\, \\'\", \"\") - It combines replacing consecutive newlines ('\\\\n\\\\s*\\\\n') with a single newline ('\\\\n') while stripping off trailing and leading quotes within scalar values from YAML dumped self.code_qa_dict. This refined output becomes input for subsequent replace functions below.\n        2. re.sub('\\\\n\\\\s*\\\\n\\, '\\\\n\\, yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float(\\'inf\\'), sort_keys=False, default_flow_style=False, indent=2)).replace(\"\\'\\, \\'\", \"\") - Similar to the first one but applied after YAML dumping operation for self.code_qa_dict without modifying the dictionary itself.\n        3. re.sub('\\\\n\\\\s*\\\\n\\, '\\\\n\\, yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float(\\'inf\\'), sort_keys=False, default_flow_style=False, indent=2)).replace(\"\\'\\, \\'\", \"\") - Same functionality as previous two but called independently for better readability in code structure.\n        4. re.sub('\\\\n\\\\s*\\\\n, \\\\n, yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float(\\'inf\\'), sort_keys=False, default_flow_style=False, indent=2)).replace - It combines replacing consecutive newlines ('\\\\n\\\\s*\\\\n') with a single newline ('\\\\n') while preserving quotes within scalar values but not stripping them off like previous cases.\n        5. re.sub - Handles regular expression matching and replacement throughout the codebase as a general utility function for string manipulation tasks.\n        6. yaml.dump - YAML serialization tool that converts Python data structures into formatted strings adhering to YAML syntax rules, here used for creating structured responses in JSON format stored within self.code_qa_dict before cleaning by replace functions mentioned above.\n        7. float('inf') - This is a constant value representing positive infinity used as maximum width parameter in yaml dumping operation to avoid truncating long strings during serialization process.\n        8. float(\\'inf\\'), sort_keys=False, default_flow_style=False, indent=2 - Parameters provided for configuring YAML formatting options while dumping self.code_qa_dict into a string format suitable for further processing by replace functions in this method. Sort keys is set to False to maintain original dictionary order; default_flow_style is False to preserve scalar values without quotes; indent=2 sets indentation level as two spaces for better readability.\n        The Calls made in `DatasetGenerator.format_response` contribute towards organizing detailed responses into a neatly formatted JSON structure which becomes part of file_details[\"file_info\"][\"code_qa_response\"].'\n    Variables:\n      Value: self\n      Purpose: 'In `DatasetGenerator`, the `format_response()` method formats the response stored in `self.code_qa_dict`. It transforms the complex dictionary data structure into a clean and organized YAML representation with enhanced readability for easier human comprehension. The variable `self` primarily represents an instance of this class with its entire context loaded from constructor parameters including Python file attributes like `file_details`, `questions`, `model_config`, etc., along with internal data structures such as `instruct_list`, `question_mapping`, and `code_qa_dict`.\n        Within `format_response()`, it sets `self.format_response()` to convert the dictionary containing code questions and responses into a multi-level JSON formatted string (i.e., `self.code_qa_response`). Furthermore, it updates another instance attribute named `file_details[\"file_info\"][\"code_qa_response\"]` with this newly formatted response string. This ensures that when the entire dataset is generated and returned by `DatasetGenerator`, this code-related JSON output will be available alongside other relevant file details for downstream usage or analysis purposes.\n        The variables processed in `self` have specific roles across various class methods as listed below:\n        1. `file_details` contains comprehensive Python file metadata fetched from its original structure to serve question answering contextual information (e.g., function definitions, class details, etc.).\n        2. `questions` stores user-provided queries to be answered about the Python file.\n        3. `model_config` holds configuration settings for language model usage like prompt template, system prompt, instruction prompt, and inference model parameters.\n        4. `detailed` indicates whether detailed responses should be generated using LLM or not.\n        5. `instruct_list` accumulates question-answer pairs during processing.\n        6. `question_mapping` maps question types to file details for efficient query handling.\n        7. `code_qa_dict` stores code questions and responses as key-value pairs.\n        8. `code_qa_response` temporarily holds the formatted JSON string output of `self.code_qa_dict`.\n        9. `file_info` is a dictionary within `file_details` storing file summary, purpose, etc., which gets updated with generated responses.\n        Hence, `self` acts as a container for all relevant data required to execute the DatasetGenerator's functionality and maintain internal consistency while formatting code question responses in an organized manner.'\n  DatasetGenerator.get_response_from_llm:\n    Inputs:\n      Value: self, query, context\n      Purpose: 'In the `DatasetGenerator` class's method `get_response_from_llm`, three primary inputs are involved - 'self', 'query', and 'context'. These parameters play significant roles as follows:\n        1. `self` refers to the current instance of the DatasetGenerator object itself. It carries essential attributes like file path, file details, base name, questions list, model configuration, detailed response flag, instruct_list, question mapping, code_qa_list, etc., which are utilized during various method calls within `get_response_from_llm`.\n        2. `query` represents the question or instruction seeking an answer from either Language Model (LLM) or internal data structures like file details dictionary. This input serves as a query to generate responses relevant to user inquiries about Python files, functions, classes, methods, etc.\n        3. `context` provides contextual information for better understanding while interacting with the LLM. Depending on the model configuration settings, 'context' can include code summaries, file code snippets, or even detailed explanations of specific code objects (if required). Context helps improve the accuracy and relevance of responses generated by the language model.\n        These inputs work collaboratively to enable `get_response_from_llm` in retrieving suitable answers for questions asked related to Python files utilizing both internal information as well as Language Model response whenever needed for comprehensive documentation extraction and detailed explanation purposes within `DatasetGenerator`. The LLM plays an active role only if configured under `model_config` when running `DatasetGenerator`; otherwise, context mostly stems from code extracts obtained via Python files or related metadata (as observed during method processing steps like 'get_code_qa()'). However, as mentioned earlier, the detailed response generation occurs only with LLM assistance ('detailed' flag is True).'\n    Calls:\n      Value: str, self.get_info_string, self.model_config['prompt_template'].format, strategy, prompt_template.format, len, self.llm.tokenize, logging.info, logging.error, math.ceil, re.sub('\\\\\\\\n\\\\\\\\s*\\\\\\\\n, \\\\n\\\\n, self.llm(prompt)).replace('<|im_end|>, ).replace, re.sub('\\\\\\\\n\\\\\\\\s*\\\\\\\\n, \\\\n\\\\n, self.llm(prompt)).replace, re.sub, self.llm, \\\\n'.join, line.lstrip, response.split, self.get_detailed_response, str(self.base_name).replace, self.format_response\n      Purpose: 'In 'DatasetGenerator.get_response_from_llm', multiple function calls facilitate different processes for generating a response using language model (LLM) insights to answer given queries considering appropriate contexts. These steps break down as follows:\n        1. Important attributes and variables involved in the method are referred like str, self.get_info_string, self.model_config['prompt_template'], strategy, prompt_template.format, len, self.llm.tokenize, logging.info, logging.error, math.ceil, re.sub('\\\\\\n\\\\\\s*\\\\\\n', '\\n\\n', self.llm(prompt)).replace('', '').replace, re.sub('\\\\\\n\\\\\\s*\\\\\\n', '\\n\\n', self.llm(prompt)).replace (which correspond to internal methods and parameters utilized in LLM interactions), line.lstrip from standard library, response.split(), self.get_detailed_response function invocation, str(self.base_name).replace('\\\\', '/'), self.format_response method call.\n        2. The method starts by iterating through context strategies to find an optimal length suitable for the language model's context. This approach avoids errors while feeding lengthy strings that could negatively affect the generated response quality or fail due to exceeding LLM constraints.\n        3. Once a fitting context size is found, it constructs a prompt combining system prompt, instruction prompt from model configuration, query, and code objects if required. Prompt templating involves using self.model_config['prompt_template'] format placeholders. The resulting prompt becomes input for the language model represented as 'self.llm(prompt)' invocation, triggering the actual inference. Errors related to model responses are logged by logging.error function.\n        4. If detailed response generation is enabled (detailed flag), get_detailed_response() is called after receiving LLM output. This method further queries LLM with refined prompts based on specific code objects' details and appends purpose descriptions for each code entity in instruct_list or relevant data structures like self.code_qa_dict.\n        5. After processing language model responses, format_response() is invoked to organize the code_qa_dict output into a readable YAML format with proper indentation and formatting. This ensures neat JSON data presentation when generating question-answer pairs.\n        6. Overall, these calls work together to retrieve meaningful insights from LLM for answering queries within 'DatasetGenerator.get_response_from_llm', enhancing dataset quality through language model intelligence.'\n    Variables:\n      Value: self, query, basename, max_context_length, response, context, prompt_template, context_size, err_msg, prompt, context_strategies\n      Purpose: 'In the `DatasetGenerator.get_response_from_llm` function within the Python script 'py2dataset\\get_python_datasets.py', several variables play crucial roles to facilitate generating language model responses for queries related to a given Python file. Here's their purpose and significance:\n        1. self: This refers to the current instance of the DatasetGenerator class, which contains all necessary attributes and methods required to process questions and generate responses. It allows accessing class properties like file_path, file_details, base_name, questions, model_config, detailed, instruct_list, question_mapping, code_qa_list, etc., for contextual processing during response generation.\n        2. query: This variable holds the actual question being asked by the user that needs an answer from either the Python file or language model depending upon use_llm flag status. It is passed to prompt templates while interacting with LLM for generating responses.\n        3. basename: It represents the base name of the Python file under consideration, which helps in organizing generated JSON outputs accordingly. In case detailed responses are enabled using LLM, this variable assists in structuring code_qa_dict within the output JSON format.\n        4. max_context_length: This value is specified in model_config under 'inference_model' dictionary for defining language model context size limit while processing queries and responses. It helps maintain manageable lengths for optimal performance when working with LLMs by ensuring query context remains reasonable during response generation.\n        5. response: Initially empty, this variable stores the final response generated either from internal code details or LLM depending upon use_llm flag status. It gets updated after processing queries and contexts through various strategies in `get_response_from_llm`.\n        6. context: This holds different string representations of the Python file context required for generating detailed responses based on 'context_strategies'. Initially empty if LLM is not used; otherwise, it accumulates code summaries or file information as per query requirements. Context plays a significant role in providing necessary background to language models during response generation.\n        7. prompt_template: A configurable template defining how prompt text would look while interacting with LLMs using provided instructions, contextual data from Python files, and code objects (if any). It is defined within model_config['prompt_template'] and gets formatted according to query details in the function execution flow.\n        8. context_size: Calculated as a result of tokenizing prompt text containing LLM query instructions and context using llm.tokenize(), it keeps track of current context size limitations in bytes (in the scope of LLM processing). Context length constraints ensure model performance by maintaining appropriate sizes without overshooting threshold limits set in max_context_length.\n        9. err_msg: A temporary error message used when context size exceeds max_context_length limit during LLM query execution, indicating that users should increase py2dataset_model_config.yaml context_length to resolve the issue. It helps developers identify potential bottlenecks in generating responses with longer contexts.\n        10. prompt: A dynamically constructed string using prompt_template and relevant variables (context, query, code_objects) that forms actual input for LLM interaction during response generation. It integrates all necessary data points required to generate accurate answers.\n        11. context_strategies: This is a list of functions that returns different context representations based on various strategies like file summary, code simplified version or nothing in some cases. The context selection strategy aims at fitting context within max_context_length limit for better LLM performance while ensuring accurate response generation.\n        Each variable mentioned above contributes to efficient interaction with language models and contextual processing in `DatasetGenerator.get_response_from_llm`, enhancing Python dataset extraction capability in generating insightful answers according to given questions related to Python files, classes, functions or methods. Their coordinated usage ensures optimal response generation while maintaining code readability and performance standards.'\n    Returns:\n      Value: response\n      Purpose: In the given context, we need to elaborate on the purpose and significance of the 'Returns' element retrieved using 'DatasetGenerator.get_response_from_llm'. Additionally, we should explain its role within the codebase. The 'DatasetGenerator.get_response_from_llm' method is responsible for querying a language model (LLM) to generate responses based on provided context and questions. It is invoked when generating detailed explanations or answering complex queries related to Python files during dataset generation. This function plays a crucial part in incorporating natural language understanding capabilities into the script by leveraging external LLMs to enhance answer quality and information delivery for code analyses.\n  DatasetGenerator.get_detailed_response:\n    Inputs:\n      Value: self, context, response\n      Purpose: 'In `DatasetGenerator.get_detailed_response`, the primary inputs are 'self', 'context', and 'response'. These parameters play significant roles during the detailed response generation process as explained below:\n        1. 'self': This refers to the instance of the DatasetGenerator class itself. It holds all necessary attributes related to file details, question-answer pairs, language model configurations, etc., making it crucial for contextual understanding while generating detailed responses. Self helps access other associated elements needed to deliver more nuanced replies beyond surface interpretation given in normal 'get_response_from_llm()' interactions with LLM service.\n        2. 'context': The provided 'context' contains a prompt created during LLM querying. It serves as a background reference while obtaining elaborate explanations from the language model for various code objects and question types ('file', 'function', 'class', 'method'). This context includes summarized file information or simplified Python code snippets along with previous responses generated by LLM to ensure coherence in detailed replies.\n        3. 'response': The 'response' parameter represents the initial output obtained from language models when queried for specific questions about the Python file or related code objects using basic templates and strategies in `get_response_from_llm()`. Detailed responses are added upon further contextualization with additional information retrieved through subsequent LLM queries. This response acts as a base to expand upon, providing more comprehensive explanations about the code entities mentioned in questions.\n        In summary, 'self', 'context', and 'response' together enable `DatasetGenerator.get_detailed_response` to generate elaborate responses by leveraging the DatasetGenerator instance data and contextualizing initial LLM outputs with additional queries for deeper insights into Python files or code objects mentioned in user questions. This method aims to provide more meaningful explanations than simple model responses, thus enhancing overall understanding of the analyzed Python codebase.'\n    Calls:\n      Value: list, item.keys, item.values, self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'], instruction_prompt=self.model_config['instruction_prompt']).format, self.model_config['prompt_template'].format, re.sub('\\\\\\\\n\\\\\\\\s*\\\\\\\\n, \\\\n\\\\n, self.llm(prompt)).replace('<|im_end|>, ).replace, re.sub('\\\\\\\\n\\\\\\\\s*\\\\\\\\n, \\\\n\\\\n, self.llm(prompt)).replace, re.sub, self.llm, logging.info, item['instruction'].startswith, instruct_key.split, item_response.strip, isinstance, self.code_qa_dict[dict_key1].get, self.code_qa_dict[dict_key1][dict_key2].update, self.code_qa_dict[dict_key1].update, logging.error\n      Purpose: 'In the 'DatasetGenerator.get_detailed_response' method within the Python script, several calls are involved to retrieve elaborate explanations related to identified code objects contained within different function entities (files, functions, classes or methods) and construct a structured dataset as responses in instruct_list format with additional contextual information. These significant calls perform various tasks as follows:\n        1. `self.model_config['prompt_template'].format(system_prompt=self.model_context_length': Maximum allowed context length for LLM queries derived from model configuration. This template is used to create prompts for generating detailed responses using the language model.\n        2. `self.model_config['instruction_prompt']`: Instruction prompt string extracted from model configuration, which helps in structuring query format for LLM interactions. It's included within the overall prompt template formatting process.\n        3. `re.sub(r'\\n\\s*\\n', '\\n\\n', self.llm(prompt))` and `self.llm(prompt)` communicate with a language model through an instance (`self.llm`) based on user configurations provided in the `model_config`. This helps fetch relevant details by considering provided query along with a wider context collected at appropriate contextual strategies via other lists called \"context_strategies\" from template 'prompt'. If errors emerge or it can't extract satisfactory responses, logging messages are generated using `logging.error`.\n        4. `item['instruction'].startswith(prefix) for prefix in excluded` filters some instructions ('Call code graph' and 'Docstring') by ensuring they do not undergo the detailed explanation process during the analysis step for organizing relevant code snippets.\n        5. `instruct_key`, `code_object`, `code_type`, `instruction`, `context`, `response`, `query` are variables used to manage different aspects of question processing and response generation. They store specific parts of instructions or extracted information from the Python file.\n        6. `item_response.strip()` removes leading/trailing whitespaces in generated responses for cleaner outputs.\n        7. `logging.info(f\"***Overall Response: {response}\")` logs detailed response strings for debugging purposes during development and testing stages.\n        8. `self.code_qa_dict` represents dictionary structure storage initialized as code query/responses associations when necessary or available detailed LLM analysis outputs after removing non-desirable types. Here 'code object' acts as a key while values can be nested dictionaries with purpose explanation under specific keys like 'Purpose'.\n        9. `self.format_response()` organizes output from `code_qa_dict` using YAML formatting for improved readability of generated JSON responses.\n        10. Various list manipulations (`.append`, `setdefault`, `update`) help maintain instruct_list and code_qa_dict data structures efficiently and add corresponding extracted items if missing while populating information required. Inherent methodological calling leads to complex interweaving with appropriate response linkages inside various categories or related aspects.\n        11. `item['input']` carries context information, usually the file's simplified code snippet summarized before. This assists LLM in understanding overall Python environment where queried elements are defined.\n        The overall goal is to combine all these actions for producing an elaborative question-answer dataset by interacting with a language model when necessary and managing responses in organized JSON format. It also considers excluding some specific instructions, maintaining contextual integrity during response generation while structuring outputs effectively.'\n    Variables:\n      Value: self, query, instruct_key, item_response, response, dict_key1, purpose_dict, context, dict_key2, output, prompt, instruct_value, value\n      Purpose: 'In `DatasetGenerator.get_detailed_response`, several variables play crucial roles to generate detailed responses for code objects within the class. Here's their explanation:\n        1. self: Refers to the current instance of the DatasetGenerator class. It has access to all instance attributes and methods, enabling contextual manipulation throughout the function execution.\n        2. query: Represents the initial question asked by a user or generated during question processing. Used within the Language Model (LLM) query creation while answering more comprehensive requests with extended details about Python objects (if needed).\n        3. instruct_key: Denotes a specific code object or keyword identified in the `code_qa_list`. This helps pinpoint particular items to fetch additional information and create detailed explanations.\n        4. item_response: Stores LLM response generated for initial queries related to file purpose, function details, etc., which are later used to append Purpose sections within code object descriptions.\n        5. response: Represents an overall output (initial query-answer pair) created by the DatasetGenerator's get_response_from_llm method before proceeding with detailed explanations for individual objects.\n        6. dict_key1 and dict_key2: These variables are used to navigate nested dictionaries within code_qa_dict when handling multiple levels of indentation in Python files (e.g., class within a module). They facilitate adding Purpose sections at relevant positions without affecting other object details.\n        7. purpose_dict: Temporary dictionary holding newly generated Purpose content for specific code objects identified by instruct_key. It is appended to the original dict if applicable, expanding object explanations in JSON outputs.\n        8. context: Supplies initial query response with relevant summary or code fragment as needed by LLM for creating extended answers during detailed explanations generation. This depends on earlier `get_response_from_llm` method results and configuration strategies like using 'context' value, Python file contents (\"Code Summary:\" plus initial model-generated explanation), and abstract summary only if explicitly stated by prompt specs (\"\", in this case).\n        9. dict_key2: Assists in navigating nested dictionaries when multiple levels of indentation exist within code objects (e.g., method within a class). It helps update detailed explanations at the correct level within JSON outputs.\n        10. output: Refers to instruct_list elements where answers are appended with generated Purpose content by get_detailed_response execution for a question. It consists of instructions (user query), input context, and response pairs.\n        11. prompt: Constructed string used as an LLM query to generate detailed explanations about code objects' purpose and functionality. It combines context information with extended queries generated using instruct_key values and item_response contents.\n        12. instruct_value: Holds specific Python object details extracted from `code_qa_list` during question processing. This variable is used when instruct_key contains double quotes (e.g., \"function\", \"class\").\n        13. value: Temporary placeholder for code object values stored within nested dictionaries of code_qa_dict while navigating through multiple levels of indentation in Python files. It helps update detailed explanations at the appropriate level within JSON outputs.'\n  DatasetGenerator.get_code_qa:\n    Inputs:\n      Value: self\n      Purpose: 'In the context given, 'self' refers to the instance of the DatasetGenerator class during execution of its methods. When analyzing the purpose and significance of Inputs to `DatasetGenerator.get_code_qa`, we need to focus on how these inputs contribute to generating code responses within this method.\n        The `DatasetGenerator.get_code_qa()` function primarily deals with collecting code responses from instruct_list (a list containing question-answer pairs generated by various processes) and updating the code_qa_dict attribute of the DatasetGenerator object. This dictionary stores responses for specific Python objects mentioned in the user queries such as classes, methods, functions, or file elements separately for organized data accessibility. It performs these actions by looping through type-specific code blocks ('code_graph', 'class', 'function', and 'method') and creating appropriate contexts using extracted information from file details (file_details dictionary). Once all relevant questions are processed according to their types, the method forms a structured JSON output representing Code Quality Analysis dictionary (\"Code Documentation\") using parsed instruct_list. It updates the attribute self.code_qa_dict while taking advantage of inner classes' details whenever applicable as described below:\n        1. For code blocks marked as 'code_graph': The get_response_from_llm() method is called to retrieve LLM response for a given query using provided context. This typically includes file summaries or simplified Python code snippets from self.file_details['file_info']['file_code_simplified'].\n        2. For 'class' and 'function': Context is created by taking relevant code blocks ('class_code', 'method_code') extracted from file details dictionary. The process_question() method generates responses for these question types using the same context along with additional information like variables or methods if required.\n        3. For 'file': Context consists of complete Python file code ('file_code'). This invokes process_question() multiple times to handle different question types related to the entire file.\n        Thus, Inputs to `DatasetGenerator.get_code_qa` play a crucial role in organizing generated responses from various processes into a structured JSON format for better understanding and analysis of Python code elements. They help create an informative dataset that can be further utilized by developers or other applications.'\n    Calls:\n      Value: item['instruction'].split, any, instruction.startswith, self.code_qa_list.append, instruction.split, responses.setdefault(code_object, []).append, responses.setdefault, responses.setdefault(instruction, []).append, responses.items, self.code_qa_dict.setdefault, str(self.base_name).replace, str, self.format_response\n      Purpose: 'In the context of 'DatasetGenerator.get_code_qa', these listed call functions or statements contribute to generating code responses from instruct_list and updating code_qa_dict accordingly for organized JSON outputs. Let's break down their roles individually:\n        1. `item['instruction'].split` - Extracts the instruction part from a dictionary item in instruct_list during get_code_qa processing.\n        2. `any(instruction.startswith(prefix) for prefix in excluded)` - Checks if an instruction matches any exclusion (excluded query types) like 'Call code graph' or 'Docstring'. If not, it continues to process the current item.\n        3. `self.code_qa_list.append({instruction: output})` - Adds a new key-value pair to code_qa_list when processing valid instructions. Key is the instruction string and value is the corresponding output generated earlier in get_response_from_llm or extracted from info dictionary directly.\n        4. `responses.setdefault(code_object, [])` - If 'code_object' doesn't exist as a key in responses dict yet, create an empty list as its value; otherwise keep existing values intact.\n        5. `responses.setdefault(instruction, []).append((code_type, response))` - Similar to previous step but for instruction key instead of code_object. Appends a tuple containing code type and corresponding output if not present in responses dict yet or appends directly otherwise.\n        6. `responses.items()` - Returns an iterable of (key, value) pairs from the responses dictionary. Used to access code objects and types for further processing.\n        7. `self.code_qa_dict.setdefault(code_object, {})` - If 'code_object' doesn't exist as a key in self.code_qa_dict yet, create an empty dict as its value; otherwise keep existing content as is. It acts like an alternative initialization of types data when parsing multi-layer structure into a simple dict format.\n        8. `str(self.base_name).replace` - Replaces '\\\\' with '/' in base_name string to maintain consistent file path notation across JSON outputs.\n        9. `self.format_response()` - Formats the code_qa_dict output into a readable YAML structure using specified strategies and cleans strings for better representation in final JSON formatted results.\n        The 'Purpose and Significance' portion of your query discusses high-level concepts about the selected instruction roles while focusing on the 'Calls made in DatasetGenerator.get_code_qa'. In this function, these calls organize extracted code responses from instruct_list into structured JSON format represented by code_qa_dict. This structure makes it easier to retrieve specific code details like functions or classes with their respective explanations for further analysis or documentation purposes.'\n    Variables:\n      Value: self, responses, basename, excluded\n      Purpose: 'In `DatasetGenerator.get_code_qa`, four primary variables play crucial roles - 'self', 'responses', 'basename', and 'excluded'. An elaborate breakdown helps us comprehend their duties.\n        1. 'self': This refers to the instance of the DatasetGenerator class itself during object creation. It carries all attributes and methods required for dataset generation, including file path details, question lists, model configurations, etc. Within `get_code_qa`, self is used mainly to access essential attributes like code_qa_list, file_details, base_name, detailed responses flag ('detailed'), question mapping dictionary ('question_mapping'), and so on for necessary operations.\n        2. 'responses': Initially an empty list inside `get_code_qa`, it starts storing the extracted answers or generated outputs related to functions or classes as code responses are accumulated throughout this method's execution. These responses are later added to the 'code_qa_dict', forming a key-value pair where keys represent unique Python entities (functions, classes) and values hold their respective responses.\n        3. 'basename': It represents the base name of the Python file being processed without path identifiers (extensions included), mainly gathered at instance creation as per its presence in attributes section of `DatasetGenerator` class - self.base_name is substituted by this variable while constructing dictionary keys to keep filenames separate yet organized. If there are multiple entities like functions and classes in a single file, 'basename' helps maintain their individuality within the JSON output structure.\n        4. 'excluded': This list contains question types that should be skipped during processing ('Call code graph', 'Docstring'). When going through 'self.questions' iteratively in `get_code_qa`, any element with an endswith matching to this list will not undergo further scrutiny as they don't need detailed processing.\n        Each of these variables serves specific purposes within `DatasetGenerator.get_code_qa` method, contributing towards generating structured JSON outputs containing question-answer pairs for Python files along with code responses. Their combined functionality ensures comprehensive dataset generation while maintaining simplicity through encapsulated organization principles across complex queries regarding classes/functions contained within said file.'\n  DatasetGenerator.process_question:\n    Inputs:\n      Value: self, question_id, query, context, info\n      Purpose: 'In the `DatasetGenerator.process_question` method within the given Python script for generating question-answer pairs, four primary inputs are utilized: self, question_id, query, context, and info. Each of these parameters serves a distinct purpose in the process of forming structured outputs.\n        1. 'self': This refers to the instance of the DatasetGenerator class itself. It holds all relevant attributes such as file path details, questions list, model configuration, etc., which are essential for generating appropriate responses. By having access to self within `process_question`, it can interact with other methods and data structures inside the class efficiently.\n        2. 'question_id': This parameter represents a unique identifier associated with each question in the provided list of questions. It helps categorize queries according to their types ('file', 'function', 'class', or 'method') during processing, allowing for better organization of responses and handling specific cases accordingly.\n        3. 'query': It denotes the actual textual query that needs answering by the DatasetGenerator instance. This input contains user-posed questions formatted with placeholders like '{filename}' or '{class_name}', which are replaced with actual values during processing to generate relevant responses. The query acts as a bridge between user queries and the subsequent response generation process.\n        4. 'context': Contextual information related to the question is passed through this parameter. Depending upon the nature of questions ('file', 'function', 'class', or 'method'), `process_question` might invoke different methods that generate context strings. For instance, in file-related queries, a full Python file code snippet acts as context, whereas for method-related queries, it's the corresponding method code. Context plays an integral role in generating accurate responses from Language Models when LLM is enabled through 'use_llm'.\n        5. 'info': This parameter holds specific details related to each question type (file, function, class, or method). It contains relevant data required for response generation like file information dictionary ('file_info'), function code ('function_code'), class information dictionary ('classes'), etc. The `process_question` method uses this input to extract necessary data from the respective dictionaries and generate appropriate answers using built-in helper methods (like 'get_info_string') for various questions scenarios such as class or method names, inputs/variables information, function variables summary, method lists in a class definition, etc.'\n    Calls:\n      Value: question_id.endswith, info.get, self.get_code_qa, self.get_response_from_llm, get_unique_elements, str, str(response).strip, self.instruct_list.append\n      Purpose: 'In the context of 'DatasetGenerator.process_question', these listed function calls/methods play significant roles while processing questions related to a Python file's contents and generating structured outputs:\n        1. `question_id.endswith`: This built-in Python string method checks if a given question ID ends with specific suffixes ('file', 'function', 'class', or 'method'). It helps categorize the type of query to determine the subsequent actions during question processing.\n        2. `info.get`: It is an inbuilt dictionary method that returns the value associated with the specified key if present, otherwise returning a default value (in this case None). This is used to extract relevant information from file details or class/function data stored as dictionaries within 'DatasetGenerator'.\n        3. `self.get_code_qa`: This method retrieves code responses from instruct_list and updates internal attributes like code_qa_dict in a structured manner, essential for generating JSON formatted outputs. It helps separate code-related questions from others.\n        4. `self.get_response_from_llm`: Invoked when the question requires language model assistance ('use_llm' flag is True). This method queries an external LLM service using a customized prompt template with contextual information and previous responses (if any) to generate detailed explanations for complex questions.\n        5. `get_unique_elements`: Used primarily to clean question query outputs before storing in instruct_list. It filters duplicates, joining distinct elements separated by commas. This improves readability when dealing with nested structures in Python code explanations.\n        6. `str` and `str(response).strip`: Convert results of computations to strings, if required (to preserve the datatype consistency within the script). The `strip()` function is employed after string conversions to remove leading/trailing whitespaces for better presentation in instruct_list outputs.\n        7. `self.instruct_list.append`: Appends new tuple ('instruction', 'input', and 'output') as dict to a Python list known as self.instruct_list. This accumulates question-answer pairs during the processing phase, which will be later used for JSON formatted outputs. These records comprise a sequence of text queries paired with appropriate explanations after traversing DatasetGenerator class methods.'\n    Variables:\n      Value: self, query, question_id, info, response, context\n      Purpose: 'In the `DatasetGenerator.process_question` method within the given Python script for generating JSON format question-answer pairs, several variables play significant roles as stated below:\n        1. self: It represents the current instance of the DatasetGenerator class when interacting with its methods or attributes. This allows access to object properties like file path, file details, questions list, model configuration, etc., required for processing questions related to a Python file.\n        2. query: This variable stores the constructed question text that needs an answer based on provided input information. It is formed using templates with contextual elements like filename or mapping parameters (e.g., class_name, method_name). During question processing stages such as `process_question_type`, this variable carries prepared queries to be evaluated by language models or other data sources.\n        3. question_id: This variable captures the unique identifier associated with each question in the list of questions provided during instantiation. It helps in determining the type of query and related processing strategies (e.g., 'file', 'function', 'class', or 'method'). `process_question_type` method utilizes this identifier to categorize queries before calling other relevant methods.\n        4. info: The 'info' variable stands for relevant dictionary objects extracted from the Python file's details dictionary - which can contain properties related to 'file', 'functions', 'classes', or 'methods'. It provides contextual data required to generate appropriate responses for specific questions. For instance, it may hold code snippets or summarized information about classes and methods in a Python file.\n        5. response: This variable stores the generated answer or explanation derived from processing queries with corresponding contexts (either LLM output or unique elements extracted from 'info' dictionary). It is appended to instruct_list after being formatted as {\"instruction\": query, \"output\": response}, structuring questions along with their answers within a Python list comprehension manner in the same function `process_question`.\n        6. context: Lastly, this variable embodies various levels of data based on LLM responses management in language modeling situations ('Context'), snippets or simplified files related to query types ('file', 'function', 'class', or 'method'). It assists in providing the necessary context for generating accurate responses from Language Models (if used) and detailed explanations about code objects when required. Context manipulation is observed throughout methods like `get_response_from_llm`, `get_detailed_response`, and `process_question`.'\n  DatasetGenerator.get_info_string:\n    Inputs:\n      Value: info, item_type\n      Purpose: 'In the context given, we need to elaborate on the roles of 'info' and 'item_type' within the `get_info_string` function present inside the DatasetGenerator class in Python script - get_python_datasets.py. They contribute mainly while constructing comprehensive answers relating to variables/arguments listed as a portion of Python file details.\n        The `get_info_string` function is responsible for extracting unique elements from complex data structures within dictionaries and joining them into a string separated by commas. It helps improve readability when presenting information about specific aspects of the Python file being processed. Let's break down its usage with these inputs:\n        1. 'info': This parameter represents a dictionary containing details related to various entities such as functions, classes, methods, variables, etc., extracted from the Python file under analysis. It acts as a primary source of data for `get_info_string` function to retrieve information required to answer certain types of questions asked by users or generated programmatically through other methods in DatasetGenerator class.\n        2. 'item_type': This argument serves as a key in the dictionary 'info', referring to particular aspects or entities about which information is demanded within a question - be it classes' names ('classes'), function input parameters ('function_inputs') etc., providing contextual significance while filtering relevant data from 'info'.\n        Inside `get_info_string`, both inputs are utilized together to fetch distinct elements related to the specified 'item_type' key in 'info'. For instance, when generating answers about classes and their methods or function inputs, these variables assist in selecting pertinent data stored as lists or strings within dictionaries for crafting comprehensible responses. This process enhances overall dataset generation efficiency by maintaining clarity amidst complex Python file details.'\n    Calls:\n      Value: .join, item.strip, str(info.get(item_type, )).split, str, info.get\n      Purpose: 'In 'DatasetGenerator.get_info_string', several calls are utilized to extract unique elements from a dictionary related to specific keys ('item_type'). Here's an explanation for each of these operations:\n        1. '.join': This is an inbuilt Python string method that concatenates its arguments separated by a provided delimiter into one long string. It appears when formatting responses for easier readability after filtering unique elements from the input string.\n        2. item.strip(): This call removes leading and trailing whitespaces from each element extracted by 'element_generator'. As strings may contain unwanted spaces, this step ensures clean data before further processing.\n        3. str(info.get(item_type, )): 'info' likely contains complex data structures, thus conversion into string format ensures proper handling throughout the script as some methods might require a string argument instead of dictionary or list types. When attempting to fetch a specific key ('item_type') from this structure with '.get', this str() conversion prepares it for that operation if required. If 'item_type' doesn't exist in 'info', get will return default None, so we wrap the whole expression inside str() for safety and ensure type consistency across cases.\n        4. info.get(item_type, ).split(): Once converted to string format ('str(...)'), 'info[item_type]' is split by default separator (',') into a list of unique elements due to string splitting operation. This breaks down the string into individual components based on commas.\n        5. str(): After splitting the string, each element might still contain whitespaces from original data. To ensure clean output, strip() method removes these spaces before returning final results as a comma-separated string.\n        6. info.get(: Unveiled above while discussing str(info.get(item_type, )).split - 'get' is used to retrieve value from dictionary 'info', given the specified key ('item_type'). If 'item_type' doesn't exist in 'info', it returns a default value as mentioned during function definition.\n        Overall, these calls in `DatasetGenerator.get_info_string` collaborate to create a clean and structured string representation of specific data from the input dictionary ('info') related to 'item_type'. This helps generate concise outputs for question responses when required.'\n    Variables:\n      Value: item_type, info\n      Purpose: 'In the context of `DatasetGenerator.get_info_string`, the variables 'item_type' and 'info' play crucial roles during the function execution. These variables have distinct functionalities as described below:\n        1. `item_type`: This variable represents a string denoting a specific category or attribute from the Python file details dictionary being processed by `get_info_string()`. As we parse different data components, the variable dynamically stores different tags related to question categories like \"class_methods\" in relation with Class type questions and so on. Essentially, 'item_type' serves as a pointer towards particular elements of interest within file details that require extraction.\n        2. `info`: It stands for a dictionary containing comprehensive data regarding the Python file extracted using various processing techniques such as loading and parsing Python files to fetch details like class names with their methods, functions, variables, etc. This dictionary acts as a repository of information required to generate question responses related to different aspects of the Python file under consideration. Within `get_info_string()`, 'info' serves as an input parameter which allows retrieving specific elements associated with 'item_type'.\n        In summary, 'item_type' helps identify the category of data to be extracted while 'info' provides the actual data related to that category from Python file details during processing inside `get_info_string`. This pair contributes towards answering comprehensive queries on files' elements (like methods and variables) as directed by associated questions within `DatasetGenerator`.'\n    Returns:\n      Value: .join([item.strip() for item in str(info.get(item_type, )).split(, ) if item])\n      Purpose: 'In the provided context, the goal is to interpret and break down how '.join([item.strip() for item in str(info.get(item_type, ).split(\", \") if item])' operates within 'DatasetGenerator.get_info_string'. This function extracts unique elements from an input string by removing duplicates and returns them as a comma-separated list. It primarily focuses on simplifying complex data structures for better readability in generated outputs.\n        Let's unpack its essential parts:\n        1. '.join([item.strip() for item in str(info.get(item_type, )).split(\", \") if item]': This line iterates over each element in the string obtained by splitting 'str(info.get(item_type, ))' using \",\" as a delimiter after stripping whitespaces from both ends of each item ('item.strip()'). The list comprehension format assures collection only for elements containing contents, avoiding any null spaces and unwanted segments due to '\"\",\"', removing multiple entries if present.\n        2. 'info.get(item_type, )': This expression retrieves the dictionary value associated with 'item_type' key from the provided 'info' dictionary or returns an empty string if none exists ('None'). It is a safeguard measure for missing keys in the dictionary.\n        3.'str(info.get(item_type, ))': Type conversion into strings facilitates slicing by delimiter processing through splitting on \", \". In the original context, this step ensures handling of dictionary values as strings before further manipulation.\n        4.'split(\", \")': This method splits the string into a list using \",\" as a separator while preserving any leading/trailing spaces (\") between words within quotes, hence enabling subsequent 'item' iterations over nonempty contents after trimming processes in ['strip()'] stage above.\n        Therefore,'DatasetGenerator.get_info_string'(which houses this joined segment) effectively parses through an input dictionary ('info') to generate a string containing distinct elements separated by commas, enhancing readability of complex data structures within generated JSON outputs.'\n  DatasetGenerator.process_question_type:\n    Inputs:\n      Value: self, question_type, question_id, question_text\n      Purpose: 'In the context of `DatasetGenerator.process_question_type`, the given inputs - 'self', 'question_type', 'question_id', and 'question_text' play significant roles during its execution. These parameters are passed when invoking this method to process different types of questions related to a Python file such as file, function, class, or method queries. Here's an explanation for each input:\n        1. `self` refers to the instance of the `DatasetGenerator` class itself. It carries all relevant attributes and methods required to handle question processing efficiently. This 'self' parameter allows accessing essential data structures like file details, questions list, model configuration, etc., stored during initialization.\n        2. `question_type` denotes the category of the query being processed within `DatasetGenerator`. It can take values like \"file\", \"function\", or \"class\". Based on this type, specific parts of the Python file are targeted to extract relevant information and generate answers for related questions. This helps customize response generation according to question context.\n        3. `question_id` represents a unique identifier associated with each question in the given list of questions passed as input during instantiation. This identifier allows efficient management of various types of questions (file, function, class, method) by organizing them in respective sections of instruct_list or updating data structures like file_info dictionary for further processing.\n        4. `question_text` represents the actual textual representation of a query awaiting an answer. It contains placeholders such as \"{filename}\" and \"{class_name}\", \"{method_name}\", etc., which are replaced with actual values during execution to generate contextually relevant responses using Python file details or code segments corresponding to each question type. These templates ensure dynamism in processing questions by allowing adaptability according to query content.\n        With these inputs combined, `DatasetGenerator.process_question_type` performs extensive operations like parsing queries against file details, generating appropriate contexts for LLM responses (if required), updating data structures with answers, and sorting instruct_list based on input length before returning it along with code_qa_dict as outputs from get_python_datasets function invocation. Their collaboration enhances structured question-answer generation according to specific query patterns pertaining to Python files' diverse components like file contents or function definitions.'\n    Calls:\n      Value: question_text.format, self.process_question, self.file_details['classes'].items, class_info.items, key.startswith, len, self.file_details[self.question_mapping[question_type]].items, self.get_info_string, .join, filter, get_unique_elements\n      Purpose: 'In the `DatasetGenerator.process_question_type`, several functions and operations are utilized to process questions related to file, function, class, or method categories. Below is an explanation of each mentioned call within this context:\n        1. `question_text.format`: This call formats question text by inserting dynamic values using string formatting techniques from Python's built-in format() method. It replaces placeholder tags (like {filename} and {question_type}_name) with actual values retrieved from given parameters, resulting in a personalized question based on file details or specific entity names (class/function).\n        2. `self.process_question`: This method is invoked to process the current question after analyzing its type. It handles generating responses for various question categories and adds them to the instruct_list data structure containing question-answer pairs.\n        3. `self.file_details['classes'].items()`: Returns an iterable of (key, value) pairs for all classes present in file details dictionary. This is used when processing questions related to 'class' or 'method'. Keys represent class names while values store information about those classes including their methods and other relevant attributes.\n        4. `class_info.items()`: Similar to the previous call but limited within a specific class definition (from file details dictionary). It returns an iterable of key-value pairs for the given class name. Keys may represent method names while values store code snippets related to those methods.\n        5. `key.startswith`: This string operation checks if the key obtained from 'class_info.items()' starts with a particular prefix (like 'class_method_'). It helps filter out irrelevant keys while handling questions specific to class methods in Python files.\n        6. `len`: Len(x) returns the length of an object x, which is used here to sort instruct_list based on input string lengths in reverse order during response generation. This ensures that longer inputs are processed first for better context understanding by LLM or other processing methods.\n        7. `self.file_details[self.question_mapping[question_type]]`.items()`: Uses Python's dictionary item method '[]' for mapping questions ('file', 'function', 'class', and 'method') with associated file information depending upon the question type. For instance, if it's a 'class' related question, this call returns an iterable of key-value pairs for classes present in file details dictionary. Keys would represent class names while values hold file detail objects relevant to each class (variables and input lists).\n        8. `self.get_info_string`: Extracts distinct values from given dictionaries like file information or entity details related to class variables/inputs/methods by filtering out empty strings using list comprehension and get_unique_elements(). It returns a string with comma-separated unique elements for better readability.\n        9. `.join`: A built-in string method used to concatenate strings with specified separator ('', ', '). This operation combines extracted variables, inputs (if any), or method names depending on question context while building required mapping variables.\n        10. `filter(None, [values])`: Used with '.join' to remove empty strings from the list of values before concatenation. It ensures only non-empty elements contribute to final string formation.\n        11. `get_unique_elements`: Cleans an input string by removing duplicates and returns a string containing unique elements separated by commas. This function is employed in self.get_info_string() for readability improvement.\n        These calls together facilitate processing questions related to various entities within Python files like classes or methods effectively by retrieving essential details required to answer these questions comprehensively through the DatasetGenerator's `process_question_type`. The whole sequence aims at enhancing question understanding and generating structured outputs in JSON format containing question-answer pairs.'\n    Variables:\n      Value: self, query, question_id, question_text, variables, info, method_name, context, mapping, question_type, inputs, combined\n      Purpose: 'In the `DatasetGenerator.process_question_type` method within generating JSON format question-answer pairs, several variables play crucial roles as follows:\n        1. self: This refers to the current instance of the DatasetGenerator class and holds all relevant data and methods for processing questions related to a Python file. It allows accessing necessary attributes like file details, base name, questions list, model configuration, etc., during question handling.\n        2. query: Represents the formulated query derived from question text while considering type (file, function, class, or method) and relevant context for LLM response generation. This variable assists in structuring input for language models to retrieve answers.\n        3. question_id: Identifies each individual question uniquely with a specific ID assigned by the caller code. It helps DatasetGenerator differentiate questions based on their type (file, function, class, or method) and process them accordingly.\n        4. question_text: Represents the actual text of the question asked by the user. This variable helps retrieve essential data points while determining relevant entities within a Python file depending on question categories (like 'file', 'function', 'class', 'method') using other internal methods such as `process_question_type`.\n        5. variables: This variable is specific to class-related questions when generating detailed responses. It contains unique elements extracted from the information dictionary related to class variables using `get_unique_elements` function. These elements provide a summary of essential variables within a particular class, increasing context awareness in explanations provided by LLMs or detailed response generation processes.\n        6. info: Stands for data dictionaries related to different file details ('file_info', 'classes', or file specific methods) depending upon the question type processed. This variable facilitates fetching necessary information required for generating appropriate responses.\n        7. method_name: Appears when processing class-related questions where it stores the name of a particular method within a class. It helps generate context for detailed response generation related to that specific method in Python code.\n        8. context: Represents the code snippet relevant to the question being processed. This could be file code, function code, class_method code (method specific code) extracted from associated file details as part of query processing during instruction building before calling `process_question`. Context ensures model comprehension regarding actual code snippets.\n        9. mapping: In specific scenarios such as file, function or method related questions, mapping serves to hold attribute name values obtained dynamically with a certain naming pattern (\"file_name\", \"{questionType}_name\", or \"{questionType}_method\") from processed query text for contextual response generation. This dictionary simplifies incorporating relevant entity names into question responses.\n        10. question_type: Signifies the type of question being processed ('file', 'function', 'class', or 'method'). It helps in determining which data structures to access within file details (e.g., 'classes' for class-related questions) and applying appropriate processing techniques through `process_question_type`.\n        11. inputs: Similar to variables, this variable is also class-specific but related to function arguments or inputs of a method. It holds unique elements extracted from the information dictionary concerning input parameters for the respective class methods for enhanced explanations when required by LLM responses.\n        12. combined: Merges 'variables' and 'inputs' strings if both are present (for class-related questions). This variable creates a single string containing essential details about class variables and inputs for better context understanding during detailed response generation.\n        Each of these variables plays an important role in breaking down complex operations within `DatasetGenerator`, making it efficient at handling diverse Python file queries and generating comprehensive JSON formatted outputs as instruct_list and code_qa_dict with related responses.'\n  DatasetGenerator.generate:\n    Inputs:\n      Value: self\n      Purpose: 'In the context given, 'self' refers to an instance of the DatasetGenerator class while discussing its generate() method invocation. This function is a crucial part of the DatasetGenerator class that generates responses for all the provided questions related to a Python file and returns instruct_list containing question-answer pairs along with code_qa_dict consisting of code responses from instruct_list.\n        The 'Inputs to `DatasetGenerator.generate`' mentioned are primarily input arguments passed during instantiation of this class object which include Python file path, file details (a dictionary), base name of the Python file, questions list as queries to be answered for that specific file, a dictionary containing model configuration details including language model setup, and a boolean flag determining if detailed responses will be generated using the Language Model. Each of these inputs serves an essential purpose:\n        1. Python file path points to the actual source code file whose information needs to be extracted.\n        2. File details contain comprehensive data about the Python file such as functions, classes, methods, variables, etc., which are required for answering questions related to it.\n        3. Base name provides the identifier of the Python file, helpful when organizing generated JSON outputs into respective folders or files later.\n        4. Questions list encompasses a set of inquiries to be answered based on Python file data, thereby guiding DatasetGenerator to produce structured information about relevant code segments and features.\n        5. Model configuration provides essential settings for language model integration if required by the user. It includes details like prompt template format, system prompt, instruction prompt, context length for LLM inference, etc., that influence how LLM interacts with generated queries during response generation.\n        6. Detailed flag decides whether elaborate explanations should be retrieved from the Language Model to provide comprehensive insights into code objects like functions or classes when needed.\n        As DatasetGenerator processes these inputs, it invokes several methods internally (such as get_response_from_llm(), process_question(), generate()) and updates instance variables such as instruct_list and code_qa_dict with collected data in a structured manner. Finally, the generate() method returns this organized information as outputs for further utilization or storage purposes.'\n    Calls:\n      Value: self.process_question_type, self.instruct_list.sort, len\n      Purpose: 'In the `DatasetGenerator.generate()` function within `get_python_datasets()`, the listed object references pertain to fundamental aspects crucial for efficient information processing and output generation. Let's elaborate on each:\n        1. self.process_question_type: This method is responsible for categorizing questions related to different entities present in a Python file like file details, functions, classes, or methods based on their respective types (e.g., 'file', 'function', 'class', or 'method'). It invokes process_question() accordingly after extracting necessary context and query format strings using mapping variables. This step ensures accurate question processing for generating appropriate responses in instruct_list.\n        2. self.instruct_list.sort: After all questions are processed through various methods, this line sorts instruct_list by the length of their input parameter in reverse order (\"len\"). By doing so, similar entities like class details are kept adjacent within instruct_list for better readability and organization when generating JSON outputs.\n        3. len: The built-in Python function 'len()' is used to determine the size of an object or iterable variable like strings or lists. In this context, it helps sort instruct_list by input length as mentioned earlier. This sorting approach groups together entities with similar contexts in JSON outputs for improved comprehension.\n        To summarize, these three calls play a vital role in structuring the generated dataset within `DatasetGenerator`. self.process_question_type manages question categorization according to Python file components; self.instruct_list.sort orders question-answer pairs for clear visual representation while maintaining contextual consistency; lastly, len optimizes this organization process based on input sizes to create coherent JSON formatted question-answer pairs from complex Python data structures.'\n    Variables:\n      Value: self\n      Purpose: 'In `DatasetGenerator.generate()`, 'self' refers to an instance of the DatasetGenerator class. Here, 'self' is used as a convention in object-oriented programming languages like Python to represent the current instance of a class when methods are called within it. In this context, describing the purpose and significance of variables related to `DatasetGenerator.generate()`, we have several key attributes and methods that contribute to its functionality:\n        1. 'file_path': Path to the input Python file used for dataset generation. This attribute helps identify the source code where questions will be answered or explanations extracted from.\n        2. 'file_details': A dictionary containing detailed information about the Python file, such as class names with their respective methods, function listings, etc. It serves as a knowledge base to generate accurate responses for user queries related to the file's contents.\n        3. 'base_name': Base name of the Python file without any path or extension details. This attribute simplifies file referencing within generated outputs.\n        4. 'questions': List of questions that need answers or explanations from the Python file. These questions are processed one by one during dataset generation.\n        5. 'model_config': A dictionary carrying the configuration parameters required to run an external Language Model for more complex and elaborate responses in cases when `use_llm` flag is true (set in __init__ method). It defines \"prompt_template\", \"system_prompt\", \"instruction_prompt\", and \"inference_model\" attributes.\n        6. 'detailed': A boolean flag indicating whether detailed responses should be generated using the Language Model or not. If True, `get_response_from_llm` method is utilized for enhanced explanations related to code objects like classes or methods in the Python file.\n        7. 'instruct_list': This list accumulates question-answer pairs as dataset generation progresses. Each item contains instruction (question text), input context for LLM if needed, and output response generated either by processing built-in Python functions or Language Model predictions.\n        8. 'question_mapping': A dictionary mapping question types ('file', 'function', 'class', 'method') to the related entities within the Python file to help streamline the classification of queries while handling them efficiently during data extraction or responses formatting steps (most notably `process_question()`).\n        9.'code_qa_list': Items within this list denote extracted code snippets that need additional explanation through Language Model queries if `use_llm` is true. These items have question labels and corresponding responses extracted from instruct_list. This list helps create the final JSON formatted 'code_qa_dict'.\n        10.'code_qa_response': An intermediate string variable holding JSON representation of code questions and responses generated during `generate()`. It gets formatted using YAML dumping techniques in `format_response()` method for improved readability.\n        11.'file_details[\"file_info\"][\"code_qa_response\"]': This attribute stores the final JSON string containing code documentation after processing all questions related to Python file code objects.\n        12.'code_qa_dict': A dictionary that holds detailed information about different Python code elements with respect to the input base name if 'detailed' is set as True (either extracted using Language Model predictions or processed within the script). This attribute gets updated during `get_code_qa()` method execution.\n        In summary, these variables contribute to a well-structured dataset generation process where questions are processed according to their types and relevant Python file details, responses are generated either by built-in Python functions or Language Model predictions (if configured), and the final output is formatted as JSON containing question-answer pairs along with code explanations. This comprehensive approach ensures accurate and organized results for users seeking insights into a given Python script.'\n    Returns:\n      Value: self.instruct_list\n      Purpose: 'In the given context, we need to elaborate on the purpose and significance of 'self.instruct_list' within the DatasetGenerator class after generating outputs using its generate() method. Self.instruct_list is a list that stores question-answer pairs retrieved during the dataset generation process. It holds structured data containing instructions (questions), input contexts related to those questions, and their respective output responses.\n        The primary role of self.instruct_list is to organize all generated answers from various question types such as file details, functions, classes, methods, etc., into a unified format ready for JSON serialization. Each item in the list is a dictionary comprising keys 'instruction', 'input', and 'output'. The 'instruction' key represents the original query posed by users, 'input' stores context information required by language models during response generation, and 'output' saves retrieved responses. After invoking generate(), self.instruct_list serves as one of the main outputs alongside code_qa_dict. These structured outputs enable further processing or visualization of generated JSON formatted question-answer pairs derived from a Python file with associated queries.'"
    purpose: 'I) Python file `py2dataset\get_python_datasets.py` serves as a central component to extract information from a given Python file along with associated questions pertaining to it and generate structured outputs in JSON format containing question-answer pairs. This dataset generation involves multiple processes, namely file processing through Language Models for generating detailed explanations and an extensive series of methods for question classification, data organization, response generation, etc. Let''s delve deeper into each aspect:


        1. Dependencies: The script relies on four external libraries - `re`, `yaml`, `logging`, and `math`. These libraries enable regular expression handling (`re`), managing complex data structures like dictionaries (`yaml`), logging errors or debugging information (`logging`), and mathematical operations (`math`) respectively.


        2. Functions: There are three user-defined functions in the script - `get_unique_elements`, `element_generator`, and `get_python_datasets`.

        a. `get_unique_elements(input_str)` cleans an input string by removing duplicates and returns a string containing unique elements separated by commas. It uses `enumerate`, `input_str[start:i].strip()`, `input_str[start:].strip()`, `input_str.strip("[]''\"")`, and `getelement generator`. Its application streamlines readability of complex data structures.

        b. `element_generator(input_str)` generates elements from an input string while maintaining contextual integrity by tracking braces (''{'', ''}''). It uses `enumerate`, `input_str[start:i].strip()`, `input_str[start:].strip()`, `yield`, and `lambda`.

        c. `get_python_datasets(file_path, file_details, base_name, questions, model_config, detailed)` instantiates a `DatasetGenerator` class using given inputs and invokes its `generate()` method to retrieve instruct_list and code_qa_dict as outputs - crucial data structures for JSON formatted question-answer pairs. It uses the `DatasetGenerator` class for extensive functionality described below.


        3. Classes: There is one major user-defined class named `DatasetGenerator`. It encapsulates multiple methods and attributes to achieve its purpose.

        a. `__init__(self, file_path, file_details, base_name, questions, model_config, detailed)` initializes the DatasetGenerator object by setting input arguments as respective instance variables while creating internal attributes for code_qa_response storage (to hold generated responses for code questions), detailed response flag based on LLM usage, and instruct_list to store question-answer pairs. It also defines question mapping between file details and question types (''file'', ''function'', ''class'', ''method'').

        b. `format_response()` formats the code_qa_dict output by applying YAML formatting techniques for readability. This ensures that generated JSON data remains organized.

        c. `get_response_from_llm(self, query, context)` queries language models using specified prompt template (derived from configuration). It attempts different context strategies and manages LLM response processing while handling errors and detailed response generation if necessary. This method relies heavily on external LLM services through the `llm` attribute.

        d. `get_detailed_response(context, response)` generates detailed responses for code objects by querying LLM again with additional context and question-specific information. It updates relevant data structures accordingly.

        e. `get_code_qa()` identifies code responses from instruct_list and updates internal attributes like code_qa_dict to store these responses in a structured manner.

        f. `process_question(self, question_id, query, context, info)` processes questions related to different file entities (''file'', ''function'', ''class'', or ''method'') by invoking other methods and appending relevant outputs to instruct_list based on the given question IDs.

        g. `get_info_string(info, item_type)` returns a string of comma-separated distinct values from given dictionary ''info'' associated with the provided key (item_type).

        h. `process_question_type(self, question_type, question_id, question_text)` dispatches further question processing as per query types such as handling function-related or class-related questions by invoking `process_question()`. It also collects additional information like variables and methods for ''class'' type questions.

        i. `generate(self)` processes all given questions and generates instruct_list with answers for JSON output. It calls multiple internal methods to accomplish this task.


        III. Key Inputs include Python file path, file details (dictionary), base name of the Python file, list of questions, model configuration (dict), and detailed flag indicating LLM usage. Outputs are instruct_list containing question-answer pairs and code_qa_dict with code responses from instruct_list. Variables like llm, use_llm, detailed, instruct_list, question_mapping, code_qa_list, etc., assist internal data manipulation for better readability. Functions/methods invoke each other forming a complex but coordinated system for comprehensive dataset generation according to specified requirements in the codebase.

        '
functions:
    get_unique_elements:
        function_name: get_unique_elements
        function_code: "def get_unique_elements(input_str: str) -> str:\n    \"\"\"\n    Clean an input string (str) and return a string of unique elements.\n    Args:\n        input_str (str): Input string.\n    Returns:\n        str: String of unique elements.\n    \"\"\"\n\n    def element_generator(input_str):\n        start, brace_level = (0, 0)\n        for i, char in enumerate(input_str):\n            if char in '{}':\n                brace_level += 1 if char == '{' else -1\n            elif char == ',' and brace_level == 0:\n                yield input_str[start:i].strip('\\'\" ')\n                start = i + 1\n        yield input_str[start:].strip('\\'\" ')\n    input_str = input_str.strip('[]\\'\"')\n    cleaned_elements = [element for element in element_generator(input_str) if element]\n    return ', '.join(cleaned_elements)"
        function_docstring: "\n    Clean an input string (str) and return a string of unique elements.\n    Args:\n        input_str (str): Input string.\n    Returns:\n        str: String of unique elements.\n    "
        function_inputs:
        - input_str
        function_defaults: []
        function_returns:
        - ''', ''.join(cleaned_elements)'
        function_calls:
        - enumerate
        - input_str[start:i].strip
        - input_str[start:].strip
        - input_str.strip
        - element_generator
        - ''', ''.join'
        function_call_inputs:
            enumerate:
            - input_str
            input_str[start:i].strip:
            - '''\''" '''
            input_str[start:].strip:
            - '''\''" '''
            input_str.strip:
            - '''[]\''"'''
            element_generator:
            - input_str
            ''', ''.join':
            - cleaned_elements
        function_variables:
        - cleaned_elements
        - start
        - input_str
        function_decorators: []
        function_annotations: []
        function_properties: []
    element_generator:
        function_name: element_generator
        function_code: "def element_generator(input_str):\n    start, brace_level = (0, 0)\n    for i, char in enumerate(input_str):\n        if char in '{}':\n            brace_level += 1 if char == '{' else -1\n        elif char == ',' and brace_level == 0:\n            yield input_str[start:i].strip('\\'\" ')\n            start = i + 1\n    yield input_str[start:].strip('\\'\" ')"
        function_docstring: null
        function_inputs:
        - input_str
        function_defaults: []
        function_returns: []
        function_calls:
        - enumerate
        - input_str[start:i].strip
        - input_str[start:].strip
        function_call_inputs:
            enumerate:
            - input_str
            input_str[start:i].strip:
            - '''\''" '''
            input_str[start:].strip:
            - '''\''" '''
        function_variables:
        - start
        - input_str
        function_decorators: []
        function_annotations: []
        function_properties: []
    get_python_datasets:
        function_name: get_python_datasets
        function_code: "def get_python_datasets(file_path: str, file_details: dict, base_name: str, questions: list[dict], model_config: dict, detailed: bool) -> tuple[list[dict], list[dict]]:\n    \"\"\"\n    Extract information from a Python file and return it in JSON format.\n    Args:\n        file_path (str): Path to the Python file.\n        file_details (dict): Dictionary containing details of the Python file.\n        base_name (str): Base name of the Python file.\n        questions (list): List of questions to be answered for the Python file.\n        model_config (dict): Dictionary containing the model configuration.\n        detailed (bool): Flag to generate detailed responses.\n    Returns:\n        tuple: Tuple containing the instruct_list and code_qa_dict.\n    \"\"\"\n    return DatasetGenerator(file_path, file_details, base_name, questions, model_config, detailed).generate()"
        function_docstring: "\n    Extract information from a Python file and return it in JSON format.\n    Args:\n        file_path (str): Path to the Python file.\n        file_details (dict): Dictionary containing details of the Python file.\n        base_name (str): Base name of the Python file.\n        questions (list): List of questions to be answered for the Python file.\n        model_config (dict): Dictionary containing the model configuration.\n        detailed (bool): Flag to generate detailed responses.\n    Returns:\n        tuple: Tuple containing the instruct_list and code_qa_dict.\n    "
        function_inputs:
        - file_path
        - file_details
        - base_name
        - questions
        - model_config
        - detailed
        function_defaults: []
        function_returns:
        - DatasetGenerator(file_path, file_details, base_name, questions, model_config, detailed).generate()
        function_calls:
        - DatasetGenerator(file_path, file_details, base_name, questions, model_config, detailed).generate
        - DatasetGenerator
        function_call_inputs:
            DatasetGenerator(file_path, file_details, base_name, questions, model_config, detailed).generate: []
            DatasetGenerator:
            - file_path
            - file_details
            - base_name
            - questions
            - model_config
            - detailed
        function_variables:
        - detailed
        - file_details
        - questions
        - file_path
        - model_config
        - base_name
        function_decorators: []
        function_annotations: []
        function_properties: []
classes:
    DatasetGenerator:
        class_name: DatasetGenerator
        class_code: "class DatasetGenerator:\n    \"\"\"\n    Generate JSON formatted dictionary outputs for a Python file.\n    Args:\n        file_path (str): Path to the Python file.\n        file_details (dict): Dictionary containing details of the Python file.\n        base_name (str): Base name of the Python file.\n        questions (list): List of questions to be answered for the Python file.\n        model_config (dict): Dictionary containing the model configuration.\n        detailed (bool): Flag to generate detailed responses.\n    Attributes:\n        file_path (str): Path to the Python file.\n        file_details (dict): Dictionary containing details of the Python file.\n        base_name (str): Base name of the Python file.\n        questions (list): List of questions to be answered for the Python file.\n        model_config (dict): Dictionary containing the model configuration.\n        llm (obj): Language model object.\n        use_llm (bool): Flag to use language model.\n        detailed (bool): Flag to generate detailed responses.\n        instruct_list (list): List of instructions and responses.\n        question_mapping (dict): Mapping of question types to file details.\n        code_qa_list (list): List of code questions and responses.\n        code_qa_response (str): Response for the code_qa_dict.\n        code_qa_dict (dict): Dictionary containing code questions and responses.\n    \"\"\"\n\n    def __init__(self, file_path: str, file_details: dict, base_name: str, questions: list[dict], model_config: dict, detailed: bool) -> None:\n        \"\"\"Initialize the DatasetGenerator class.\"\"\"\n        self.file_path = file_path\n        self.file_details = file_details\n        self.base_name = base_name\n        self.questions = questions\n        self.model_config = model_config\n        self.llm = model_config['model'] if model_config else None\n        self.use_llm = bool(model_config)\n        self.detailed = detailed if self.use_llm else False\n        self.instruct_list = []\n        self.question_mapping = {'file': 'file', 'function': 'functions', 'class': 'classes', 'method': 'classes'}\n        self.code_qa_list = []\n        self.code_qa_response = ''\n\n    def format_response(self):\n        \"\"\"Format the response for the code_qa_dict.\"\"\"\n        self.code_qa_response = re.sub('\\\\n\\\\s*\\\\n', '\\n', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False, indent=2)).replace(\"''\", \"'\").strip('\"').strip(\"'\").strip()\n        self.file_details['file_info']['code_qa_response'] = self.code_qa_response\n\n    def get_response_from_llm(self, query: str, context: str) -> str:\n        \"\"\"\n        Get language model response to query for given context.\n        Args:\n            query (str): Query to be answered.\n            context (str): Context for the query.\n        Returns:\n            str: Language model response.\n        \"\"\"\n        context_strategies = [lambda: str(context), lambda: f\"```python\\n{self.file_details['file_info']['file_code_simplified']}\\n```\", lambda: f\"```python\\n{self.get_info_string(self.file_details['file_info'], 'file_summary')}\\n```\", lambda: '']\n        max_context_length = self.model_config['inference_model']['model_params']['context_length']\n        prompt_template = self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'], instruction_prompt=self.model_config['instruction_prompt'])\n        for strategy in context_strategies:\n            context = strategy()\n            prompt = prompt_template.format(context=context, query=query, code_objects=self.code_qa_response)\n            context_size = len(self.llm.tokenize(prompt))\n            logging.info(f'***Context Size: {context_size}')\n            if context_size <= 0.7 * max_context_length:\n                break\n        else:\n            err_msg = 'Model response failed, increase py2dataset_model_config.yaml context_length'\n            logging.error(f'{err_msg} > {math.ceil(context_size / 0.7)}')\n            return ''\n        context = self.code_qa_list if context == '' else context\n        response = ''\n        try:\n            response = re.sub('\\\\n\\\\s*\\\\n', '\\n\\n', self.llm(prompt)).replace('<|im_end|>', '').replace('</|im_end|>', '')\n            response = '\\n'.join((line.lstrip() for line in response.split('\\n')))\n            logging.info(f'***Overall Response: {response}')\n        except Exception as error:\n            logging.error(f'Failed to generate model response: {error}')\n        if self.detailed:\n            self.get_detailed_response(context, response)\n        basename = str(self.base_name).replace('\\\\', '/')\n        self.code_qa_dict = {basename: self.code_qa_dict}\n        self.code_qa_dict[basename] = {'Code Documentation': [response], **self.code_qa_dict[basename]}\n        self.format_response()\n        self.file_details['file_info']['purpose'] = response\n        return response\n\n    def get_detailed_response(self, context: str, response: str) -> None:\n        \"\"\"\n        Generate detailed responses for code objects.\n        Args:\n            context (str): Context for the response.\n            response (str): Response from the model.\n        \"\"\"\n        for item in self.code_qa_list:\n            try:\n                instruct_key = list(item.keys())[0]\n                instruct_value = list(item.values())[0]\n                query = f'Describe the Purpose and Significance of these {instruct_key}: [{instruct_value}] and Explain what each of these {instruct_key} does in the code.'\n                prompt = self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'], instruction_prompt=self.model_config['instruction_prompt']).format(context=f'{context}/nCode Summary:/n{response}', query=f'{query}', code_objects=f'{instruct_value}')\n                item_response = re.sub('\\\\n\\\\s*\\\\n', '\\n\\n', self.llm(prompt)).replace('<|im_end|>', '').replace('</|im_end|>', '')\n                logging.info(f'\\n***Itemized Response: {query}\\n{item_response}')\n                for item in self.instruct_list:\n                    if item['instruction'].startswith(instruct_key):\n                        output = f'\\n\\nPurpose and Significance:\\n{item_response}'\n                        item['output'] += output\n                if '`' in instruct_key:\n                    dict_key1 = instruct_key.split('`')[1]\n                    dict_key2 = instruct_key.split()[0]\n                else:\n                    dict_key1 = instruct_key\n                    dict_key2 = ''\n                purpose_dict = {'Purpose': item_response.strip()}\n                if dict_key1 not in self.code_qa_dict:\n                    self.code_qa_dict[dict_key1] = {}\n                elif not isinstance(self.code_qa_dict[dict_key1], dict):\n                    value = self.code_qa_dict[dict_key1]\n                    self.code_qa_dict[dict_key1] = {'Value': value}\n                if dict_key2:\n                    if not isinstance(self.code_qa_dict[dict_key1], dict):\n                        self.code_qa_dict[dict_key1] = {}\n                    if dict_key2 not in self.code_qa_dict[dict_key1] or not isinstance(self.code_qa_dict[dict_key1][dict_key2], dict):\n                        value = self.code_qa_dict[dict_key1].get(dict_key2, '')\n                        self.code_qa_dict[dict_key1][dict_key2] = {'Value': value}\n                    self.code_qa_dict[dict_key1][dict_key2].update(purpose_dict)\n                else:\n                    self.code_qa_dict[dict_key1].update(purpose_dict)\n            except Exception as error:\n                logging.error(f'Failed to generate detailed response: {error}')\n\n    def get_code_qa(self):\n        \"\"\"Get code responses from the instruct_list and update.\"\"\"\n        excluded = {'Call code graph', 'Docstring'}\n        self.code_qa_list = []\n        responses = {}\n        for item in self.instruct_list:\n            instruction, output = (item['instruction'].split(' in Python file:')[0], item['output'])\n            if not any((instruction.startswith(prefix) for prefix in excluded)):\n                self.code_qa_list.append({instruction: output})\n                if '`' in instruction:\n                    code_object, code_type = (instruction.split('`')[1], instruction.split()[0])\n                    responses.setdefault(code_object, []).append((code_type, output))\n                else:\n                    responses.setdefault(instruction, []).append((instruction, output))\n        self.code_qa_dict = {}\n        for code_object, type_responses in responses.items():\n            self.code_qa_dict[code_object] = {}\n            for code_type, response in type_responses:\n                if code_object == code_type:\n                    self.code_qa_dict[code_object] = response\n                else:\n                    self.code_qa_dict.setdefault(code_object, {})[code_type] = response\n        if not self.use_llm:\n            basename = str(self.base_name).replace('\\\\', '/')\n            self.code_qa_dict = {basename: self.code_qa_dict}\n        self.format_response()\n\n    def process_question(self, question_id: str, query: str, context: str, info: dict) -> None:\n        \"\"\"Process question and add the generated response to the instruct_list.\"\"\"\n        if question_id.endswith(('code_graph', 'docstring')):\n            response = info.get(question_id, {})\n        elif question_id.endswith('file_purpose'):\n            self.get_code_qa()\n            response = self.get_response_from_llm(query, context) if self.use_llm else ''\n        else:\n            response = get_unique_elements(str(info.get(question_id, '')))\n        if response and response != 'None':\n            response = str(response).strip()\n            self.instruct_list.append({'instruction': query, 'input': context, 'output': response})\n\n    @staticmethod\n    def get_info_string(info: dict, item_type: str) -> str:\n        \"\"\"Get string from info dictionary.\"\"\"\n        return ', '.join([item.strip() for item in str(info.get(item_type, '')).split(',') if item])\n\n    def process_question_type(self, question_type: str, question_id: str, question_text: str) -> None:\n        \"\"\"\n        Process questions related to a file, function, class, or method.\n        Args:\n            question_type (str): Type of question.\n            question_id (str): Question ID.\n            question_text (str): Question text.\n        \"\"\"\n        if question_type == 'file':\n            query = question_text.format(filename=self.base_name)\n            info = self.file_details['file_info']\n            context = f\"```python\\n{self.file_details['file_info']['file_code']}\\n```\"\n            self.process_question(question_id, query, context, info)\n        elif question_type == 'method':\n            for class_name, class_info in self.file_details['classes'].items():\n                for key, method_info in class_info.items():\n                    if key.startswith('class_method_'):\n                        method_name = f\"{class_name}.{key[len('class_method_'):]}\"\n                        context = f\"```python\\n{method_info['method_code']}\\n```\"\n                        mapping = {'class_name': class_name, 'method_name': method_name}\n                        query = question_text.format(filename=self.base_name, **mapping)\n                        self.process_question(question_id, query, context, method_info)\n        else:\n            for name, info in self.file_details[self.question_mapping[question_type]].items():\n                context = f\"```python\\n{info[f'{question_type}_code']}\\n```\"\n                mapping = {f'{question_type}_name': name}\n                if question_id == f'{question_type}_purpose' and self.use_llm:\n                    variables = self.get_info_string(info, f'{question_type}_variables')\n                    inputs = self.get_info_string(info, f'{question_type}_inputs')\n                    combined = ', '.join(filter(None, [variables, inputs]))\n                    mapping[f'{question_type}_variables'] = get_unique_elements(combined)\n                    if question_type == 'class':\n                        mapping[f'{question_type}_methods'] = self.get_info_string(info, f'{question_type}_methods')\n                query = question_text.format(filename=self.base_name, **mapping)\n                self.process_question(question_id, query, context, info)\n\n    def generate(self) -> tuple[list[dict], list[dict]]:\n        \"\"\"Generate responses for all the questions and returns the instruct_list.\"\"\"\n        for question in self.questions:\n            self.process_question_type(question['type'], question['id'], question['text'])\n        self.instruct_list.sort(key=lambda x: len(x['input']), reverse=True)\n        return self.instruct_list"
        class_docstring: "\n    Generate JSON formatted dictionary outputs for a Python file.\n    Args:\n        file_path (str): Path to the Python file.\n        file_details (dict): Dictionary containing details of the Python file.\n        base_name (str): Base name of the Python file.\n        questions (list): List of questions to be answered for the Python file.\n        model_config (dict): Dictionary containing the model configuration.\n        detailed (bool): Flag to generate detailed responses.\n    Attributes:\n        file_path (str): Path to the Python file.\n        file_details (dict): Dictionary containing details of the Python file.\n        base_name (str): Base name of the Python file.\n        questions (list): List of questions to be answered for the Python file.\n        model_config (dict): Dictionary containing the model configuration.\n        llm (obj): Language model object.\n        use_llm (bool): Flag to use language model.\n        detailed (bool): Flag to generate detailed responses.\n        instruct_list (list): List of instructions and responses.\n        question_mapping (dict): Mapping of question types to file details.\n        code_qa_list (list): List of code questions and responses.\n        code_qa_response (str): Response for the code_qa_dict.\n        code_qa_dict (dict): Dictionary containing code questions and responses.\n    "
        class_inputs: null
        class_defaults: null
        class_returns:
        - response
        - ''', ''.join([item.strip() for item in str(info.get(item_type, '''')).split('','') if item])'
        - self.instruct_list
        - ''''''
        class_calls:
        - bool
        - re.sub('\\n\\s*\\n', '\n', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False, indent=2)).replace("''", "'").strip('"').strip("'").strip
        - re.sub('\\n\\s*\\n', '\n', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False, indent=2)).replace("''", "'").strip('"').strip
        - re.sub('\\n\\s*\\n', '\n', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False, indent=2)).replace("''", "'").strip
        - re.sub('\\n\\s*\\n', '\n', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False, indent=2)).replace
        - re.sub
        - yaml.dump
        - float
        - str
        - self.get_info_string
        - self.model_config['prompt_template'].format
        - strategy
        - prompt_template.format
        - len
        - self.llm.tokenize
        - logging.info
        - logging.error
        - math.ceil
        - re.sub('\\n\\s*\\n', '\n\n', self.llm(prompt)).replace('<|im_end|>', '').replace
        - re.sub('\\n\\s*\\n', '\n\n', self.llm(prompt)).replace
        - self.llm
        - '''\n''.join'
        - line.lstrip
        - response.split
        - self.get_detailed_response
        - str(self.base_name).replace
        - self.format_response
        - list
        - item.keys
        - item.values
        - self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'], instruction_prompt=self.model_config['instruction_prompt']).format
        - item['instruction'].startswith
        - instruct_key.split
        - item_response.strip
        - isinstance
        - self.code_qa_dict[dict_key1].get
        - self.code_qa_dict[dict_key1][dict_key2].update
        - self.code_qa_dict[dict_key1].update
        - item['instruction'].split
        - any
        - instruction.startswith
        - self.code_qa_list.append
        - instruction.split
        - responses.setdefault(code_object, []).append
        - responses.setdefault
        - responses.setdefault(instruction, []).append
        - responses.items
        - self.code_qa_dict.setdefault
        - question_id.endswith
        - info.get
        - self.get_code_qa
        - self.get_response_from_llm
        - get_unique_elements
        - str(response).strip
        - self.instruct_list.append
        - ''', ''.join'
        - item.strip
        - str(info.get(item_type, '')).split
        - question_text.format
        - self.process_question
        - self.file_details['classes'].items
        - class_info.items
        - key.startswith
        - self.file_details[self.question_mapping[question_type]].items
        - filter
        - self.process_question_type
        - self.instruct_list.sort
        class_call_inputs:
            bool:
            - model_config
            ? re.sub('\\n\\s*\\n', '\n', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False, indent=2)).replace("''", "'").strip('"').strip("'").strip
            : []
            ? re.sub('\\n\\s*\\n', '\n', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False, indent=2)).replace("''", "'").strip('"').strip
            :   - '"''"'
            ? re.sub('\\n\\s*\\n', '\n', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False, indent=2)).replace("''", "'").strip
            :   - '''"'''
            ? re.sub('\\n\\s*\\n', '\n', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False, indent=2)).replace
            :   - '"''''"'
                - '"''"'
            re.sub:
            - '''\\n\\s*\\n'''
            - '''\n'''
            - yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False, indent=2)
            - '''\\n\\s*\\n'''
            - '''\n\n'''
            - self.llm(prompt)
            - '''\\n\\s*\\n'''
            - '''\n\n'''
            - self.llm(prompt)
            yaml.dump:
            - self.code_qa_dict
            float:
            - '''inf'''
            str:
            - context
            - self.base_name
            - self.base_name
            - info.get(question_id, '')
            - response
            - info.get(item_type, '')
            self.get_info_string:
            - self.file_details['file_info']
            - '''file_summary'''
            - info
            - f'{question_type}_variables'
            - info
            - f'{question_type}_inputs'
            - info
            - f'{question_type}_methods'
            self.model_config['prompt_template'].format: []
            strategy: []
            prompt_template.format: []
            len:
            - self.llm.tokenize(prompt)
            - '''class_method_'''
            - x['input']
            self.llm.tokenize:
            - prompt
            logging.info:
            - 'f''***Context Size: {context_size}'''
            - 'f''***Overall Response: {response}'''
            - 'f''\n***Itemized Response: {query}\n{item_response}'''
            logging.error:
            - f'{err_msg} > {math.ceil(context_size / 0.7)}'
            - 'f''Failed to generate model response: {error}'''
            - 'f''Failed to generate detailed response: {error}'''
            math.ceil:
            - context_size / 0.7
            re.sub('\\n\\s*\\n', '\n\n', self.llm(prompt)).replace('<|im_end|>', '').replace:
            - '''</|im_end|>'''
            - ''''''
            - '''</|im_end|>'''
            - ''''''
            re.sub('\\n\\s*\\n', '\n\n', self.llm(prompt)).replace:
            - '''<|im_end|>'''
            - ''''''
            - '''<|im_end|>'''
            - ''''''
            self.llm:
            - prompt
            - prompt
            '''\n''.join':
            - (line.lstrip() for line in response.split('\n'))
            line.lstrip: []
            response.split:
            - '''\n'''
            self.get_detailed_response:
            - context
            - response
            str(self.base_name).replace:
            - '''\\'''
            - '''/'''
            - '''\\'''
            - '''/'''
            self.format_response: []
            list:
            - item.keys()
            - item.values()
            item.keys: []
            item.values: []
            ? self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'], instruction_prompt=self.model_config['instruction_prompt']).format
            : []
            item['instruction'].startswith:
            - instruct_key
            instruct_key.split:
            - '''`'''
            item_response.strip: []
            isinstance:
            - self.code_qa_dict[dict_key1]
            - dict
            - self.code_qa_dict[dict_key1]
            - dict
            - self.code_qa_dict[dict_key1][dict_key2]
            - dict
            self.code_qa_dict[dict_key1].get:
            - dict_key2
            - ''''''
            self.code_qa_dict[dict_key1][dict_key2].update:
            - purpose_dict
            self.code_qa_dict[dict_key1].update:
            - purpose_dict
            item['instruction'].split:
            - ''' in Python file:'''
            any:
            - (instruction.startswith(prefix) for prefix in excluded)
            instruction.startswith:
            - prefix
            self.code_qa_list.append:
            - '{instruction: output}'
            instruction.split:
            - '''`'''
            responses.setdefault(code_object, []).append:
            - (code_type, output)
            responses.setdefault:
            - code_object
            - '[]'
            - instruction
            - '[]'
            responses.setdefault(instruction, []).append:
            - (instruction, output)
            responses.items: []
            self.code_qa_dict.setdefault:
            - code_object
            - '{}'
            question_id.endswith:
            - ('code_graph', 'docstring')
            - '''file_purpose'''
            info.get:
            - question_id
            - '{}'
            - question_id
            - ''''''
            - item_type
            - ''''''
            self.get_code_qa: []
            self.get_response_from_llm:
            - query
            - context
            get_unique_elements:
            - str(info.get(question_id, ''))
            - combined
            str(response).strip: []
            self.instruct_list.append:
            - '{''instruction'': query, ''input'': context, ''output'': response}'
            ''', ''.join':
            - '[item.strip() for item in str(info.get(item_type, '''')).split('','') if item]'
            - filter(None, [variables, inputs])
            item.strip: []
            str(info.get(item_type, '')).split:
            - ''','''
            question_text.format: []
            self.process_question:
            - question_id
            - query
            - context
            - info
            - question_id
            - query
            - context
            - method_info
            - question_id
            - query
            - context
            - info
            self.file_details['classes'].items: []
            class_info.items: []
            key.startswith:
            - '''class_method_'''
            self.file_details[self.question_mapping[question_type]].items: []
            filter:
            - None
            - '[variables, inputs]'
            self.process_question_type:
            - question['type']
            - question['id']
            - question['text']
            self.instruct_list.sort: []
        class_variables:
        - variables
        - basename
        - max_context_length
        - context
        - context_size
        - err_msg
        - instruct_value
        - inputs
        - value
        - query
        - dict_key1
        - dict_key2
        - output
        - excluded
        - item_response
        - info
        - instruct_key
        - method_name
        - purpose_dict
        - prompt_template
        - combined
        - response
        - responses
        - prompt
        - context_strategies
        - mapping
        class_decorators: []
        class_annotations: []
        class_properties:
        - self.code_qa_dict
        - self.code_qa_response
        - self.file_path
        - self.model_config
        - self.base_name
        - self.questions
        - self.llm
        - self.use_llm
        - self.detailed
        - self.code_qa_list
        - self.instruct_list
        - self.file_details
        - self.question_mapping
        class_attributes:
        - file_path
        - file_details
        - base_name
        - questions
        - model_config
        - llm
        - use_llm
        - detailed
        - instruct_list
        - question_mapping
        - code_qa_list
        - code_qa_response
        - code_qa_response
        - code_qa_dict
        - code_qa_list
        - code_qa_dict
        - code_qa_dict
        class_methods:
        - __init__
        - format_response
        - get_response_from_llm
        - get_detailed_response
        - get_code_qa
        - process_question
        - get_info_string
        - process_question_type
        - generate
        class_inheritance: []
        class_static_methods:
        - get_info_string
        class_method___init__:
            method_name: __init__
            method_code: "def __init__(self, file_path: str, file_details: dict, base_name: str, questions: list[dict], model_config: dict, detailed: bool) -> None:\n    \"\"\"Initialize the DatasetGenerator class.\"\"\"\n    self.file_path = file_path\n    self.file_details = file_details\n    self.base_name = base_name\n    self.questions = questions\n    self.model_config = model_config\n    self.llm = model_config['model'] if model_config else None\n    self.use_llm = bool(model_config)\n    self.detailed = detailed if self.use_llm else False\n    self.instruct_list = []\n    self.question_mapping = {'file': 'file', 'function': 'functions', 'class': 'classes', 'method': 'classes'}\n    self.code_qa_list = []\n    self.code_qa_response = ''"
            method_docstring: Initialize the DatasetGenerator class.
            method_inputs:
            - self
            - file_path
            - file_details
            - base_name
            - questions
            - model_config
            - detailed
            method_defaults: []
            method_returns: []
            method_calls:
            - bool
            method_call_inputs:
                bool:
                - model_config
            method_variables:
            - self
            - detailed
            - file_details
            - questions
            - file_path
            - model_config
            - base_name
            method_decorators: []
            method_annotations: []
            method_properties:
            - self.code_qa_response
            - self.file_path
            - self.model_config
            - self.base_name
            - self.questions
            - self.llm
            - self.use_llm
            - self.detailed
            - self.code_qa_list
            - self.instruct_list
            - self.file_details
            - self.question_mapping
        class_method_format_response:
            method_name: format_response
            method_code: "def format_response(self):\n    \"\"\"Format the response for the code_qa_dict.\"\"\"\n    self.code_qa_response = re.sub('\\\\n\\\\s*\\\\n', '\\n', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False, indent=2)).replace(\"''\", \"'\").strip('\"').strip(\"'\").strip()\n    self.file_details['file_info']['code_qa_response'] = self.code_qa_response"
            method_docstring: Format the response for the code_qa_dict.
            method_inputs:
            - self
            method_defaults: []
            method_returns: []
            method_calls:
            - re.sub('\\n\\s*\\n', '\n', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False, indent=2)).replace("''", "'").strip('"').strip("'").strip
            - re.sub('\\n\\s*\\n', '\n', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False, indent=2)).replace("''", "'").strip('"').strip
            - re.sub('\\n\\s*\\n', '\n', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False, indent=2)).replace("''", "'").strip
            - re.sub('\\n\\s*\\n', '\n', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False, indent=2)).replace
            - re.sub
            - yaml.dump
            - float
            method_call_inputs:
                ? re.sub('\\n\\s*\\n', '\n', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False, indent=2)).replace("''", "'").strip('"').strip("'").strip
                : []
                ? re.sub('\\n\\s*\\n', '\n', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False, indent=2)).replace("''", "'").strip('"').strip
                :   - '"''"'
                ? re.sub('\\n\\s*\\n', '\n', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False, indent=2)).replace("''", "'").strip
                :   - '''"'''
                ? re.sub('\\n\\s*\\n', '\n', yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False, indent=2)).replace
                :   - '"''''"'
                    - '"''"'
                re.sub:
                - '''\\n\\s*\\n'''
                - '''\n'''
                - yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False, indent=2)
                yaml.dump:
                - self.code_qa_dict
                float:
                - '''inf'''
            method_variables:
            - self
            method_decorators: []
            method_annotations: []
            method_properties:
            - self.code_qa_response
        class_method_get_response_from_llm:
            method_name: get_response_from_llm
            method_code: "def get_response_from_llm(self, query: str, context: str) -> str:\n    \"\"\"\n        Get language model response to query for given context.\n        Args:\n            query (str): Query to be answered.\n            context (str): Context for the query.\n        Returns:\n            str: Language model response.\n        \"\"\"\n    context_strategies = [lambda: str(context), lambda: f\"```python\\n{self.file_details['file_info']['file_code_simplified']}\\n```\", lambda: f\"```python\\n{self.get_info_string(self.file_details['file_info'], 'file_summary')}\\n```\", lambda: '']\n    max_context_length = self.model_config['inference_model']['model_params']['context_length']\n    prompt_template = self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'], instruction_prompt=self.model_config['instruction_prompt'])\n    for strategy in context_strategies:\n        context = strategy()\n        prompt = prompt_template.format(context=context, query=query, code_objects=self.code_qa_response)\n        context_size = len(self.llm.tokenize(prompt))\n        logging.info(f'***Context Size: {context_size}')\n        if context_size <= 0.7 * max_context_length:\n            break\n    else:\n        err_msg = 'Model response failed, increase py2dataset_model_config.yaml context_length'\n        logging.error(f'{err_msg} > {math.ceil(context_size / 0.7)}')\n        return ''\n    context = self.code_qa_list if context == '' else context\n    response = ''\n    try:\n        response = re.sub('\\\\n\\\\s*\\\\n', '\\n\\n', self.llm(prompt)).replace('<|im_end|>', '').replace('</|im_end|>', '')\n        response = '\\n'.join((line.lstrip() for line in response.split('\\n')))\n        logging.info(f'***Overall Response: {response}')\n    except Exception as error:\n        logging.error(f'Failed to generate model response: {error}')\n    if self.detailed:\n        self.get_detailed_response(context, response)\n    basename = str(self.base_name).replace('\\\\', '/')\n    self.code_qa_dict = {basename: self.code_qa_dict}\n    self.code_qa_dict[basename] = {'Code Documentation': [response], **self.code_qa_dict[basename]}\n    self.format_response()\n    self.file_details['file_info']['purpose'] = response\n    return response"
            method_docstring: "\n        Get language model response to query for given context.\n        Args:\n            query (str): Query to be answered.\n            context (str): Context for the query.\n        Returns:\n            str: Language model response.\n        "
            method_inputs:
            - self
            - query
            - context
            method_defaults: []
            method_returns:
            - response
            - ''''''
            method_calls:
            - str
            - self.get_info_string
            - self.model_config['prompt_template'].format
            - strategy
            - prompt_template.format
            - len
            - self.llm.tokenize
            - logging.info
            - logging.error
            - math.ceil
            - re.sub('\\n\\s*\\n', '\n\n', self.llm(prompt)).replace('<|im_end|>', '').replace
            - re.sub('\\n\\s*\\n', '\n\n', self.llm(prompt)).replace
            - re.sub
            - self.llm
            - '''\n''.join'
            - line.lstrip
            - response.split
            - self.get_detailed_response
            - str(self.base_name).replace
            - self.format_response
            method_call_inputs:
                str:
                - context
                - self.base_name
                self.get_info_string:
                - self.file_details['file_info']
                - '''file_summary'''
                self.model_config['prompt_template'].format: []
                strategy: []
                prompt_template.format: []
                len:
                - self.llm.tokenize(prompt)
                self.llm.tokenize:
                - prompt
                logging.info:
                - 'f''***Context Size: {context_size}'''
                - 'f''***Overall Response: {response}'''
                logging.error:
                - f'{err_msg} > {math.ceil(context_size / 0.7)}'
                - 'f''Failed to generate model response: {error}'''
                math.ceil:
                - context_size / 0.7
                re.sub('\\n\\s*\\n', '\n\n', self.llm(prompt)).replace('<|im_end|>', '').replace:
                - '''</|im_end|>'''
                - ''''''
                re.sub('\\n\\s*\\n', '\n\n', self.llm(prompt)).replace:
                - '''<|im_end|>'''
                - ''''''
                re.sub:
                - '''\\n\\s*\\n'''
                - '''\n\n'''
                - self.llm(prompt)
                self.llm:
                - prompt
                '''\n''.join':
                - (line.lstrip() for line in response.split('\n'))
                line.lstrip: []
                response.split:
                - '''\n'''
                self.get_detailed_response:
                - context
                - response
                str(self.base_name).replace:
                - '''\\'''
                - '''/'''
                self.format_response: []
            method_variables:
            - self
            - query
            - basename
            - max_context_length
            - response
            - context
            - prompt_template
            - context_size
            - err_msg
            - prompt
            - context_strategies
            method_decorators: []
            method_annotations: []
            method_properties:
            - self.code_qa_dict
        class_method_get_detailed_response:
            method_name: get_detailed_response
            method_code: "def get_detailed_response(self, context: str, response: str) -> None:\n    \"\"\"\n        Generate detailed responses for code objects.\n        Args:\n            context (str): Context for the response.\n            response (str): Response from the model.\n        \"\"\"\n    for item in self.code_qa_list:\n        try:\n            instruct_key = list(item.keys())[0]\n            instruct_value = list(item.values())[0]\n            query = f'Describe the Purpose and Significance of these {instruct_key}: [{instruct_value}] and Explain what each of these {instruct_key} does in the code.'\n            prompt = self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'], instruction_prompt=self.model_config['instruction_prompt']).format(context=f'{context}/nCode Summary:/n{response}', query=f'{query}', code_objects=f'{instruct_value}')\n            item_response = re.sub('\\\\n\\\\s*\\\\n', '\\n\\n', self.llm(prompt)).replace('<|im_end|>', '').replace('</|im_end|>', '')\n            logging.info(f'\\n***Itemized Response: {query}\\n{item_response}')\n            for item in self.instruct_list:\n                if item['instruction'].startswith(instruct_key):\n                    output = f'\\n\\nPurpose and Significance:\\n{item_response}'\n                    item['output'] += output\n            if '`' in instruct_key:\n                dict_key1 = instruct_key.split('`')[1]\n                dict_key2 = instruct_key.split()[0]\n            else:\n                dict_key1 = instruct_key\n                dict_key2 = ''\n            purpose_dict = {'Purpose': item_response.strip()}\n            if dict_key1 not in self.code_qa_dict:\n                self.code_qa_dict[dict_key1] = {}\n            elif not isinstance(self.code_qa_dict[dict_key1], dict):\n                value = self.code_qa_dict[dict_key1]\n                self.code_qa_dict[dict_key1] = {'Value': value}\n            if dict_key2:\n                if not isinstance(self.code_qa_dict[dict_key1], dict):\n                    self.code_qa_dict[dict_key1] = {}\n                if dict_key2 not in self.code_qa_dict[dict_key1] or not isinstance(self.code_qa_dict[dict_key1][dict_key2], dict):\n                    value = self.code_qa_dict[dict_key1].get(dict_key2, '')\n                    self.code_qa_dict[dict_key1][dict_key2] = {'Value': value}\n                self.code_qa_dict[dict_key1][dict_key2].update(purpose_dict)\n            else:\n                self.code_qa_dict[dict_key1].update(purpose_dict)\n        except Exception as error:\n            logging.error(f'Failed to generate detailed response: {error}')"
            method_docstring: "\n        Generate detailed responses for code objects.\n        Args:\n            context (str): Context for the response.\n            response (str): Response from the model.\n        "
            method_inputs:
            - self
            - context
            - response
            method_defaults: []
            method_returns: []
            method_calls:
            - list
            - item.keys
            - item.values
            - self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'], instruction_prompt=self.model_config['instruction_prompt']).format
            - self.model_config['prompt_template'].format
            - re.sub('\\n\\s*\\n', '\n\n', self.llm(prompt)).replace('<|im_end|>', '').replace
            - re.sub('\\n\\s*\\n', '\n\n', self.llm(prompt)).replace
            - re.sub
            - self.llm
            - logging.info
            - item['instruction'].startswith
            - instruct_key.split
            - item_response.strip
            - isinstance
            - self.code_qa_dict[dict_key1].get
            - self.code_qa_dict[dict_key1][dict_key2].update
            - self.code_qa_dict[dict_key1].update
            - logging.error
            method_call_inputs:
                list:
                - item.keys()
                - item.values()
                item.keys: []
                item.values: []
                ? self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'], instruction_prompt=self.model_config['instruction_prompt']).format
                : []
                self.model_config['prompt_template'].format: []
                re.sub('\\n\\s*\\n', '\n\n', self.llm(prompt)).replace('<|im_end|>', '').replace:
                - '''</|im_end|>'''
                - ''''''
                re.sub('\\n\\s*\\n', '\n\n', self.llm(prompt)).replace:
                - '''<|im_end|>'''
                - ''''''
                re.sub:
                - '''\\n\\s*\\n'''
                - '''\n\n'''
                - self.llm(prompt)
                self.llm:
                - prompt
                logging.info:
                - 'f''\n***Itemized Response: {query}\n{item_response}'''
                item['instruction'].startswith:
                - instruct_key
                instruct_key.split:
                - '''`'''
                item_response.strip: []
                isinstance:
                - self.code_qa_dict[dict_key1]
                - dict
                - self.code_qa_dict[dict_key1]
                - dict
                - self.code_qa_dict[dict_key1][dict_key2]
                - dict
                self.code_qa_dict[dict_key1].get:
                - dict_key2
                - ''''''
                self.code_qa_dict[dict_key1][dict_key2].update:
                - purpose_dict
                self.code_qa_dict[dict_key1].update:
                - purpose_dict
                logging.error:
                - 'f''Failed to generate detailed response: {error}'''
            method_variables:
            - self
            - query
            - instruct_key
            - item_response
            - response
            - dict_key1
            - purpose_dict
            - context
            - dict_key2
            - output
            - prompt
            - instruct_value
            - value
            method_decorators: []
            method_annotations: []
            method_properties: []
        class_method_get_code_qa:
            method_name: get_code_qa
            method_code: "def get_code_qa(self):\n    \"\"\"Get code responses from the instruct_list and update.\"\"\"\n    excluded = {'Call code graph', 'Docstring'}\n    self.code_qa_list = []\n    responses = {}\n    for item in self.instruct_list:\n        instruction, output = (item['instruction'].split(' in Python file:')[0], item['output'])\n        if not any((instruction.startswith(prefix) for prefix in excluded)):\n            self.code_qa_list.append({instruction: output})\n            if '`' in instruction:\n                code_object, code_type = (instruction.split('`')[1], instruction.split()[0])\n                responses.setdefault(code_object, []).append((code_type, output))\n            else:\n                responses.setdefault(instruction, []).append((instruction, output))\n    self.code_qa_dict = {}\n    for code_object, type_responses in responses.items():\n        self.code_qa_dict[code_object] = {}\n        for code_type, response in type_responses:\n            if code_object == code_type:\n                self.code_qa_dict[code_object] = response\n            else:\n                self.code_qa_dict.setdefault(code_object, {})[code_type] = response\n    if not self.use_llm:\n        basename = str(self.base_name).replace('\\\\', '/')\n        self.code_qa_dict = {basename: self.code_qa_dict}\n    self.format_response()"
            method_docstring: Get code responses from the instruct_list and update.
            method_inputs:
            - self
            method_defaults: []
            method_returns: []
            method_calls:
            - item['instruction'].split
            - any
            - instruction.startswith
            - self.code_qa_list.append
            - instruction.split
            - responses.setdefault(code_object, []).append
            - responses.setdefault
            - responses.setdefault(instruction, []).append
            - responses.items
            - self.code_qa_dict.setdefault
            - str(self.base_name).replace
            - str
            - self.format_response
            method_call_inputs:
                item['instruction'].split:
                - ''' in Python file:'''
                any:
                - (instruction.startswith(prefix) for prefix in excluded)
                instruction.startswith:
                - prefix
                self.code_qa_list.append:
                - '{instruction: output}'
                instruction.split:
                - '''`'''
                responses.setdefault(code_object, []).append:
                - (code_type, output)
                responses.setdefault:
                - code_object
                - '[]'
                - instruction
                - '[]'
                responses.setdefault(instruction, []).append:
                - (instruction, output)
                responses.items: []
                self.code_qa_dict.setdefault:
                - code_object
                - '{}'
                str(self.base_name).replace:
                - '''\\'''
                - '''/'''
                str:
                - self.base_name
                self.format_response: []
            method_variables:
            - self
            - responses
            - basename
            - excluded
            method_decorators: []
            method_annotations: []
            method_properties:
            - self.code_qa_dict
            - self.code_qa_list
        class_method_process_question:
            method_name: process_question
            method_code: "def process_question(self, question_id: str, query: str, context: str, info: dict) -> None:\n    \"\"\"Process question and add the generated response to the instruct_list.\"\"\"\n    if question_id.endswith(('code_graph', 'docstring')):\n        response = info.get(question_id, {})\n    elif question_id.endswith('file_purpose'):\n        self.get_code_qa()\n        response = self.get_response_from_llm(query, context) if self.use_llm else ''\n    else:\n        response = get_unique_elements(str(info.get(question_id, '')))\n    if response and response != 'None':\n        response = str(response).strip()\n        self.instruct_list.append({'instruction': query, 'input': context, 'output': response})"
            method_docstring: Process question and add the generated response to the instruct_list.
            method_inputs:
            - self
            - question_id
            - query
            - context
            - info
            method_defaults: []
            method_returns: []
            method_calls:
            - question_id.endswith
            - info.get
            - self.get_code_qa
            - self.get_response_from_llm
            - get_unique_elements
            - str
            - str(response).strip
            - self.instruct_list.append
            method_call_inputs:
                question_id.endswith:
                - ('code_graph', 'docstring')
                - '''file_purpose'''
                info.get:
                - question_id
                - '{}'
                - question_id
                - ''''''
                self.get_code_qa: []
                self.get_response_from_llm:
                - query
                - context
                get_unique_elements:
                - str(info.get(question_id, ''))
                str:
                - info.get(question_id, '')
                - response
                str(response).strip: []
                self.instruct_list.append:
                - '{''instruction'': query, ''input'': context, ''output'': response}'
            method_variables:
            - self
            - query
            - question_id
            - info
            - response
            - context
            method_decorators: []
            method_annotations: []
            method_properties: []
        class_method_get_info_string:
            method_name: get_info_string
            method_code: "@staticmethod\ndef get_info_string(info: dict, item_type: str) -> str:\n    \"\"\"Get string from info dictionary.\"\"\"\n    return ', '.join([item.strip() for item in str(info.get(item_type, '')).split(',') if item])"
            method_docstring: Get string from info dictionary.
            method_inputs:
            - info
            - item_type
            method_defaults: []
            method_returns:
            - ''', ''.join([item.strip() for item in str(info.get(item_type, '''')).split('','') if item])'
            method_calls:
            - ''', ''.join'
            - item.strip
            - str(info.get(item_type, '')).split
            - str
            - info.get
            method_call_inputs:
                ''', ''.join':
                - '[item.strip() for item in str(info.get(item_type, '''')).split('','') if item]'
                item.strip: []
                str(info.get(item_type, '')).split:
                - ''','''
                str:
                - info.get(item_type, '')
                info.get:
                - item_type
                - ''''''
            method_variables:
            - item_type
            - info
            method_decorators:
            - staticmethod
            method_annotations: []
            method_properties: []
        class_method_process_question_type:
            method_name: process_question_type
            method_code: "def process_question_type(self, question_type: str, question_id: str, question_text: str) -> None:\n    \"\"\"\n        Process questions related to a file, function, class, or method.\n        Args:\n            question_type (str): Type of question.\n            question_id (str): Question ID.\n            question_text (str): Question text.\n        \"\"\"\n    if question_type == 'file':\n        query = question_text.format(filename=self.base_name)\n        info = self.file_details['file_info']\n        context = f\"```python\\n{self.file_details['file_info']['file_code']}\\n```\"\n        self.process_question(question_id, query, context, info)\n    elif question_type == 'method':\n        for class_name, class_info in self.file_details['classes'].items():\n            for key, method_info in class_info.items():\n                if key.startswith('class_method_'):\n                    method_name = f\"{class_name}.{key[len('class_method_'):]}\"\n                    context = f\"```python\\n{method_info['method_code']}\\n```\"\n                    mapping = {'class_name': class_name, 'method_name': method_name}\n                    query = question_text.format(filename=self.base_name, **mapping)\n                    self.process_question(question_id, query, context, method_info)\n    else:\n        for name, info in self.file_details[self.question_mapping[question_type]].items():\n            context = f\"```python\\n{info[f'{question_type}_code']}\\n```\"\n            mapping = {f'{question_type}_name': name}\n            if question_id == f'{question_type}_purpose' and self.use_llm:\n                variables = self.get_info_string(info, f'{question_type}_variables')\n                inputs = self.get_info_string(info, f'{question_type}_inputs')\n                combined = ', '.join(filter(None, [variables, inputs]))\n                mapping[f'{question_type}_variables'] = get_unique_elements(combined)\n                if question_type == 'class':\n                    mapping[f'{question_type}_methods'] = self.get_info_string(info, f'{question_type}_methods')\n            query = question_text.format(filename=self.base_name, **mapping)\n            self.process_question(question_id, query, context, info)"
            method_docstring: "\n        Process questions related to a file, function, class, or method.\n        Args:\n            question_type (str): Type of question.\n            question_id (str): Question ID.\n            question_text (str): Question text.\n        "
            method_inputs:
            - self
            - question_type
            - question_id
            - question_text
            method_defaults: []
            method_returns: []
            method_calls:
            - question_text.format
            - self.process_question
            - self.file_details['classes'].items
            - class_info.items
            - key.startswith
            - len
            - self.file_details[self.question_mapping[question_type]].items
            - self.get_info_string
            - ''', ''.join'
            - filter
            - get_unique_elements
            method_call_inputs:
                question_text.format: []
                self.process_question:
                - question_id
                - query
                - context
                - info
                - question_id
                - query
                - context
                - method_info
                - question_id
                - query
                - context
                - info
                self.file_details['classes'].items: []
                class_info.items: []
                key.startswith:
                - '''class_method_'''
                len:
                - '''class_method_'''
                self.file_details[self.question_mapping[question_type]].items: []
                self.get_info_string:
                - info
                - f'{question_type}_variables'
                - info
                - f'{question_type}_inputs'
                - info
                - f'{question_type}_methods'
                ''', ''.join':
                - filter(None, [variables, inputs])
                filter:
                - None
                - '[variables, inputs]'
                get_unique_elements:
                - combined
            method_variables:
            - self
            - query
            - question_id
            - question_text
            - variables
            - info
            - method_name
            - context
            - mapping
            - question_type
            - inputs
            - combined
            method_decorators: []
            method_annotations: []
            method_properties: []
        class_method_generate:
            method_name: generate
            method_code: "def generate(self) -> tuple[list[dict], list[dict]]:\n    \"\"\"Generate responses for all the questions and returns the instruct_list.\"\"\"\n    for question in self.questions:\n        self.process_question_type(question['type'], question['id'], question['text'])\n    self.instruct_list.sort(key=lambda x: len(x['input']), reverse=True)\n    return self.instruct_list"
            method_docstring: Generate responses for all the questions and returns the instruct_list.
            method_inputs:
            - self
            method_defaults: []
            method_returns:
            - self.instruct_list
            method_calls:
            - self.process_question_type
            - self.instruct_list.sort
            - len
            method_call_inputs:
                self.process_question_type:
                - question['type']
                - question['id']
                - question['text']
                self.instruct_list.sort: []
                len:
                - x['input']
            method_variables:
            - self
            method_decorators: []
            method_annotations: []
            method_properties: []
