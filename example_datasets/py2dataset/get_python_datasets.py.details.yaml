file_info:
  file_code: "\"\"\"\nGenerates JSON format question-answer pairs and instructions\
    \ for a Python file\nRequirements:\n[req01] The `DatasetGenerator` class shall:\n\
    \        a. Accept Python file path, file details, base name, list of questions,\n\
    \        and model configuration as input during instantiation.\n        b. Initialize\
    \ and store Python file path, file details, base name,\n        question list,\
    \ llm, and use_llm flag as class attributes.\n        d. Provide `get_response_from_llm`\
    \ method to retrieve llm response.\n        e. Provide `process_question` method\
    \ to process each question, generate\n        corresponding responses, and add\
    \ to the instruct_list.\n        f. Provide `process_question_type` method to\
    \ process questions\n        related to the file, functions, classes, and methods.\n\
    \        g. Provide `generate` method to generate responses for all questions\n\
    \        and return the instruct_list.\n        h. Internally manage question\
    \ mapping to file details.\n[req02] The `get_python_datasets` function shall:\n\
    \        a. Accept a Python file path, file details, base name, questions list,\n\
    \        and model config as input.\n        b. Instantiate `DatasetGenerator`\
    \ class using the provided input.\n        c. Generate instruct_list using `DatasetGenerator`\
    \ `generate` method.\n        d. Return the generated `instruct_list`.\n[req03]\
    \ The `clean_and_get_unique_elements` function shall:\n        a. Clean an input\
    \ string (str) and return a string of unique elements.\n\"\"\"\nimport logging\n\
    import re\nimport math\nfrom typing import Dict, List, Tuple\n\n\ndef clean_and_get_unique_elements(input_str:\
    \ str) -> str:\n    \"\"\"\n    Clean an input string (str) and return a string\
    \ of unique elements.\n    Args:\n        input_str (str): The input string to\
    \ be cleaned.\n    Returns:\n        str: The cleaned string.\n    \"\"\"\n\n\
    \    def element_generator(input_str):\n        start, brace_level = 0, 0\n  \
    \      for i, char in enumerate(input_str):\n            if char in \"{}\":\n\
    \                brace_level += 1 if char == \"{\" else -1\n            elif char\
    \ == \",\" and brace_level == 0:\n                yield input_str[start:i]\n \
    \               start = i + 1\n        yield input_str[start:]\n\n    input_str\
    \ = input_str.strip(\"[]'\\\"\").strip()\n    cleaned_elements = [\n        element.strip(\"\
    '\\\" \").strip()\n        for element in element_generator(input_str)\n     \
    \   if element.strip()\n    ]\n    # add a ' ' around each element\n    cleaned_elements\
    \ = [f\"'{element}'\" for element in cleaned_elements]\n    returned_elements\
    \ = \", \".join(cleaned_elements)\n    return returned_elements\n\n\nclass DatasetGenerator:\n\
    \    \"\"\"\n    Generate JSON formatted dictionary outputs for a Python file.\n\
    \    Attributes:\n        file_path (str): The path to the Python file.\n    \
    \    file_details (Dict[str, Any]): Details of the Python file.\n        base_name\
    \ (str): The base name of the Python file.\n        questions (List[Dict[str,\
    \ str]]): Questions for generating responses.\n        instruct_list (List[Dict[str,\
    \ str]]): Storage for generated instructions.\n        question_mapping (Dict[str,\
    \ str]): Mapping of question types to keys in file details.\n        use_llm (bool):\
    \ Flag indicating if a language model should be used.\n        llm (object): The\
    \ language model for generating responses.\n        prompt (str): The prompt format\
    \ for querying the language model.\n    Methods:\n        get_response_from_llm(query:\
    \ str, context: str) -> str:\n            Get language model response to query\
    \ for given context.\n        process_question(question_type: str, question_id:\
    \ str, query: str, context: str, info: Dict) -> None:\n            Process question\
    \ and add the generated response to the instruct_list.\n        get_info_string(info,\
    \ item_type) -> str:\n            Get string from info dictionary.\n        process_question_type(question_type:\
    \ str, question_id: str, question_text: str) -> None:\n            Process questions\
    \ related to a file, function, class, or method.\n        generate() -> Tuple[List[Dict],\
    \ List[Dict]]:\n            Generate responses for all the questions and returns\
    \ the instruct_list.\n    \"\"\"\n\n    def __init__(\n        self,\n       \
    \ file_path: str,\n        file_details: Dict,\n        base_name: str,\n    \
    \    questions: List[Dict],\n        model_config: Dict,\n        detailed: bool,\n\
    \    ) -> None:\n        \"\"\"\n        Initialize the DatasetGenerator class.\n\
    \        Args:\n            file_path (str): The path to the Python file.\n  \
    \          file_details (Dict[str, Any]): Details of the Python file.\n      \
    \      base_name (str): The base name of the Python file.\n            questions\
    \ (List[Dict[str, str]]): Questions for generating responses.\n            model_config\
    \ (Dict): Configuration for the language model.\n        Returns:\n          \
    \  None\n        \"\"\"\n        self.file_path = file_path\n        self.file_details\
    \ = file_details\n        self.base_name = base_name\n        self.questions =\
    \ questions\n        self.model_config = model_config\n        if model_config\
    \ is not None:\n            self.llm = model_config[\"model\"]\n            self.use_llm\
    \ = True\n            self.detailed = detailed\n        else:\n            self.llm\
    \ = None\n            self.use_llm = False\n            self.detailed = False\n\
    \        self.instruct_list = []\n        self.question_mapping = {\n        \
    \    \"file\": \"file\",\n            \"function\": \"functions\",\n         \
    \   \"class\": \"classes\",\n            \"method\": \"classes\",\n        }\n\
    \        self.code_qa_list = []\n        self.code_qa_response = \"\"\n\n    def\
    \ get_response_from_llm(self, query: str, context: str) -> str:\n        \"\"\"\
    \n        Get language model response to query for given context.\n        Args:\n\
    \            query (str): The query to be used for generating the response.\n\
    \            context (str): The context to be used for generating the response.\n\
    \        Returns:\n            str: The generated response.\n        \"\"\"\n\
    \        context_strategies = [\n            lambda: \"{}\".format(str(context)),\n\
    \            lambda: \"```python\\n{}\\n```\".format(\n                str(self.file_details[\"\
    file_info\"][\"file_code_simplified\"])\n            ),\n            lambda: \"\
    ```python\\n{}\\n```\".format(\n                self.get_info_string(self.file_details[\"\
    file_info\"], \"file_summary\")\n            ),\n            lambda: \"\",\n \
    \       ]\n\n        max_context_length = self.model_config[\"inference_model\"\
    ][\"model_params\"][\n            \"context_length\"\n        ]\n        prompt_template\
    \ = self.model_config[\"prompt_template\"].format(\n            system_prompt=self.model_config[\"\
    system_prompt\"],\n            instruction_prompt=self.model_config[\"instruction_prompt\"\
    ],\n        )\n\n        for strategy in context_strategies:\n            context\
    \ = strategy()\n            prompt = prompt_template.format(\n               \
    \ context=context, query=query, code_objects=self.code_qa_response\n         \
    \   )\n            context_size = len(self.llm.tokenize(prompt))\n           \
    \ logging.info(\"***Context Size: \" + str(context_size))\n            if context_size\
    \ <= 0.70 * max_context_length:\n                break\n        else:\n      \
    \      err_msg = f\"Model response failed, increase py2dataset_model_config.yaml\
    \ context_length > {math.ceil(context_size/0.70)}\"\n            logging.error(err_msg)\n\
    \            return \"\"\n        if context == \"\":\n            context = self.code_qa_list\n\
    \n        response = \"\"\n        try:\n            response = re.sub(r\"\\n\\\
    s*\\n\", \"\\n\\n\", self.llm(prompt))\n            response = response.replace(\"\
    <|im_end|>\", \"\")\n            response = \"\\n\".join([line.lstrip() for line\
    \ in response.split(\"\\n\")])\n            logging.info(f\"***Overall Response:\
    \ {response}\")\n        except Exception as error:\n            logging.error(f\"\
    Failed to generate model response: {error}\")\n\n        if self.detailed:\n \
    \           for item in self.code_qa_list:\n                try:\n           \
    \         instruct_key = list(item.keys())[0]\n                    instruct_value\
    \ = list(item.values())[0]\n                    query = f\"Describe the Purpose\
    \ and Significance of these {instruct_key}: [{instruct_value}] and Explain what\
    \ each of these {instruct_key} does in the code.\"\n                    prompt\
    \ = (\n                        self.model_config[\"prompt_template\"]\n      \
    \                  .format(\n                            system_prompt=self.model_config[\"\
    system_prompt\"],\n                            instruction_prompt=self.model_config[\"\
    instruction_prompt\"],\n                        )\n                        .format(\n\
    \                            context=f\"{context}/nCode Summary:/n{response}\"\
    ,\n                            query=f\"{query}\", code_objects=f\"{instruct_value}\"\
    \n                        )\n                    )\n                    item_response\
    \ = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", self.llm(prompt))\n                  \
    \  item_response = item_response.replace(\"<|im_end|>\", \"\")\n             \
    \       logging.info(f\"\\n***Itemized Response: {query}\\n{item_response}\")\n\
    \                    for item in self.instruct_list:\n                       \
    \ if item[\"instruction\"].startswith(instruct_key):\n                       \
    \     output = f\"\\n\\nPurpose and Significance:\\n{item_response}\"\n      \
    \                      item[\"output\"] += output\n                          \
    \  break\n\n                except Exception as error:\n                    logging.error(f\"\
    Failed to generate model response: {error}\")\n\n        return response\n\n \
    \   def get_code_qa(self):\n        \"\"\"\n        Get code responses from the\
    \ instruct_list and update:\n            code_qa_list (List[Dict]): List of code\
    \ question-answer pairs.\n            code_qa_response (str): structured text\
    \ for code question-answer pairs.\n        \"\"\"\n        excluded_instructions\
    \ = {\"Call code graph\", \"Docstring\"}\n        self.code_qa_list = []\n   \
    \     code_objects_responses = {}\n\n        for item in self.instruct_list:\n\
    \            instruction = item[\"instruction\"].split(\" in Python file:\")[0]\n\
    \            output = item[\"output\"]\n            if any(instruction.startswith(prefix)\
    \ for prefix in excluded_instructions):\n                continue\n          \
    \  self.code_qa_list.append({instruction: output})\n            if \"`\" in instruction:\n\
    \                code_object = instruction.split(\"`\")[1]\n                code_type\
    \ = instruction.split()[0]\n                code_objects_responses.setdefault(code_object,\
    \ []).append(\n                    (code_type, output)\n                )\n\n\
    \        self.code_qa_response = \"\"\n        for idx, (code_object, type_responses)\
    \ in enumerate(\n            code_objects_responses.items(), start=1\n       \
    \ ):\n            self.code_qa_response += f\"{idx}) {code_object}:\\n\"\n   \
    \         for subidx, (code_type, response) in enumerate(type_responses, start=1):\n\
    \                self.code_qa_response += f\"{idx}.{subidx}. {code_type}: {response}\\\
    n\"\n\n    def process_question(\n        self, question_type: str, question_id:\
    \ str, query: str, context: str, info: Dict\n    ) -> None:\n        \"\"\"\n\
    \        Process question and add the generated response to the instruct_list.\n\
    \        Args:\n            question_type (str): The type of question to be processed.\n\
    \            question_id (str): The ID of the question to be processed.\n    \
    \        query (str): The query to be processed.\n            context (str): The\
    \ context to be used for generating the response.\n            info (Dict): The\
    \ information of the Python file.\n        Returns:\n            None\n      \
    \  \"\"\"\n        response = \"\"\n        if question_id.endswith((\"code_graph\"\
    , \"docstring\")):\n            response = info.get(question_id, {})\n       \
    \ elif question_id.endswith(\"file_purpose\"):  # file_purpose is last question\n\
    \            self.get_code_qa()\n            if self.use_llm:\n              \
    \  response = self.get_response_from_llm(query, context)\n        else:\n    \
    \        response = clean_and_get_unique_elements(str(info.get(question_id, \"\
    \")))\n        response = str(response).strip()\n\n        if response and response\
    \ != \"None\":\n            context = f\"```python\\n{context}\\n```\"\n     \
    \       self.instruct_list.append(\n                {\"instruction\": query, \"\
    input\": context, \"output\": response}\n            )\n\n    @staticmethod\n\
    \    def get_info_string(info: Dict, item_type: str) -> str:\n        \"\"\"\n\
    \        Get string from info dictionary.\n        Args:\n            info (Dict):\
    \ The information of the Python file.\n            item_type (str): The type of\
    \ item to get the string from.\n        Returns:\n            str: The string\
    \ from the info.\n        \"\"\"\n        return \", \".join(\n            [item.strip()\
    \ for item in str(info.get(item_type, \"\")).split(\",\") if item]\n        )\n\
    \n    def process_question_type(\n        self, question_type: str, question_id:\
    \ str, question_text: str\n    ) -> None:\n        \"\"\"\n        Process questions\
    \ related to a file, function, class, or method.\n        Args:\n            question_type\
    \ (str): The type of question to be processed.\n            question_id (str):\
    \ The ID of the question to be processed.\n            question_text (str): The\
    \ text of the question to be processed.\n        Returns:\n            None\n\
    \        \"\"\"\n        if question_type == \"file\":\n            query = question_text.format(filename=self.base_name)\n\
    \            info = self.file_details[\"file_info\"]\n            context = self.file_details[\"\
    file_info\"][\"file_code\"]\n            self.process_question(question_type,\
    \ question_id, query, context, info)\n        elif question_type == \"method\"\
    :\n            for class_name, class_info in self.file_details[\"classes\"].items():\n\
    \                for key, method_info in class_info.items():\n               \
    \     if key.startswith(\"class_method_\"):\n                        method_name\
    \ = f\"{class_name}.{key[len('class_method_'):]}\"\n                        context\
    \ = method_info[\"method_code\"]\n                        mapping = {\"class_name\"\
    : class_name, \"method_name\": method_name}\n                        query = question_text.format(filename=self.base_name,\
    \ **mapping)\n                        self.process_question(\n               \
    \             question_type, question_id, query, context, method_info\n      \
    \                  )\n        else:  # question_type is 'function' or 'class'\n\
    \            for name, info in self.file_details[\n                self.question_mapping[question_type]\n\
    \            ].items():\n                context = info[f\"{question_type}_code\"\
    ]\n                mapping = {f\"{question_type}_name\": name}\n             \
    \   if question_id == f\"{question_type}_purpose\" and self.use_llm:\n       \
    \             variables = self.get_info_string(info, f\"{question_type}_variables\"\
    )\n                    inputs = self.get_info_string(info, f\"{question_type}_inputs\"\
    )\n                    combined = \", \".join([s for s in [variables, inputs]\
    \ if s])\n                    mapping[\n                        f\"{question_type}_variables\"\
    \n                    ] = clean_and_get_unique_elements(combined)\n\n        \
    \            if question_type == \"class\":\n                        methods_string\
    \ = self.get_info_string(\n                            info, f\"{question_type}_methods\"\
    \n                        )\n                        mapping[f\"{question_type}_methods\"\
    ] = methods_string\n\n                query = question_text.format(filename=self.base_name,\
    \ **mapping)\n                self.process_question(question_type, question_id,\
    \ query, context, info)\n\n    def generate(self) -> Tuple[List[Dict], List[Dict]]:\n\
    \        \"\"\"\n        Generate responses for all the questions and returns\
    \ the instruct_list.\n        Args:\n            None\n        Returns:\n    \
    \        Tuple[List[Dict], List[Dict]]:  .\n        \"\"\"\n        for question\
    \ in self.questions:\n            self.process_question_type(\n              \
    \  question[\"type\"], question[\"id\"], question[\"text\"]\n            )\n \
    \       self.instruct_list.sort(key=lambda x: len(x[\"input\"]), reverse=True)\n\
    \        return self.instruct_list\n\n\ndef get_python_datasets(\n    file_path:\
    \ str,\n    file_details: Dict,\n    base_name: str,\n    questions: List[Dict],\n\
    \    model_config: Dict,\n    detailed: bool,\n) -> Tuple[List[Dict], List[Dict]]:\n\
    \    \"\"\"\n    Extract information from a Python file and return it in JSON\
    \ format.\n    Args:\n        file_path (str): The path to the Python file.\n\
    \        file_details (Dict): The details of the file.\n        base_name (str):\
    \ The base Python code filename.\n        questions (List[Dict]): The list of\
    \ questions.\n        llm (object): The language model to be used for generating\
    \ responses.\n        prompt (str): The prompt to be used for generating responses.\n\
    \        detailed (bool): Flag indicating if detailed information should be extracted.\n\
    \    Returns:\n        Tuple[List[Dict], List[Dict]]: Extracted information in\
    \ JSON format.\n    \"\"\"\n    return DatasetGenerator(\n        file_path, file_details,\
    \ base_name, questions, model_config, detailed\n    ).generate()\n"
  file_dependencies:
  - logging
  - typing
  - math
  - re
  file_functions:
  - clean_and_get_unique_elements
  - element_generator
  - get_python_datasets
  file_classes:
  - DatasetGenerator
  file_constants: []
  file_summary: '{dependencies: [logging, typing, math, re], function_defs: [{clean_and_get_unique_elements:
    {inputs: [input_str], calls: [enumerate, input_str.strip(''[]\\''\'').strip, input_str.strip,
    element.strip(''\\''\ '').strip, element.strip, element_generator, '', ''.join],
    call_inputs: {enumerate: [input_str], input_str.strip(''[]\\''\'').strip: [],
    input_str.strip: [''[]\\''\''], element.strip(''\\''\ '').strip: [], element.strip:
    [''\\''\ ''], element_generator: [input_str], '', ''.join: [cleaned_elements]},
    returns: [returned_elements]}}, {element_generator: {inputs: [input_str], calls:
    [enumerate], call_inputs: {enumerate: [input_str]}, returns: []}}, {get_python_datasets:
    {inputs: [file_path, file_details, base_name, questions, model_config, detailed],
    calls: [DatasetGenerator(file_path, file_details, base_name, questions, model_config,
    detailed).generate, DatasetGenerator], call_inputs: {DatasetGenerator(file_path,
    file_details, base_name, questions, model_config, detailed).generate: [], DatasetGenerator:
    [file_path, file_details, base_name, questions, model_config, detailed]}, returns:
    [DatasetGenerator(file_path, file_details, base_name, questions, model_config,
    detailed).generate()]}}], class_defs: [{DatasetGenerator: {method_defs: {__init__:
    {inputs: [self, file_path, file_details, base_name, questions, model_config, detailed],
    calls: [], call_inputs: {}, returns: []}, get_response_from_llm: {inputs: [self,
    query, context], calls: [''{}''.format, str, ''```python\\n{}\\n```''.format,
    self.get_info_string, self.model_config[''prompt_template''].format, strategy,
    prompt_template.format, len, self.llm.tokenize, logging.info, math.ceil, logging.error,
    re.sub, self.llm, response.replace, ''\\n''.join, line.lstrip, response.split,
    list, item.keys, item.values, self.model_config[''prompt_template''].format(system_prompt=self.model_config[''system_prompt''],
    instruction_prompt=self.model_config[''instruction_prompt'']).format, item_response.replace,
    item[''instruction''].startswith], call_inputs: {''{}''.format: [str(context)],
    str: [context, self.file_details[''file_info''][''file_code_simplified''], context_size],
    ''```python\\n{}\\n```''.format: [str(self.file_details[''file_info''][''file_code_simplified'']),
    self.get_info_string(self.file_details[''file_info''], ''file_summary'')], self.get_info_string:
    [self.file_details[''file_info''], ''file_summary''], self.model_config[''prompt_template''].format:
    [], strategy: [], prompt_template.format: [], len: [self.llm.tokenize(prompt)],
    self.llm.tokenize: [prompt], logging.info: [''***Context Size: '' + str(context_size),
    f''***Overall Response: {response}'', f''\\n***Itemized Response: {query}\\n{item_response}''],
    math.ceil: [context_size / 0.7], logging.error: [err_msg, f''Failed to generate
    model response: {error}'', f''Failed to generate model response: {error}''], re.sub:
    [''\\\\n\\\\s*\\\\n'', ''\\n\\n'', self.llm(prompt), ''\\\\n\\\\s*\\\\n'', ''\\n\\n'',
    self.llm(prompt)], self.llm: [prompt, prompt], response.replace: [''<|im_end|>'',
    ''''], ''\\n''.join: [[line.lstrip() for line in response.split(''\\n'')]], line.lstrip:
    [], response.split: [''\\n''], list: [item.keys(), item.values()], item.keys:
    [], item.values: [], self.model_config[''prompt_template''].format(system_prompt=self.model_config[''system_prompt''],
    instruction_prompt=self.model_config[''instruction_prompt'']).format: [], item_response.replace:
    [''<|im_end|>'', ''''], item[''instruction''].startswith: [instruct_key]}, returns:
    [response, '''']}, get_code_qa: {inputs: [self], calls: [item[''instruction''].split,
    any, instruction.startswith, self.code_qa_list.append, instruction.split, code_objects_responses.setdefault(code_object,
    []).append, code_objects_responses.setdefault, enumerate, code_objects_responses.items],
    call_inputs: {item[''instruction''].split: ['' in Python file:''], any: [(instruction.startswith(prefix)
    for prefix in excluded_instructions)], instruction.startswith: [prefix], self.code_qa_list.append:
    [{instruction: output}], instruction.split: [''`''], code_objects_responses.setdefault(code_object,
    []).append: [(code_type, output)], code_objects_responses.setdefault: [code_object,
    []], enumerate: [code_objects_responses.items(), type_responses], code_objects_responses.items:
    []}, returns: []}, process_question: {inputs: [self, question_type, question_id,
    query, context, info], calls: [question_id.endswith, info.get, self.get_code_qa,
    self.get_response_from_llm, clean_and_get_unique_elements, str, str(response).strip,
    self.instruct_list.append], call_inputs: {question_id.endswith: [(''code_graph'',
    ''docstring''), ''file_purpose''], info.get: [question_id, {}, question_id, ''''],
    self.get_code_qa: [], self.get_response_from_llm: [query, context], clean_and_get_unique_elements:
    [str(info.get(question_id, ''''))], str: [info.get(question_id, ''''), response],
    str(response).strip: [], self.instruct_list.append: [{''instruction'': query,
    ''input'': context, ''output'': response}]}, returns: []}, get_info_string: {inputs:
    [info, item_type], calls: ['', ''.join, item.strip, str(info.get(item_type, '''')).split,
    str, info.get], call_inputs: {'', ''.join: [[item.strip() for item in str(info.get(item_type,
    '''')).split('','') if item]], item.strip: [], str(info.get(item_type, '''')).split:
    ['',''], str: [info.get(item_type, '''')], info.get: [item_type, '''']}, returns:
    ['', ''.join([item.strip() for item in str(info.get(item_type, '''')).split('','')
    if item])]}, process_question_type: {inputs: [self, question_type, question_id,
    question_text], calls: [question_text.format, self.process_question, self.file_details[''classes''].items,
    class_info.items, key.startswith, len, self.file_details[self.question_mapping[question_type]].items,
    self.get_info_string, '', ''.join, clean_and_get_unique_elements], call_inputs:
    {question_text.format: [], self.process_question: [question_type, question_id,
    query, context, info, question_type, question_id, query, context, method_info,
    question_type, question_id, query, context, info], self.file_details[''classes''].items:
    [], class_info.items: [], key.startswith: [''class_method_''], len: [''class_method_''],
    self.file_details[self.question_mapping[question_type]].items: [], self.get_info_string:
    [info, f''{question_type}_variables'', info, f''{question_type}_inputs'', info,
    f''{question_type}_methods''], '', ''.join: [[s for s in [variables, inputs] if
    s]], clean_and_get_unique_elements: [combined]}, returns: []}, generate: {inputs:
    [self], calls: [self.process_question_type, self.instruct_list.sort, len], call_inputs:
    {self.process_question_type: [question[''type''], question[''id''], question[''text'']],
    self.instruct_list.sort: [], len: [x[''input'']]}, returns: [self.instruct_list]}}}}]}'
  file_code_simplified: "import logging\nimport re\nimport math\nfrom typing import\
    \ Dict, List, Tuple\n\ndef clean_and_get_unique_elements(input_str: str) -> str:\n\
    \n    def element_generator(input_str):\n        start, brace_level = (0, 0)\n\
    \        for i, char in enumerate(input_str):\n            if char in '{}':\n\
    \                brace_level += 1 if char == '{' else -1\n            elif char\
    \ == ',' and brace_level == 0:\n                yield input_str[start:i]\n   \
    \             start = i + 1\n        yield input_str[start:]\n    input_str =\
    \ input_str.strip('[]\\'\"').strip()\n    cleaned_elements = [element.strip('\\\
    '\" ').strip() for element in element_generator(input_str) if element.strip()]\n\
    \    cleaned_elements = [f\"'{element}'\" for element in cleaned_elements]\n \
    \   returned_elements = ', '.join(cleaned_elements)\n    return returned_elements\n\
    \nclass DatasetGenerator:\n\n    def __init__(self, file_path: str, file_details:\
    \ Dict, base_name: str, questions: List[Dict], model_config: Dict, detailed: bool)\
    \ -> None:\n        self.file_path = file_path\n        self.file_details = file_details\n\
    \        self.base_name = base_name\n        self.questions = questions\n    \
    \    self.model_config = model_config\n        if model_config is not None:\n\
    \            self.llm = model_config['model']\n            self.use_llm = True\n\
    \            self.detailed = detailed\n        else:\n            self.llm = None\n\
    \            self.use_llm = False\n            self.detailed = False\n       \
    \ self.instruct_list = []\n        self.question_mapping = {'file': 'file', 'function':\
    \ 'functions', 'class': 'classes', 'method': 'classes'}\n        self.code_qa_list\
    \ = []\n        self.code_qa_response = ''\n\n    def get_response_from_llm(self,\
    \ query: str, context: str) -> str:\n        context_strategies = [lambda: '{}'.format(str(context)),\
    \ lambda: '```python\\n{}\\n```'.format(str(self.file_details['file_info']['file_code_simplified'])),\
    \ lambda: '```python\\n{}\\n```'.format(self.get_info_string(self.file_details['file_info'],\
    \ 'file_summary')), lambda: '']\n        max_context_length = self.model_config['inference_model']['model_params']['context_length']\n\
    \        prompt_template = self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'],\
    \ instruction_prompt=self.model_config['instruction_prompt'])\n        for strategy\
    \ in context_strategies:\n            context = strategy()\n            prompt\
    \ = prompt_template.format(context=context, query=query, code_objects=self.code_qa_response)\n\
    \            context_size = len(self.llm.tokenize(prompt))\n            logging.info('***Context\
    \ Size: ' + str(context_size))\n            if context_size <= 0.7 * max_context_length:\n\
    \                break\n        else:\n            err_msg = f'Model response\
    \ failed, increase py2dataset_model_config.yaml context_length > {math.ceil(context_size\
    \ / 0.7)}'\n            logging.error(err_msg)\n            return ''\n      \
    \  if context == '':\n            context = self.code_qa_list\n        response\
    \ = ''\n        try:\n            response = re.sub('\\\\n\\\\s*\\\\n', '\\n\\\
    n', self.llm(prompt))\n            response = response.replace('<|im_end|>', '')\n\
    \            response = '\\n'.join([line.lstrip() for line in response.split('\\\
    n')])\n            logging.info(f'***Overall Response: {response}')\n        except\
    \ Exception as error:\n            logging.error(f'Failed to generate model response:\
    \ {error}')\n        if self.detailed:\n            for item in self.code_qa_list:\n\
    \                try:\n                    instruct_key = list(item.keys())[0]\n\
    \                    instruct_value = list(item.values())[0]\n               \
    \     query = f'Describe the Purpose and Significance of these {instruct_key}:\
    \ [{instruct_value}] and Explain what each of these {instruct_key} does in the\
    \ code.'\n                    prompt = self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'],\
    \ instruction_prompt=self.model_config['instruction_prompt']).format(context=f'{context}/nCode\
    \ Summary:/n{response}', query=f'{query}', code_objects=f'{instruct_value}')\n\
    \                    item_response = re.sub('\\\\n\\\\s*\\\\n', '\\n\\n', self.llm(prompt))\n\
    \                    item_response = item_response.replace('<|im_end|>', '')\n\
    \                    logging.info(f'\\n***Itemized Response: {query}\\n{item_response}')\n\
    \                    for item in self.instruct_list:\n                       \
    \ if item['instruction'].startswith(instruct_key):\n                         \
    \   output = f'\\n\\nPurpose and Significance:\\n{item_response}'\n          \
    \                  item['output'] += output\n                            break\n\
    \                except Exception as error:\n                    logging.error(f'Failed\
    \ to generate model response: {error}')\n        return response\n\n    def get_code_qa(self):\n\
    \        excluded_instructions = {'Call code graph', 'Docstring'}\n        self.code_qa_list\
    \ = []\n        code_objects_responses = {}\n        for item in self.instruct_list:\n\
    \            instruction = item['instruction'].split(' in Python file:')[0]\n\
    \            output = item['output']\n            if any((instruction.startswith(prefix)\
    \ for prefix in excluded_instructions)):\n                continue\n         \
    \   self.code_qa_list.append({instruction: output})\n            if '`' in instruction:\n\
    \                code_object = instruction.split('`')[1]\n                code_type\
    \ = instruction.split()[0]\n                code_objects_responses.setdefault(code_object,\
    \ []).append((code_type, output))\n        self.code_qa_response = ''\n      \
    \  for idx, (code_object, type_responses) in enumerate(code_objects_responses.items(),\
    \ start=1):\n            self.code_qa_response += f'{idx}) {code_object}:\\n'\n\
    \            for subidx, (code_type, response) in enumerate(type_responses, start=1):\n\
    \                self.code_qa_response += f'{idx}.{subidx}. {code_type}: {response}\\\
    n'\n\n    def process_question(self, question_type: str, question_id: str, query:\
    \ str, context: str, info: Dict) -> None:\n        response = ''\n        if question_id.endswith(('code_graph',\
    \ 'docstring')):\n            response = info.get(question_id, {})\n        elif\
    \ question_id.endswith('file_purpose'):\n            self.get_code_qa()\n    \
    \        if self.use_llm:\n                response = self.get_response_from_llm(query,\
    \ context)\n        else:\n            response = clean_and_get_unique_elements(str(info.get(question_id,\
    \ '')))\n        response = str(response).strip()\n        if response and response\
    \ != 'None':\n            context = f'```python\\n{context}\\n```'\n         \
    \   self.instruct_list.append({'instruction': query, 'input': context, 'output':\
    \ response})\n\n    @staticmethod\n    def get_info_string(info: Dict, item_type:\
    \ str) -> str:\n        return ', '.join([item.strip() for item in str(info.get(item_type,\
    \ '')).split(',') if item])\n\n    def process_question_type(self, question_type:\
    \ str, question_id: str, question_text: str) -> None:\n        if question_type\
    \ == 'file':\n            query = question_text.format(filename=self.base_name)\n\
    \            info = self.file_details['file_info']\n            context = self.file_details['file_info']['file_code']\n\
    \            self.process_question(question_type, question_id, query, context,\
    \ info)\n        elif question_type == 'method':\n            for class_name,\
    \ class_info in self.file_details['classes'].items():\n                for key,\
    \ method_info in class_info.items():\n                    if key.startswith('class_method_'):\n\
    \                        method_name = f\"{class_name}.{key[len('class_method_'):]}\"\
    \n                        context = method_info['method_code']\n             \
    \           mapping = {'class_name': class_name, 'method_name': method_name}\n\
    \                        query = question_text.format(filename=self.base_name,\
    \ **mapping)\n                        self.process_question(question_type, question_id,\
    \ query, context, method_info)\n        else:\n            for name, info in self.file_details[self.question_mapping[question_type]].items():\n\
    \                context = info[f'{question_type}_code']\n                mapping\
    \ = {f'{question_type}_name': name}\n                if question_id == f'{question_type}_purpose'\
    \ and self.use_llm:\n                    variables = self.get_info_string(info,\
    \ f'{question_type}_variables')\n                    inputs = self.get_info_string(info,\
    \ f'{question_type}_inputs')\n                    combined = ', '.join([s for\
    \ s in [variables, inputs] if s])\n                    mapping[f'{question_type}_variables']\
    \ = clean_and_get_unique_elements(combined)\n                    if question_type\
    \ == 'class':\n                        methods_string = self.get_info_string(info,\
    \ f'{question_type}_methods')\n                        mapping[f'{question_type}_methods']\
    \ = methods_string\n                query = question_text.format(filename=self.base_name,\
    \ **mapping)\n                self.process_question(question_type, question_id,\
    \ query, context, info)\n\n    def generate(self) -> Tuple[List[Dict], List[Dict]]:\n\
    \        for question in self.questions:\n            self.process_question_type(question['type'],\
    \ question['id'], question['text'])\n        self.instruct_list.sort(key=lambda\
    \ x: len(x['input']), reverse=True)\n        return self.instruct_list\n\ndef\
    \ get_python_datasets(file_path: str, file_details: Dict, base_name: str, questions:\
    \ List[Dict], model_config: Dict, detailed: bool) -> Tuple[List[Dict], List[Dict]]:\n\
    \    return DatasetGenerator(file_path, file_details, base_name, questions, model_config,\
    \ detailed).generate()"
  entire_code_graph:
    nodes:
    - DatasetGenerator
    - DatasetGenerator.__init__
    - DatasetGenerator.get_response_from_llm
    - DatasetGenerator.get_code_qa
    - DatasetGenerator.process_question
    - DatasetGenerator.get_info_string
    - DatasetGenerator.process_question_type
    - DatasetGenerator.generate
    - clean_and_get_unique_elements
    - element_generator
    - get_python_datasets
    - enumerate
    - input_str.strip('[]\'"').strip
    - input_str.strip
    - element.strip('\'" ').strip
    - element.strip
    - ''', ''.join'
    - DatasetGenerator(file_path, file_details, base_name, questions, model_config,
      detailed).generate
    - '''{}''.format'
    - str
    - '''```python\n{}\n```''.format'
    - self.model_config['prompt_template'].format
    - strategy
    - prompt_template.format
    - len
    - self.llm.tokenize
    - logging.info
    - math.ceil
    - logging.error
    - re.sub
    - self.llm
    - response.replace
    - '''\n''.join'
    - line.lstrip
    - response.split
    - list
    - item.keys
    - item.values
    - self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'],
      instruction_prompt=self.model_config['instruction_prompt']).format
    - item_response.replace
    - item['instruction'].startswith
    - item['instruction'].split
    - any
    - instruction.startswith
    - self.code_qa_list.append
    - instruction.split
    - code_objects_responses.setdefault(code_object, []).append
    - code_objects_responses.setdefault
    - code_objects_responses.items
    - question_id.endswith
    - info.get
    - str(response).strip
    - self.instruct_list.append
    - item.strip
    - str(info.get(item_type, '')).split
    - question_text.format
    - self.file_details['classes'].items
    - class_info.items
    - key.startswith
    - self.file_details[self.question_mapping[question_type]].items
    - self.instruct_list.sort
    edges:
    - source: DatasetGenerator
      target: DatasetGenerator.__init__
      target_inputs:
      - self
      - file_path
      - file_details
      - base_name
      - questions
      - model_config
      - detailed
      target_returns: []
    - source: DatasetGenerator
      target: DatasetGenerator.get_response_from_llm
      target_inputs:
      - self
      - query
      - context
      target_returns:
      - response
      - ''''''
    - source: DatasetGenerator
      target: DatasetGenerator.get_code_qa
      target_inputs:
      - self
      target_returns: []
    - source: DatasetGenerator
      target: DatasetGenerator.process_question
      target_inputs:
      - self
      - question_type
      - question_id
      - query
      - context
      - info
      target_returns: []
    - source: DatasetGenerator
      target: DatasetGenerator.get_info_string
      target_inputs:
      - info
      - item_type
      target_returns:
      - ''', ''.join([item.strip() for item in str(info.get(item_type, '''')).split('','')
        if item])'
    - source: DatasetGenerator
      target: DatasetGenerator.process_question_type
      target_inputs:
      - self
      - question_type
      - question_id
      - question_text
      target_returns: []
    - source: DatasetGenerator
      target: DatasetGenerator.generate
      target_inputs:
      - self
      target_returns:
      - self.instruct_list
    - source: DatasetGenerator.get_response_from_llm
      target: '''{}''.format'
      target_inputs:
      - str(context)
    - source: DatasetGenerator.get_response_from_llm
      target: str
      target_inputs:
      - context
      - self.file_details['file_info']['file_code_simplified']
      - context_size
    - source: DatasetGenerator.get_response_from_llm
      target: '''```python\n{}\n```''.format'
      target_inputs:
      - str(self.file_details['file_info']['file_code_simplified'])
      - self.get_info_string(self.file_details['file_info'], 'file_summary')
    - source: DatasetGenerator.get_response_from_llm
      target: DatasetGenerator.get_info_string
      target_inputs:
      - info
      - item_type
      target_returns:
      - ''', ''.join([item.strip() for item in str(info.get(item_type, '''')).split('','')
        if item])'
    - source: DatasetGenerator.get_response_from_llm
      target: self.model_config['prompt_template'].format
      target_inputs: []
    - source: DatasetGenerator.get_response_from_llm
      target: strategy
      target_inputs: []
    - source: DatasetGenerator.get_response_from_llm
      target: prompt_template.format
      target_inputs: []
    - source: DatasetGenerator.get_response_from_llm
      target: len
      target_inputs:
      - self.llm.tokenize(prompt)
    - source: DatasetGenerator.get_response_from_llm
      target: self.llm.tokenize
      target_inputs:
      - prompt
    - source: DatasetGenerator.get_response_from_llm
      target: logging.info
      target_inputs:
      - '''***Context Size: '' + str(context_size)'
      - 'f''***Overall Response: {response}'''
      - 'f''\n***Itemized Response: {query}\n{item_response}'''
    - source: DatasetGenerator.get_response_from_llm
      target: math.ceil
      target_inputs:
      - context_size / 0.7
    - source: DatasetGenerator.get_response_from_llm
      target: logging.error
      target_inputs:
      - err_msg
      - 'f''Failed to generate model response: {error}'''
      - 'f''Failed to generate model response: {error}'''
    - source: DatasetGenerator.get_response_from_llm
      target: re.sub
      target_inputs:
      - '''\\n\\s*\\n'''
      - '''\n\n'''
      - self.llm(prompt)
      - '''\\n\\s*\\n'''
      - '''\n\n'''
      - self.llm(prompt)
    - source: DatasetGenerator.get_response_from_llm
      target: self.llm
      target_inputs:
      - prompt
      - prompt
    - source: DatasetGenerator.get_response_from_llm
      target: response.replace
      target_inputs:
      - '''<|im_end|>'''
      - ''''''
    - source: DatasetGenerator.get_response_from_llm
      target: '''\n''.join'
      target_inputs:
      - '[line.lstrip() for line in response.split(''\n'')]'
    - source: DatasetGenerator.get_response_from_llm
      target: line.lstrip
      target_inputs: []
    - source: DatasetGenerator.get_response_from_llm
      target: response.split
      target_inputs:
      - '''\n'''
    - source: DatasetGenerator.get_response_from_llm
      target: list
      target_inputs:
      - item.keys()
      - item.values()
    - source: DatasetGenerator.get_response_from_llm
      target: item.keys
      target_inputs: []
    - source: DatasetGenerator.get_response_from_llm
      target: item.values
      target_inputs: []
    - source: DatasetGenerator.get_response_from_llm
      target: self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'],
        instruction_prompt=self.model_config['instruction_prompt']).format
      target_inputs: []
    - source: DatasetGenerator.get_response_from_llm
      target: item_response.replace
      target_inputs:
      - '''<|im_end|>'''
      - ''''''
    - source: DatasetGenerator.get_response_from_llm
      target: item['instruction'].startswith
      target_inputs:
      - instruct_key
    - source: DatasetGenerator.get_code_qa
      target: item['instruction'].split
      target_inputs:
      - ''' in Python file:'''
    - source: DatasetGenerator.get_code_qa
      target: any
      target_inputs:
      - (instruction.startswith(prefix) for prefix in excluded_instructions)
    - source: DatasetGenerator.get_code_qa
      target: instruction.startswith
      target_inputs:
      - prefix
    - source: DatasetGenerator.get_code_qa
      target: self.code_qa_list.append
      target_inputs:
      - '{instruction: output}'
    - source: DatasetGenerator.get_code_qa
      target: instruction.split
      target_inputs:
      - '''`'''
    - source: DatasetGenerator.get_code_qa
      target: code_objects_responses.setdefault(code_object, []).append
      target_inputs:
      - (code_type, output)
    - source: DatasetGenerator.get_code_qa
      target: code_objects_responses.setdefault
      target_inputs:
      - code_object
      - '[]'
    - source: DatasetGenerator.get_code_qa
      target: enumerate
      target_inputs:
      - code_objects_responses.items()
      - type_responses
    - source: DatasetGenerator.get_code_qa
      target: code_objects_responses.items
      target_inputs: []
    - source: DatasetGenerator.process_question
      target: question_id.endswith
      target_inputs:
      - ('code_graph', 'docstring')
      - '''file_purpose'''
    - source: DatasetGenerator.process_question
      target: info.get
      target_inputs:
      - question_id
      - '{}'
      - question_id
      - ''''''
    - source: DatasetGenerator.process_question
      target: DatasetGenerator.get_code_qa
      target_inputs:
      - self
      target_returns: []
    - source: DatasetGenerator.process_question
      target: DatasetGenerator.get_response_from_llm
      target_inputs:
      - self
      - query
      - context
      target_returns:
      - response
      - ''''''
    - source: DatasetGenerator.process_question
      target: clean_and_get_unique_elements
      target_inputs:
      - str(info.get(question_id, ''))
      target_returns:
      - returned_elements
    - source: DatasetGenerator.process_question
      target: str
      target_inputs:
      - info.get(question_id, '')
      - response
    - source: DatasetGenerator.process_question
      target: str(response).strip
      target_inputs: []
    - source: DatasetGenerator.process_question
      target: self.instruct_list.append
      target_inputs:
      - '{''instruction'': query, ''input'': context, ''output'': response}'
    - source: DatasetGenerator.get_info_string
      target: ''', ''.join'
      target_inputs:
      - '[item.strip() for item in str(info.get(item_type, '''')).split('','') if
        item]'
    - source: DatasetGenerator.get_info_string
      target: item.strip
      target_inputs: []
    - source: DatasetGenerator.get_info_string
      target: str(info.get(item_type, '')).split
      target_inputs:
      - ''','''
    - source: DatasetGenerator.get_info_string
      target: str
      target_inputs:
      - info.get(item_type, '')
    - source: DatasetGenerator.get_info_string
      target: info.get
      target_inputs:
      - item_type
      - ''''''
    - source: DatasetGenerator.process_question_type
      target: question_text.format
      target_inputs: []
    - source: DatasetGenerator.process_question_type
      target: DatasetGenerator.process_question
      target_inputs:
      - self
      - question_type
      - question_id
      - query
      - context
      - info
      target_returns: []
    - source: DatasetGenerator.process_question_type
      target: self.file_details['classes'].items
      target_inputs: []
    - source: DatasetGenerator.process_question_type
      target: class_info.items
      target_inputs: []
    - source: DatasetGenerator.process_question_type
      target: key.startswith
      target_inputs:
      - '''class_method_'''
    - source: DatasetGenerator.process_question_type
      target: len
      target_inputs:
      - '''class_method_'''
    - source: DatasetGenerator.process_question_type
      target: self.file_details[self.question_mapping[question_type]].items
      target_inputs: []
    - source: DatasetGenerator.process_question_type
      target: DatasetGenerator.get_info_string
      target_inputs:
      - info
      - item_type
      target_returns:
      - ''', ''.join([item.strip() for item in str(info.get(item_type, '''')).split('','')
        if item])'
    - source: DatasetGenerator.process_question_type
      target: ''', ''.join'
      target_inputs:
      - '[s for s in [variables, inputs] if s]'
    - source: DatasetGenerator.process_question_type
      target: clean_and_get_unique_elements
      target_inputs:
      - combined
      target_returns:
      - returned_elements
    - source: DatasetGenerator.generate
      target: DatasetGenerator.process_question_type
      target_inputs:
      - self
      - question_type
      - question_id
      - question_text
      target_returns: []
    - source: DatasetGenerator.generate
      target: self.instruct_list.sort
      target_inputs: []
    - source: DatasetGenerator.generate
      target: len
      target_inputs:
      - x['input']
    - source: clean_and_get_unique_elements
      target: enumerate
      target_inputs:
      - input_str
    - source: clean_and_get_unique_elements
      target: input_str.strip('[]\'"').strip
      target_inputs: []
    - source: clean_and_get_unique_elements
      target: input_str.strip
      target_inputs:
      - '''[]\''"'''
    - source: clean_and_get_unique_elements
      target: element.strip('\'" ').strip
      target_inputs: []
    - source: clean_and_get_unique_elements
      target: element.strip
      target_inputs:
      - '''\''" '''
    - source: clean_and_get_unique_elements
      target: element_generator
      target_inputs:
      - input_str
      target_returns: []
    - source: clean_and_get_unique_elements
      target: ''', ''.join'
      target_inputs:
      - cleaned_elements
    - source: element_generator
      target: enumerate
      target_inputs:
      - input_str
    - source: get_python_datasets
      target: DatasetGenerator(file_path, file_details, base_name, questions, model_config,
        detailed).generate
      target_inputs: []
    - source: get_python_datasets
      target: DatasetGenerator
      target_inputs:
      - file_path
      - file_details
      - base_name
      - questions
      - model_config
      - detailed
      target_returns: []
  control_flow_structure:
  - ? 'def get_python_datasets(file_path: str, file_details: Dict, base_name: str,
      questions: List[Dict], model_config: Dict, detailed: bool)'
    : - return:
        - DatasetGenerator(file_path, file_details, base_name, questions, model_config,
          detailed).generate()
  - import logging
  - import re
  - import math
  - from typing import Dict, List, Tuple
  - 'def clean_and_get_unique_elements(input_str: str)':
    - def element_generator(input_str):
      - start, brace_level = (0, 0)
      - for (i, char) in enumerate(input_str):
        - if char in '{}':
          - brace_level += 1 if char == '{' else -1
          elif char == ',' and brace_level == 0:
          - yield input_str[start:i]
          - start = i + 1
      - yield input_str[start:]
    - input_str = input_str.strip('[]\'"').strip()
    - cleaned_elements = [element.strip('\'" ').strip() for element in element_generator(input_str)
      if element.strip()]
    - cleaned_elements = [f"'{element}'" for element in cleaned_elements]
    - returned_elements = ', '.join(cleaned_elements)
    - return:
      - returned_elements
  - class DatasetGenerator:
    - ? 'def __init__(self, file_path: str, file_details: Dict, base_name: str, questions:
        List[Dict], model_config: Dict, detailed: bool)'
      : - self.file_path = file_path
        - self.file_details = file_details
        - self.base_name = base_name
        - self.questions = questions
        - self.model_config = model_config
        - if model_config is not None:
          - self.llm = model_config['model']
          - self.use_llm = True
          - self.detailed = detailed
          else:
          - self.llm = None
          - self.use_llm = False
          - self.detailed = False
        - self.instruct_list = []
        - 'self.question_mapping = {''file'': ''file'', ''function'': ''functions'',
          ''class'': ''classes'', ''method'': ''classes''}'
        - self.code_qa_list = []
        - self.code_qa_response = ''
    - 'def get_response_from_llm(self, query: str, context: str)':
      - 'context_strategies = [lambda: ''{}''.format(str(context)), lambda: ''```python\n{}\n```''.format(str(self.file_details[''file_info''][''file_code_simplified''])),
        lambda: ''```python\n{}\n```''.format(self.get_info_string(self.file_details[''file_info''],
        ''file_summary'')), lambda: '''']'
      - max_context_length = self.model_config['inference_model']['model_params']['context_length']
      - prompt_template = self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'],
        instruction_prompt=self.model_config['instruction_prompt'])
      - for strategy in context_strategies:
        - context = strategy()
        - prompt = prompt_template.format(context=context, query=query, code_objects=self.code_qa_response)
        - context_size = len(self.llm.tokenize(prompt))
        - 'logging.info(''***Context Size: '' + str(context_size))'
        - if context_size <= 0.7 * max_context_length:
          - break
      - if context == '':
        - context = self.code_qa_list
      - response = ''
      - try:
        - response = re.sub('\\n\\s*\\n', '\n\n', self.llm(prompt))
        - response = response.replace('<|im_end|>', '')
        - response = '\n'.join([line.lstrip() for line in response.split('\n')])
        - 'logging.info(f''***Overall Response: {response}'')'
        except:
        - 'except Exception as :':
          - 'logging.error(f''Failed to generate model response: {error}'')'
      - if self.detailed:
        - for item in self.code_qa_list:
          - try:
            - instruct_key = list(item.keys())[0]
            - instruct_value = list(item.values())[0]
            - 'query = f''Describe the Purpose and Significance of these {instruct_key}:
              [{instruct_value}] and Explain what each of these {instruct_key} does
              in the code.'''
            - prompt = self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'],
              instruction_prompt=self.model_config['instruction_prompt']).format(context=f'{context}/nCode
              Summary:/n{response}', query=f'{query}', code_objects=f'{instruct_value}')
            - item_response = re.sub('\\n\\s*\\n', '\n\n', self.llm(prompt))
            - item_response = item_response.replace('<|im_end|>', '')
            - 'logging.info(f''\n***Itemized Response: {query}\n{item_response}'')'
            - for item in self.instruct_list:
              - if item['instruction'].startswith(instruct_key):
                - output = f'\n\nPurpose and Significance:\n{item_response}'
                - item['output'] += output
                - break
            except:
            - 'except Exception as :':
              - 'logging.error(f''Failed to generate model response: {error}'')'
      - return:
        - response
    - def get_code_qa(self):
      - excluded_instructions = {'Call code graph', 'Docstring'}
      - self.code_qa_list = []
      - code_objects_responses = {}
      - for item in self.instruct_list:
        - instruction = item['instruction'].split(' in Python file:')[0]
        - output = item['output']
        - if any((instruction.startswith(prefix) for prefix in excluded_instructions)):
          - continue
        - 'self.code_qa_list.append({instruction: output})'
        - if '`' in instruction:
          - code_object = instruction.split('`')[1]
          - code_type = instruction.split()[0]
          - code_objects_responses.setdefault(code_object, []).append((code_type,
            output))
      - self.code_qa_response = ''
      - for (idx, (code_object, type_responses)) in enumerate(code_objects_responses.items(), start=1):
        - self.code_qa_response += f'{idx}) {code_object}:\n'
        - for (subidx, (code_type, response)) in enumerate(type_responses, start=1):
          - 'self.code_qa_response += f''{idx}.{subidx}. {code_type}: {response}\n'''
    - 'def process_question(self, question_type: str, question_id: str, query: str, context: str, info: Dict)':
      - response = ''
      - if question_id.endswith(('code_graph', 'docstring')):
        - response = info.get(question_id, {})
        elif question_id.endswith('file_purpose'):
        - self.get_code_qa()
        - if self.use_llm:
          - response = self.get_response_from_llm(query, context)
        else:
        - response = clean_and_get_unique_elements(str(info.get(question_id, '')))
      - response = str(response).strip()
      - if response and response != 'None':
        - context = f'```python\n{context}\n```'
        - 'self.instruct_list.append({''instruction'': query, ''input'': context,
          ''output'': response})'
    - 'def get_info_string(info: Dict, item_type: str)':
      - return:
        - ''', ''.join([item.strip() for item in str(info.get(item_type, '''')).split('','')
          if item])'
    - 'def process_question_type(self, question_type: str, question_id: str, question_text: str)':
      - if question_type == 'file':
        - query = question_text.format(filename=self.base_name)
        - info = self.file_details['file_info']
        - context = self.file_details['file_info']['file_code']
        - self.process_question(question_type, question_id, query, context, info)
        elif question_type == 'method':
        - for (class_name, class_info) in self.file_details['classes'].items():
          - for (key, method_info) in class_info.items():
            - if key.startswith('class_method_'):
              - method_name = f"{class_name}.{key[len('class_method_'):]}"
              - context = method_info['method_code']
              - 'mapping = {''class_name'': class_name, ''method_name'': method_name}'
              - query = question_text.format(filename=self.base_name, **mapping)
              - self.process_question(question_type, question_id, query, context,
                method_info)
        else:
        - for (name, info) in self.file_details[self.question_mapping[question_type]].items():
          - context = info[f'{question_type}_code']
          - 'mapping = {f''{question_type}_name'': name}'
          - if question_id == f'{question_type}_purpose' and self.use_llm:
            - variables = self.get_info_string(info, f'{question_type}_variables')
            - inputs = self.get_info_string(info, f'{question_type}_inputs')
            - combined = ', '.join([s for s in [variables, inputs] if s])
            - mapping[f'{question_type}_variables'] = clean_and_get_unique_elements(combined)
            - if question_type == 'class':
              - methods_string = self.get_info_string(info, f'{question_type}_methods')
              - mapping[f'{question_type}_methods'] = methods_string
          - query = question_text.format(filename=self.base_name, **mapping)
          - self.process_question(question_type, question_id, query, context, info)
    - def generate(self):
      - for question in self.questions:
        - self.process_question_type(question['type'], question['id'], question['text'])
      - 'self.instruct_list.sort(key=lambda x: len(x[''input'']), reverse=True)'
      - return:
        - self.instruct_list
  plant_uml: "@startuml\n  def [{'return': ['DatasetGenerator(file_path, file_details,\
    \ base_name, questions, model_config, detailed).generate()']}] {\n    :return;\n\
    \    :DatasetGenerator(file_path, file_details, base_name, questions, model_config,\
    \ detailed).generate();\n  }\n  :import logging;\n  :import re;\n  :import math;\n\
    \  :from typing import Dict, List, Tuple;\n  def [{'def element_generator(input_str)':\
    \ ['start, brace_level = (0, 0)', {'for (i, char) in enumerate(input_str)': [{\"\
    if char in '{}'\": [\"brace_level += 1 if char == '{' else -1\"], \"elif char\
    \ == ',' and brace_level == 0\": ['yield input_str[start:i]', 'start = i + 1']}]},\
    \ 'yield input_str[start:]']}, 'input_str = input_str.strip(\\'[]\\\\\\'\"\\').strip()',\
    \ 'cleaned_elements = [element.strip(\\'\\\\\\'\" \\').strip() for element in\
    \ element_generator(input_str) if element.strip()]', 'cleaned_elements = [f\"\\\
    '{element}\\'\" for element in cleaned_elements]', \"returned_elements = ', '.join(cleaned_elements)\"\
    , {'return': ['returned_elements']}] {\n    def ['start, brace_level = (0, 0)',\
    \ {'for (i, char) in enumerate(input_str)': [{\"if char in '{}'\": [\"brace_level\
    \ += 1 if char == '{' else -1\"], \"elif char == ',' and brace_level == 0\": ['yield\
    \ input_str[start:i]', 'start = i + 1']}]}, 'yield input_str[start:]'] {\n   \
    \   :start, brace_level = (0, 0);\n      while ([{\"if char in '{}'\": [\"brace_level\
    \ += 1 if char == '{' else -1\"], \"elif char == ',' and brace_level == 0\": ['yield\
    \ input_str[start:i]', 'start = i + 1']}]) {\n        if ([\"brace_level += 1\
    \ if char == '{' else -1\"]) {\n          :brace_level += 1 if char == '{' else\
    \ -1;\n        }\n      }\n      :yield input_str[start:];\n    }\n    :input_str\
    \ = input_str.strip('[]\\'\"').strip();\n    :cleaned_elements = [element.strip('\\\
    '\" ').strip() for element in element_generator(input_str) if element.strip()];\n\
    \    :cleaned_elements = [f\"'{element}'\" for element in cleaned_elements];\n\
    \    :returned_elements = ', '.join(cleaned_elements);\n    :return;\n    :returned_elements;\n\
    \  }\n  class [{'def __init__(self, file_path: str, file_details: Dict, base_name:\
    \ str, questions: List[Dict], model_config: Dict, detailed: bool)': ['self.file_path\
    \ = file_path', 'self.file_details = file_details', 'self.base_name = base_name',\
    \ 'self.questions = questions', 'self.model_config = model_config', {'if model_config\
    \ is not None': [\"self.llm = model_config['model']\", 'self.use_llm = True',\
    \ 'self.detailed = detailed'], 'else': ['self.llm = None', 'self.use_llm = False',\
    \ 'self.detailed = False']}, 'self.instruct_list = []', \"self.question_mapping\
    \ = {'file': 'file', 'function': 'functions', 'class': 'classes', 'method': 'classes'}\"\
    , 'self.code_qa_list = []', \"self.code_qa_response = ''\"]}, {'def get_response_from_llm(self,\
    \ query: str, context: str)': [\"context_strategies = [lambda: '{}'.format(str(context)),\
    \ lambda: '```python\\\\n{}\\\\n```'.format(str(self.file_details['file_info']['file_code_simplified'])),\
    \ lambda: '```python\\\\n{}\\\\n```'.format(self.get_info_string(self.file_details['file_info'],\
    \ 'file_summary')), lambda: '']\", \"max_context_length = self.model_config['inference_model']['model_params']['context_length']\"\
    , \"prompt_template = self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'],\
    \ instruction_prompt=self.model_config['instruction_prompt'])\", {'for strategy\
    \ in context_strategies': ['context = strategy()', 'prompt = prompt_template.format(context=context,\
    \ query=query, code_objects=self.code_qa_response)', 'context_size = len(self.llm.tokenize(prompt))',\
    \ \"logging.info('***Context Size: ' + str(context_size))\", {'if context_size\
    \ <= 0.7 * max_context_length': ['break']}]}, {\"if context == ''\": ['context\
    \ = self.code_qa_list']}, \"response = ''\", {'try': [\"response = re.sub('\\\\\
    \\\\n\\\\\\\\s*\\\\\\\\n', '\\\\n\\\\n', self.llm(prompt))\", \"response = response.replace('<|im_end|>',\
    \ '')\", \"response = '\\\\n'.join([line.lstrip() for line in response.split('\\\
    \\n')])\", \"logging.info(f'***Overall Response: {response}')\"], 'except': [{'except\
    \ Exception as :': [\"logging.error(f'Failed to generate model response: {error}')\"\
    ]}]}, {'if self.detailed': [{'for item in self.code_qa_list': [{'try': ['instruct_key\
    \ = list(item.keys())[0]', 'instruct_value = list(item.values())[0]', \"query\
    \ = f'Describe the Purpose and Significance of these {instruct_key}: [{instruct_value}]\
    \ and Explain what each of these {instruct_key} does in the code.'\", \"prompt\
    \ = self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'],\
    \ instruction_prompt=self.model_config['instruction_prompt']).format(context=f'{context}/nCode\
    \ Summary:/n{response}', query=f'{query}', code_objects=f'{instruct_value}')\"\
    , \"item_response = re.sub('\\\\\\\\n\\\\\\\\s*\\\\\\\\n', '\\\\n\\\\n', self.llm(prompt))\"\
    , \"item_response = item_response.replace('<|im_end|>', '')\", \"logging.info(f'\\\
    \\n***Itemized Response: {query}\\\\n{item_response}')\", {'for item in self.instruct_list':\
    \ [{\"if item['instruction'].startswith(instruct_key)\": [\"output = f'\\\\n\\\
    \\nPurpose and Significance:\\\\n{item_response}'\", \"item['output'] += output\"\
    , 'break']}]}], 'except': [{'except Exception as :': [\"logging.error(f'Failed\
    \ to generate model response: {error}')\"]}]}]}]}, {'return': ['response']}]},\
    \ {'def get_code_qa(self)': [\"excluded_instructions = {'Call code graph', 'Docstring'}\"\
    , 'self.code_qa_list = []', 'code_objects_responses = {}', {'for item in self.instruct_list':\
    \ [\"instruction = item['instruction'].split(' in Python file:')[0]\", \"output\
    \ = item['output']\", {'if any((instruction.startswith(prefix) for prefix in excluded_instructions))':\
    \ ['continue']}, 'self.code_qa_list.append({instruction: output})', {\"if '`'\
    \ in instruction\": [\"code_object = instruction.split('`')[1]\", 'code_type =\
    \ instruction.split()[0]', 'code_objects_responses.setdefault(code_object, []).append((code_type,\
    \ output))']}]}, \"self.code_qa_response = ''\", {'for (idx, (code_object, type_responses))\
    \ in enumerate(code_objects_responses.items(), start=1)': [\"self.code_qa_response\
    \ += f'{idx}) {code_object}:\\\\n'\", {'for (subidx, (code_type, response)) in\
    \ enumerate(type_responses, start=1)': [\"self.code_qa_response += f'{idx}.{subidx}.\
    \ {code_type}: {response}\\\\n'\"]}]}]}, {'def process_question(self, question_type:\
    \ str, question_id: str, query: str, context: str, info: Dict)': [\"response =\
    \ ''\", {\"if question_id.endswith(('code_graph', 'docstring'))\": ['response\
    \ = info.get(question_id, {})'], \"elif question_id.endswith('file_purpose')\"\
    : ['self.get_code_qa()', {'if self.use_llm': ['response = self.get_response_from_llm(query,\
    \ context)']}], 'else': [\"response = clean_and_get_unique_elements(str(info.get(question_id,\
    \ '')))\"]}, 'response = str(response).strip()', {\"if response and response !=\
    \ 'None'\": [\"context = f'```python\\\\n{context}\\\\n```'\", \"self.instruct_list.append({'instruction':\
    \ query, 'input': context, 'output': response})\"]}]}, {'def get_info_string(info:\
    \ Dict, item_type: str)': [{'return': [\"', '.join([item.strip() for item in str(info.get(item_type,\
    \ '')).split(',') if item])\"]}]}, {'def process_question_type(self, question_type:\
    \ str, question_id: str, question_text: str)': [{\"if question_type == 'file'\"\
    : ['query = question_text.format(filename=self.base_name)', \"info = self.file_details['file_info']\"\
    , \"context = self.file_details['file_info']['file_code']\", 'self.process_question(question_type,\
    \ question_id, query, context, info)'], \"elif question_type == 'method'\": [{\"\
    for (class_name, class_info) in self.file_details['classes'].items()\": [{'for\
    \ (key, method_info) in class_info.items()': [{\"if key.startswith('class_method_')\"\
    : ['method_name = f\"{class_name}.{key[len(\\'class_method_\\'):]}\"', \"context\
    \ = method_info['method_code']\", \"mapping = {'class_name': class_name, 'method_name':\
    \ method_name}\", 'query = question_text.format(filename=self.base_name, **mapping)',\
    \ 'self.process_question(question_type, question_id, query, context, method_info)']}]}]}],\
    \ 'else': [{'for (name, info) in self.file_details[self.question_mapping[question_type]].items()':\
    \ [\"context = info[f'{question_type}_code']\", \"mapping = {f'{question_type}_name':\
    \ name}\", {\"if question_id == f'{question_type}_purpose' and self.use_llm\"\
    : [\"variables = self.get_info_string(info, f'{question_type}_variables')\", \"\
    inputs = self.get_info_string(info, f'{question_type}_inputs')\", \"combined =\
    \ ', '.join([s for s in [variables, inputs] if s])\", \"mapping[f'{question_type}_variables']\
    \ = clean_and_get_unique_elements(combined)\", {\"if question_type == 'class'\"\
    : [\"methods_string = self.get_info_string(info, f'{question_type}_methods')\"\
    , \"mapping[f'{question_type}_methods'] = methods_string\"]}]}, 'query = question_text.format(filename=self.base_name,\
    \ **mapping)', 'self.process_question(question_type, question_id, query, context,\
    \ info)']}]}]}, {'def generate(self)': [{'for question in self.questions': [\"\
    self.process_question_type(question['type'], question['id'], question['text'])\"\
    ]}, \"self.instruct_list.sort(key=lambda x: len(x['input']), reverse=True)\",\
    \ {'return': ['self.instruct_list']}]}] {\n    def ['self.file_path = file_path',\
    \ 'self.file_details = file_details', 'self.base_name = base_name', 'self.questions\
    \ = questions', 'self.model_config = model_config', {'if model_config is not None':\
    \ [\"self.llm = model_config['model']\", 'self.use_llm = True', 'self.detailed\
    \ = detailed'], 'else': ['self.llm = None', 'self.use_llm = False', 'self.detailed\
    \ = False']}, 'self.instruct_list = []', \"self.question_mapping = {'file': 'file',\
    \ 'function': 'functions', 'class': 'classes', 'method': 'classes'}\", 'self.code_qa_list\
    \ = []', \"self.code_qa_response = ''\"] {\n      :self.file_path = file_path;\n\
    \      :self.file_details = file_details;\n      :self.base_name = base_name;\n\
    \      :self.questions = questions;\n      :self.model_config = model_config;\n\
    \      if ([\"self.llm = model_config['model']\", 'self.use_llm = True', 'self.detailed\
    \ = detailed']) {\n        :self.llm = model_config['model'];\n        :self.use_llm\
    \ = True;\n        :self.detailed = detailed;\n      }\n      :self.instruct_list\
    \ = [];\n      :self.question_mapping = {'file': 'file', 'function': 'functions',\
    \ 'class': 'classes', 'method': 'classes'};\n      :self.code_qa_list = [];\n\
    \      :self.code_qa_response = '';\n    }\n    def [\"context_strategies = [lambda:\
    \ '{}'.format(str(context)), lambda: '```python\\\\n{}\\\\n```'.format(str(self.file_details['file_info']['file_code_simplified'])),\
    \ lambda: '```python\\\\n{}\\\\n```'.format(self.get_info_string(self.file_details['file_info'],\
    \ 'file_summary')), lambda: '']\", \"max_context_length = self.model_config['inference_model']['model_params']['context_length']\"\
    , \"prompt_template = self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'],\
    \ instruction_prompt=self.model_config['instruction_prompt'])\", {'for strategy\
    \ in context_strategies': ['context = strategy()', 'prompt = prompt_template.format(context=context,\
    \ query=query, code_objects=self.code_qa_response)', 'context_size = len(self.llm.tokenize(prompt))',\
    \ \"logging.info('***Context Size: ' + str(context_size))\", {'if context_size\
    \ <= 0.7 * max_context_length': ['break']}]}, {\"if context == ''\": ['context\
    \ = self.code_qa_list']}, \"response = ''\", {'try': [\"response = re.sub('\\\\\
    \\\\n\\\\\\\\s*\\\\\\\\n', '\\\\n\\\\n', self.llm(prompt))\", \"response = response.replace('<|im_end|>',\
    \ '')\", \"response = '\\\\n'.join([line.lstrip() for line in response.split('\\\
    \\n')])\", \"logging.info(f'***Overall Response: {response}')\"], 'except': [{'except\
    \ Exception as :': [\"logging.error(f'Failed to generate model response: {error}')\"\
    ]}]}, {'if self.detailed': [{'for item in self.code_qa_list': [{'try': ['instruct_key\
    \ = list(item.keys())[0]', 'instruct_value = list(item.values())[0]', \"query\
    \ = f'Describe the Purpose and Significance of these {instruct_key}: [{instruct_value}]\
    \ and Explain what each of these {instruct_key} does in the code.'\", \"prompt\
    \ = self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'],\
    \ instruction_prompt=self.model_config['instruction_prompt']).format(context=f'{context}/nCode\
    \ Summary:/n{response}', query=f'{query}', code_objects=f'{instruct_value}')\"\
    , \"item_response = re.sub('\\\\\\\\n\\\\\\\\s*\\\\\\\\n', '\\\\n\\\\n', self.llm(prompt))\"\
    , \"item_response = item_response.replace('<|im_end|>', '')\", \"logging.info(f'\\\
    \\n***Itemized Response: {query}\\\\n{item_response}')\", {'for item in self.instruct_list':\
    \ [{\"if item['instruction'].startswith(instruct_key)\": [\"output = f'\\\\n\\\
    \\nPurpose and Significance:\\\\n{item_response}'\", \"item['output'] += output\"\
    , 'break']}]}], 'except': [{'except Exception as :': [\"logging.error(f'Failed\
    \ to generate model response: {error}')\"]}]}]}]}, {'return': ['response']}] {\n\
    \      :context_strategies = [lambda: '{}'.format(str(context)), lambda: '```python\\\
    n{}\\n```'.format(str(self.file_details['file_info']['file_code_simplified'])),\
    \ lambda: '```python\\n{}\\n```'.format(self.get_info_string(self.file_details['file_info'],\
    \ 'file_summary')), lambda: ''];\n      :max_context_length = self.model_config['inference_model']['model_params']['context_length'];\n\
    \      :prompt_template = self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'],\
    \ instruction_prompt=self.model_config['instruction_prompt']);\n      while (['context\
    \ = strategy()', 'prompt = prompt_template.format(context=context, query=query,\
    \ code_objects=self.code_qa_response)', 'context_size = len(self.llm.tokenize(prompt))',\
    \ \"logging.info('***Context Size: ' + str(context_size))\", {'if context_size\
    \ <= 0.7 * max_context_length': ['break']}]) {\n        :context = strategy();\n\
    \        :prompt = prompt_template.format(context=context, query=query, code_objects=self.code_qa_response);\n\
    \        :context_size = len(self.llm.tokenize(prompt));\n        :logging.info('***Context\
    \ Size: ' + str(context_size));\n        if (['break']) {\n          :break;\n\
    \        }\n      }\n      if (['context = self.code_qa_list']) {\n        :context\
    \ = self.code_qa_list;\n      }\n      :response = '';\n      :try;\n      :response\
    \ = re.sub('\\\\n\\\\s*\\\\n', '\\n\\n', self.llm(prompt));\n      :response =\
    \ response.replace('<|im_end|>', '');\n      :response = '\\n'.join([line.lstrip()\
    \ for line in response.split('\\n')]);\n      :logging.info(f'***Overall Response:\
    \ {response}');\n      if ([{'for item in self.code_qa_list': [{'try': ['instruct_key\
    \ = list(item.keys())[0]', 'instruct_value = list(item.values())[0]', \"query\
    \ = f'Describe the Purpose and Significance of these {instruct_key}: [{instruct_value}]\
    \ and Explain what each of these {instruct_key} does in the code.'\", \"prompt\
    \ = self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'],\
    \ instruction_prompt=self.model_config['instruction_prompt']).format(context=f'{context}/nCode\
    \ Summary:/n{response}', query=f'{query}', code_objects=f'{instruct_value}')\"\
    , \"item_response = re.sub('\\\\\\\\n\\\\\\\\s*\\\\\\\\n', '\\\\n\\\\n', self.llm(prompt))\"\
    , \"item_response = item_response.replace('<|im_end|>', '')\", \"logging.info(f'\\\
    \\n***Itemized Response: {query}\\\\n{item_response}')\", {'for item in self.instruct_list':\
    \ [{\"if item['instruction'].startswith(instruct_key)\": [\"output = f'\\\\n\\\
    \\nPurpose and Significance:\\\\n{item_response}'\", \"item['output'] += output\"\
    , 'break']}]}], 'except': [{'except Exception as :': [\"logging.error(f'Failed\
    \ to generate model response: {error}')\"]}]}]}]) {\n        while ([{'try': ['instruct_key\
    \ = list(item.keys())[0]', 'instruct_value = list(item.values())[0]', \"query\
    \ = f'Describe the Purpose and Significance of these {instruct_key}: [{instruct_value}]\
    \ and Explain what each of these {instruct_key} does in the code.'\", \"prompt\
    \ = self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'],\
    \ instruction_prompt=self.model_config['instruction_prompt']).format(context=f'{context}/nCode\
    \ Summary:/n{response}', query=f'{query}', code_objects=f'{instruct_value}')\"\
    , \"item_response = re.sub('\\\\\\\\n\\\\\\\\s*\\\\\\\\n', '\\\\n\\\\n', self.llm(prompt))\"\
    , \"item_response = item_response.replace('<|im_end|>', '')\", \"logging.info(f'\\\
    \\n***Itemized Response: {query}\\\\n{item_response}')\", {'for item in self.instruct_list':\
    \ [{\"if item['instruction'].startswith(instruct_key)\": [\"output = f'\\\\n\\\
    \\nPurpose and Significance:\\\\n{item_response}'\", \"item['output'] += output\"\
    , 'break']}]}], 'except': [{'except Exception as :': [\"logging.error(f'Failed\
    \ to generate model response: {error}')\"]}]}]) {\n          :try;\n         \
    \ :instruct_key = list(item.keys())[0];\n          :instruct_value = list(item.values())[0];\n\
    \          :query = f'Describe the Purpose and Significance of these {instruct_key}:\
    \ [{instruct_value}] and Explain what each of these {instruct_key} does in the\
    \ code.';\n          :prompt = self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'],\
    \ instruction_prompt=self.model_config['instruction_prompt']).format(context=f'{context}/nCode\
    \ Summary:/n{response}', query=f'{query}', code_objects=f'{instruct_value}');\n\
    \          :item_response = re.sub('\\\\n\\\\s*\\\\n', '\\n\\n', self.llm(prompt));\n\
    \          :item_response = item_response.replace('<|im_end|>', '');\n       \
    \   :logging.info(f'\\n***Itemized Response: {query}\\n{item_response}');\n  \
    \        while ([{\"if item['instruction'].startswith(instruct_key)\": [\"output\
    \ = f'\\\\n\\\\nPurpose and Significance:\\\\n{item_response}'\", \"item['output']\
    \ += output\", 'break']}]) {\n            if ([\"output = f'\\\\n\\\\nPurpose\
    \ and Significance:\\\\n{item_response}'\", \"item['output'] += output\", 'break'])\
    \ {\n              :output = f'\\n\\nPurpose and Significance:\\n{item_response}';\n\
    \              :item['output'] += output;\n              :break;\n           \
    \ }\n          }\n        }\n      }\n      :return;\n      :response;\n    }\n\
    \    def [\"excluded_instructions = {'Call code graph', 'Docstring'}\", 'self.code_qa_list\
    \ = []', 'code_objects_responses = {}', {'for item in self.instruct_list': [\"\
    instruction = item['instruction'].split(' in Python file:')[0]\", \"output = item['output']\"\
    , {'if any((instruction.startswith(prefix) for prefix in excluded_instructions))':\
    \ ['continue']}, 'self.code_qa_list.append({instruction: output})', {\"if '`'\
    \ in instruction\": [\"code_object = instruction.split('`')[1]\", 'code_type =\
    \ instruction.split()[0]', 'code_objects_responses.setdefault(code_object, []).append((code_type,\
    \ output))']}]}, \"self.code_qa_response = ''\", {'for (idx, (code_object, type_responses))\
    \ in enumerate(code_objects_responses.items(), start=1)': [\"self.code_qa_response\
    \ += f'{idx}) {code_object}:\\\\n'\", {'for (subidx, (code_type, response)) in\
    \ enumerate(type_responses, start=1)': [\"self.code_qa_response += f'{idx}.{subidx}.\
    \ {code_type}: {response}\\\\n'\"]}]}] {\n      :excluded_instructions = {'Call\
    \ code graph', 'Docstring'};\n      :self.code_qa_list = [];\n      :code_objects_responses\
    \ = {};\n      while ([\"instruction = item['instruction'].split(' in Python file:')[0]\"\
    , \"output = item['output']\", {'if any((instruction.startswith(prefix) for prefix\
    \ in excluded_instructions))': ['continue']}, 'self.code_qa_list.append({instruction:\
    \ output})', {\"if '`' in instruction\": [\"code_object = instruction.split('`')[1]\"\
    , 'code_type = instruction.split()[0]', 'code_objects_responses.setdefault(code_object,\
    \ []).append((code_type, output))']}]) {\n        :instruction = item['instruction'].split('\
    \ in Python file:')[0];\n        :output = item['output'];\n        if (['continue'])\
    \ {\n          :continue;\n        }\n        :self.code_qa_list.append({instruction:\
    \ output});\n        if ([\"code_object = instruction.split('`')[1]\", 'code_type\
    \ = instruction.split()[0]', 'code_objects_responses.setdefault(code_object, []).append((code_type,\
    \ output))']) {\n          :code_object = instruction.split('`')[1];\n       \
    \   :code_type = instruction.split()[0];\n          :code_objects_responses.setdefault(code_object,\
    \ []).append((code_type, output));\n        }\n      }\n      :self.code_qa_response\
    \ = '';\n      while ([\"self.code_qa_response += f'{idx}) {code_object}:\\\\\
    n'\", {'for (subidx, (code_type, response)) in enumerate(type_responses, start=1)':\
    \ [\"self.code_qa_response += f'{idx}.{subidx}. {code_type}: {response}\\\\n'\"\
    ]}]) {\n        :self.code_qa_response += f'{idx}) {code_object}:\\n';\n     \
    \   while ([\"self.code_qa_response += f'{idx}.{subidx}. {code_type}: {response}\\\
    \\n'\"]) {\n          :self.code_qa_response += f'{idx}.{subidx}. {code_type}:\
    \ {response}\\n';\n        }\n      }\n    }\n    def [\"response = ''\", {\"\
    if question_id.endswith(('code_graph', 'docstring'))\": ['response = info.get(question_id,\
    \ {})'], \"elif question_id.endswith('file_purpose')\": ['self.get_code_qa()',\
    \ {'if self.use_llm': ['response = self.get_response_from_llm(query, context)']}],\
    \ 'else': [\"response = clean_and_get_unique_elements(str(info.get(question_id,\
    \ '')))\"]}, 'response = str(response).strip()', {\"if response and response !=\
    \ 'None'\": [\"context = f'```python\\\\n{context}\\\\n```'\", \"self.instruct_list.append({'instruction':\
    \ query, 'input': context, 'output': response})\"]}] {\n      :response = '';\n\
    \      if (['response = info.get(question_id, {})']) {\n        :response = info.get(question_id,\
    \ {});\n      }\n      :response = str(response).strip();\n      if ([\"context\
    \ = f'```python\\\\n{context}\\\\n```'\", \"self.instruct_list.append({'instruction':\
    \ query, 'input': context, 'output': response})\"]) {\n        :context = f'```python\\\
    n{context}\\n```';\n        :self.instruct_list.append({'instruction': query,\
    \ 'input': context, 'output': response});\n      }\n    }\n    def [{'return':\
    \ [\"', '.join([item.strip() for item in str(info.get(item_type, '')).split(',')\
    \ if item])\"]}] {\n      :return;\n      :', '.join([item.strip() for item in\
    \ str(info.get(item_type, '')).split(',') if item]);\n    }\n    def [{\"if question_type\
    \ == 'file'\": ['query = question_text.format(filename=self.base_name)', \"info\
    \ = self.file_details['file_info']\", \"context = self.file_details['file_info']['file_code']\"\
    , 'self.process_question(question_type, question_id, query, context, info)'],\
    \ \"elif question_type == 'method'\": [{\"for (class_name, class_info) in self.file_details['classes'].items()\"\
    : [{'for (key, method_info) in class_info.items()': [{\"if key.startswith('class_method_')\"\
    : ['method_name = f\"{class_name}.{key[len(\\'class_method_\\'):]}\"', \"context\
    \ = method_info['method_code']\", \"mapping = {'class_name': class_name, 'method_name':\
    \ method_name}\", 'query = question_text.format(filename=self.base_name, **mapping)',\
    \ 'self.process_question(question_type, question_id, query, context, method_info)']}]}]}],\
    \ 'else': [{'for (name, info) in self.file_details[self.question_mapping[question_type]].items()':\
    \ [\"context = info[f'{question_type}_code']\", \"mapping = {f'{question_type}_name':\
    \ name}\", {\"if question_id == f'{question_type}_purpose' and self.use_llm\"\
    : [\"variables = self.get_info_string(info, f'{question_type}_variables')\", \"\
    inputs = self.get_info_string(info, f'{question_type}_inputs')\", \"combined =\
    \ ', '.join([s for s in [variables, inputs] if s])\", \"mapping[f'{question_type}_variables']\
    \ = clean_and_get_unique_elements(combined)\", {\"if question_type == 'class'\"\
    : [\"methods_string = self.get_info_string(info, f'{question_type}_methods')\"\
    , \"mapping[f'{question_type}_methods'] = methods_string\"]}]}, 'query = question_text.format(filename=self.base_name,\
    \ **mapping)', 'self.process_question(question_type, question_id, query, context,\
    \ info)']}]}] {\n      if (['query = question_text.format(filename=self.base_name)',\
    \ \"info = self.file_details['file_info']\", \"context = self.file_details['file_info']['file_code']\"\
    , 'self.process_question(question_type, question_id, query, context, info)'])\
    \ {\n        :query = question_text.format(filename=self.base_name);\n       \
    \ :info = self.file_details['file_info'];\n        :context = self.file_details['file_info']['file_code'];\n\
    \        :self.process_question(question_type, question_id, query, context, info);\n\
    \      }\n    }\n    def [{'for question in self.questions': [\"self.process_question_type(question['type'],\
    \ question['id'], question['text'])\"]}, \"self.instruct_list.sort(key=lambda\
    \ x: len(x['input']), reverse=True)\", {'return': ['self.instruct_list']}] {\n\
    \      while ([\"self.process_question_type(question['type'], question['id'],\
    \ question['text'])\"]) {\n        :self.process_question_type(question['type'],\
    \ question['id'], question['text']);\n      }\n      :self.instruct_list.sort(key=lambda\
    \ x: len(x['input']), reverse=True);\n      :return;\n      :self.instruct_list;\n\
    \    }\n  }\nend\n@enduml"
functions:
  clean_and_get_unique_elements:
    function_name: clean_and_get_unique_elements
    function_code: "def clean_and_get_unique_elements(input_str: str) -> str:\n  \
      \  \"\"\"\n    Clean an input string (str) and return a string of unique elements.\n\
      \    Args:\n        input_str (str): The input string to be cleaned.\n    Returns:\n\
      \        str: The cleaned string.\n    \"\"\"\n\n    def element_generator(input_str):\n\
      \        start, brace_level = (0, 0)\n        for i, char in enumerate(input_str):\n\
      \            if char in '{}':\n                brace_level += 1 if char == '{'\
      \ else -1\n            elif char == ',' and brace_level == 0:\n            \
      \    yield input_str[start:i]\n                start = i + 1\n        yield\
      \ input_str[start:]\n    input_str = input_str.strip('[]\\'\"').strip()\n  \
      \  cleaned_elements = [element.strip('\\'\" ').strip() for element in element_generator(input_str)\
      \ if element.strip()]\n    cleaned_elements = [f\"'{element}'\" for element\
      \ in cleaned_elements]\n    returned_elements = ', '.join(cleaned_elements)\n\
      \    return returned_elements"
    function_docstring: "\n    Clean an input string (str) and return a string of\
      \ unique elements.\n    Args:\n        input_str (str): The input string to\
      \ be cleaned.\n    Returns:\n        str: The cleaned string.\n    "
    function_inputs:
    - input_str
    function_defaults: []
    function_returns:
    - returned_elements
    function_calls:
    - enumerate
    - input_str.strip('[]\'"').strip
    - input_str.strip
    - element.strip('\'" ').strip
    - element.strip
    - element_generator
    - ''', ''.join'
    function_call_inputs:
      enumerate:
      - input_str
      input_str.strip('[]\'"').strip: []
      input_str.strip:
      - '''[]\''"'''
      element.strip('\'" ').strip: []
      element.strip:
      - '''\''" '''
      element_generator:
      - input_str
      ''', ''.join':
      - cleaned_elements
    function_variables:
    - input_str
    - cleaned_elements
    - start
    - returned_elements
    function_decorators: []
    function_annotations: []
    function_properties: []
  element_generator:
    function_name: element_generator
    function_code: "def element_generator(input_str):\n    start, brace_level = (0,\
      \ 0)\n    for i, char in enumerate(input_str):\n        if char in '{}':\n \
      \           brace_level += 1 if char == '{' else -1\n        elif char == ','\
      \ and brace_level == 0:\n            yield input_str[start:i]\n            start\
      \ = i + 1\n    yield input_str[start:]"
    function_docstring: null
    function_inputs:
    - input_str
    function_defaults: []
    function_returns: []
    function_calls:
    - enumerate
    function_call_inputs:
      enumerate:
      - input_str
    function_variables:
    - input_str
    - start
    function_decorators: []
    function_annotations: []
    function_properties: []
  get_python_datasets:
    function_name: get_python_datasets
    function_code: "def get_python_datasets(file_path: str, file_details: Dict, base_name:\
      \ str, questions: List[Dict], model_config: Dict, detailed: bool) -> Tuple[List[Dict],\
      \ List[Dict]]:\n    \"\"\"\n    Extract information from a Python file and return\
      \ it in JSON format.\n    Args:\n        file_path (str): The path to the Python\
      \ file.\n        file_details (Dict): The details of the file.\n        base_name\
      \ (str): The base Python code filename.\n        questions (List[Dict]): The\
      \ list of questions.\n        llm (object): The language model to be used for\
      \ generating responses.\n        prompt (str): The prompt to be used for generating\
      \ responses.\n        detailed (bool): Flag indicating if detailed information\
      \ should be extracted.\n    Returns:\n        Tuple[List[Dict], List[Dict]]:\
      \ Extracted information in JSON format.\n    \"\"\"\n    return DatasetGenerator(file_path,\
      \ file_details, base_name, questions, model_config, detailed).generate()"
    function_docstring: "\n    Extract information from a Python file and return it\
      \ in JSON format.\n    Args:\n        file_path (str): The path to the Python\
      \ file.\n        file_details (Dict): The details of the file.\n        base_name\
      \ (str): The base Python code filename.\n        questions (List[Dict]): The\
      \ list of questions.\n        llm (object): The language model to be used for\
      \ generating responses.\n        prompt (str): The prompt to be used for generating\
      \ responses.\n        detailed (bool): Flag indicating if detailed information\
      \ should be extracted.\n    Returns:\n        Tuple[List[Dict], List[Dict]]:\
      \ Extracted information in JSON format.\n    "
    function_inputs:
    - file_path
    - file_details
    - base_name
    - questions
    - model_config
    - detailed
    function_defaults: []
    function_returns:
    - DatasetGenerator(file_path, file_details, base_name, questions, model_config,
      detailed).generate()
    function_calls:
    - DatasetGenerator(file_path, file_details, base_name, questions, model_config,
      detailed).generate
    - DatasetGenerator
    function_call_inputs:
      DatasetGenerator(file_path, file_details, base_name, questions, model_config, detailed).generate: []
      DatasetGenerator:
      - file_path
      - file_details
      - base_name
      - questions
      - model_config
      - detailed
    function_variables:
    - model_config
    - questions
    - file_details
    - file_path
    - detailed
    - base_name
    function_decorators: []
    function_annotations: []
    function_properties: []
classes:
  DatasetGenerator:
    class_name: DatasetGenerator
    class_code: "class DatasetGenerator:\n    \"\"\"\n    Generate JSON formatted\
      \ dictionary outputs for a Python file.\n    Attributes:\n        file_path\
      \ (str): The path to the Python file.\n        file_details (Dict[str, Any]):\
      \ Details of the Python file.\n        base_name (str): The base name of the\
      \ Python file.\n        questions (List[Dict[str, str]]): Questions for generating\
      \ responses.\n        instruct_list (List[Dict[str, str]]): Storage for generated\
      \ instructions.\n        question_mapping (Dict[str, str]): Mapping of question\
      \ types to keys in file details.\n        use_llm (bool): Flag indicating if\
      \ a language model should be used.\n        llm (object): The language model\
      \ for generating responses.\n        prompt (str): The prompt format for querying\
      \ the language model.\n    Methods:\n        get_response_from_llm(query: str,\
      \ context: str) -> str:\n            Get language model response to query for\
      \ given context.\n        process_question(question_type: str, question_id:\
      \ str, query: str, context: str, info: Dict) -> None:\n            Process question\
      \ and add the generated response to the instruct_list.\n        get_info_string(info,\
      \ item_type) -> str:\n            Get string from info dictionary.\n       \
      \ process_question_type(question_type: str, question_id: str, question_text:\
      \ str) -> None:\n            Process questions related to a file, function,\
      \ class, or method.\n        generate() -> Tuple[List[Dict], List[Dict]]:\n\
      \            Generate responses for all the questions and returns the instruct_list.\n\
      \    \"\"\"\n\n    def __init__(self, file_path: str, file_details: Dict, base_name:\
      \ str, questions: List[Dict], model_config: Dict, detailed: bool) -> None:\n\
      \        \"\"\"\n        Initialize the DatasetGenerator class.\n        Args:\n\
      \            file_path (str): The path to the Python file.\n            file_details\
      \ (Dict[str, Any]): Details of the Python file.\n            base_name (str):\
      \ The base name of the Python file.\n            questions (List[Dict[str, str]]):\
      \ Questions for generating responses.\n            model_config (Dict): Configuration\
      \ for the language model.\n        Returns:\n            None\n        \"\"\"\
      \n        self.file_path = file_path\n        self.file_details = file_details\n\
      \        self.base_name = base_name\n        self.questions = questions\n  \
      \      self.model_config = model_config\n        if model_config is not None:\n\
      \            self.llm = model_config['model']\n            self.use_llm = True\n\
      \            self.detailed = detailed\n        else:\n            self.llm =\
      \ None\n            self.use_llm = False\n            self.detailed = False\n\
      \        self.instruct_list = []\n        self.question_mapping = {'file': 'file',\
      \ 'function': 'functions', 'class': 'classes', 'method': 'classes'}\n      \
      \  self.code_qa_list = []\n        self.code_qa_response = ''\n\n    def get_response_from_llm(self,\
      \ query: str, context: str) -> str:\n        \"\"\"\n        Get language model\
      \ response to query for given context.\n        Args:\n            query (str):\
      \ The query to be used for generating the response.\n            context (str):\
      \ The context to be used for generating the response.\n        Returns:\n  \
      \          str: The generated response.\n        \"\"\"\n        context_strategies\
      \ = [lambda: '{}'.format(str(context)), lambda: '```python\\n{}\\n```'.format(str(self.file_details['file_info']['file_code_simplified'])),\
      \ lambda: '```python\\n{}\\n```'.format(self.get_info_string(self.file_details['file_info'],\
      \ 'file_summary')), lambda: '']\n        max_context_length = self.model_config['inference_model']['model_params']['context_length']\n\
      \        prompt_template = self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'],\
      \ instruction_prompt=self.model_config['instruction_prompt'])\n        for strategy\
      \ in context_strategies:\n            context = strategy()\n            prompt\
      \ = prompt_template.format(context=context, query=query, code_objects=self.code_qa_response)\n\
      \            context_size = len(self.llm.tokenize(prompt))\n            logging.info('***Context\
      \ Size: ' + str(context_size))\n            if context_size <= 0.7 * max_context_length:\n\
      \                break\n        else:\n            err_msg = f'Model response\
      \ failed, increase py2dataset_model_config.yaml context_length > {math.ceil(context_size\
      \ / 0.7)}'\n            logging.error(err_msg)\n            return ''\n    \
      \    if context == '':\n            context = self.code_qa_list\n        response\
      \ = ''\n        try:\n            response = re.sub('\\\\n\\\\s*\\\\n', '\\\
      n\\n', self.llm(prompt))\n            response = response.replace('<|im_end|>',\
      \ '')\n            response = '\\n'.join([line.lstrip() for line in response.split('\\\
      n')])\n            logging.info(f'***Overall Response: {response}')\n      \
      \  except Exception as error:\n            logging.error(f'Failed to generate\
      \ model response: {error}')\n        if self.detailed:\n            for item\
      \ in self.code_qa_list:\n                try:\n                    instruct_key\
      \ = list(item.keys())[0]\n                    instruct_value = list(item.values())[0]\n\
      \                    query = f'Describe the Purpose and Significance of these\
      \ {instruct_key}: [{instruct_value}] and Explain what each of these {instruct_key}\
      \ does in the code.'\n                    prompt = self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'],\
      \ instruction_prompt=self.model_config['instruction_prompt']).format(context=f'{context}/nCode\
      \ Summary:/n{response}', query=f'{query}', code_objects=f'{instruct_value}')\n\
      \                    item_response = re.sub('\\\\n\\\\s*\\\\n', '\\n\\n', self.llm(prompt))\n\
      \                    item_response = item_response.replace('<|im_end|>', '')\n\
      \                    logging.info(f'\\n***Itemized Response: {query}\\n{item_response}')\n\
      \                    for item in self.instruct_list:\n                     \
      \   if item['instruction'].startswith(instruct_key):\n                     \
      \       output = f'\\n\\nPurpose and Significance:\\n{item_response}'\n    \
      \                        item['output'] += output\n                        \
      \    break\n                except Exception as error:\n                   \
      \ logging.error(f'Failed to generate model response: {error}')\n        return\
      \ response\n\n    def get_code_qa(self):\n        \"\"\"\n        Get code responses\
      \ from the instruct_list and update:\n            code_qa_list (List[Dict]):\
      \ List of code question-answer pairs.\n            code_qa_response (str): structured\
      \ text for code question-answer pairs.\n        \"\"\"\n        excluded_instructions\
      \ = {'Call code graph', 'Docstring'}\n        self.code_qa_list = []\n     \
      \   code_objects_responses = {}\n        for item in self.instruct_list:\n \
      \           instruction = item['instruction'].split(' in Python file:')[0]\n\
      \            output = item['output']\n            if any((instruction.startswith(prefix)\
      \ for prefix in excluded_instructions)):\n                continue\n       \
      \     self.code_qa_list.append({instruction: output})\n            if '`' in\
      \ instruction:\n                code_object = instruction.split('`')[1]\n  \
      \              code_type = instruction.split()[0]\n                code_objects_responses.setdefault(code_object,\
      \ []).append((code_type, output))\n        self.code_qa_response = ''\n    \
      \    for idx, (code_object, type_responses) in enumerate(code_objects_responses.items(),\
      \ start=1):\n            self.code_qa_response += f'{idx}) {code_object}:\\\
      n'\n            for subidx, (code_type, response) in enumerate(type_responses,\
      \ start=1):\n                self.code_qa_response += f'{idx}.{subidx}. {code_type}:\
      \ {response}\\n'\n\n    def process_question(self, question_type: str, question_id:\
      \ str, query: str, context: str, info: Dict) -> None:\n        \"\"\"\n    \
      \    Process question and add the generated response to the instruct_list.\n\
      \        Args:\n            question_type (str): The type of question to be\
      \ processed.\n            question_id (str): The ID of the question to be processed.\n\
      \            query (str): The query to be processed.\n            context (str):\
      \ The context to be used for generating the response.\n            info (Dict):\
      \ The information of the Python file.\n        Returns:\n            None\n\
      \        \"\"\"\n        response = ''\n        if question_id.endswith(('code_graph',\
      \ 'docstring')):\n            response = info.get(question_id, {})\n       \
      \ elif question_id.endswith('file_purpose'):\n            self.get_code_qa()\n\
      \            if self.use_llm:\n                response = self.get_response_from_llm(query,\
      \ context)\n        else:\n            response = clean_and_get_unique_elements(str(info.get(question_id,\
      \ '')))\n        response = str(response).strip()\n        if response and response\
      \ != 'None':\n            context = f'```python\\n{context}\\n```'\n       \
      \     self.instruct_list.append({'instruction': query, 'input': context, 'output':\
      \ response})\n\n    @staticmethod\n    def get_info_string(info: Dict, item_type:\
      \ str) -> str:\n        \"\"\"\n        Get string from info dictionary.\n \
      \       Args:\n            info (Dict): The information of the Python file.\n\
      \            item_type (str): The type of item to get the string from.\n   \
      \     Returns:\n            str: The string from the info.\n        \"\"\"\n\
      \        return ', '.join([item.strip() for item in str(info.get(item_type,\
      \ '')).split(',') if item])\n\n    def process_question_type(self, question_type:\
      \ str, question_id: str, question_text: str) -> None:\n        \"\"\"\n    \
      \    Process questions related to a file, function, class, or method.\n    \
      \    Args:\n            question_type (str): The type of question to be processed.\n\
      \            question_id (str): The ID of the question to be processed.\n  \
      \          question_text (str): The text of the question to be processed.\n\
      \        Returns:\n            None\n        \"\"\"\n        if question_type\
      \ == 'file':\n            query = question_text.format(filename=self.base_name)\n\
      \            info = self.file_details['file_info']\n            context = self.file_details['file_info']['file_code']\n\
      \            self.process_question(question_type, question_id, query, context,\
      \ info)\n        elif question_type == 'method':\n            for class_name,\
      \ class_info in self.file_details['classes'].items():\n                for key,\
      \ method_info in class_info.items():\n                    if key.startswith('class_method_'):\n\
      \                        method_name = f\"{class_name}.{key[len('class_method_'):]}\"\
      \n                        context = method_info['method_code']\n           \
      \             mapping = {'class_name': class_name, 'method_name': method_name}\n\
      \                        query = question_text.format(filename=self.base_name,\
      \ **mapping)\n                        self.process_question(question_type, question_id,\
      \ query, context, method_info)\n        else:\n            for name, info in\
      \ self.file_details[self.question_mapping[question_type]].items():\n       \
      \         context = info[f'{question_type}_code']\n                mapping =\
      \ {f'{question_type}_name': name}\n                if question_id == f'{question_type}_purpose'\
      \ and self.use_llm:\n                    variables = self.get_info_string(info,\
      \ f'{question_type}_variables')\n                    inputs = self.get_info_string(info,\
      \ f'{question_type}_inputs')\n                    combined = ', '.join([s for\
      \ s in [variables, inputs] if s])\n                    mapping[f'{question_type}_variables']\
      \ = clean_and_get_unique_elements(combined)\n                    if question_type\
      \ == 'class':\n                        methods_string = self.get_info_string(info,\
      \ f'{question_type}_methods')\n                        mapping[f'{question_type}_methods']\
      \ = methods_string\n                query = question_text.format(filename=self.base_name,\
      \ **mapping)\n                self.process_question(question_type, question_id,\
      \ query, context, info)\n\n    def generate(self) -> Tuple[List[Dict], List[Dict]]:\n\
      \        \"\"\"\n        Generate responses for all the questions and returns\
      \ the instruct_list.\n        Args:\n            None\n        Returns:\n  \
      \          Tuple[List[Dict], List[Dict]]:  .\n        \"\"\"\n        for question\
      \ in self.questions:\n            self.process_question_type(question['type'],\
      \ question['id'], question['text'])\n        self.instruct_list.sort(key=lambda\
      \ x: len(x['input']), reverse=True)\n        return self.instruct_list"
    class_docstring: "\n    Generate JSON formatted dictionary outputs for a Python\
      \ file.\n    Attributes:\n        file_path (str): The path to the Python file.\n\
      \        file_details (Dict[str, Any]): Details of the Python file.\n      \
      \  base_name (str): The base name of the Python file.\n        questions (List[Dict[str,\
      \ str]]): Questions for generating responses.\n        instruct_list (List[Dict[str,\
      \ str]]): Storage for generated instructions.\n        question_mapping (Dict[str,\
      \ str]): Mapping of question types to keys in file details.\n        use_llm\
      \ (bool): Flag indicating if a language model should be used.\n        llm (object):\
      \ The language model for generating responses.\n        prompt (str): The prompt\
      \ format for querying the language model.\n    Methods:\n        get_response_from_llm(query:\
      \ str, context: str) -> str:\n            Get language model response to query\
      \ for given context.\n        process_question(question_type: str, question_id:\
      \ str, query: str, context: str, info: Dict) -> None:\n            Process question\
      \ and add the generated response to the instruct_list.\n        get_info_string(info,\
      \ item_type) -> str:\n            Get string from info dictionary.\n       \
      \ process_question_type(question_type: str, question_id: str, question_text:\
      \ str) -> None:\n            Process questions related to a file, function,\
      \ class, or method.\n        generate() -> Tuple[List[Dict], List[Dict]]:\n\
      \            Generate responses for all the questions and returns the instruct_list.\n\
      \    "
    class_inputs: null
    class_defaults: null
    class_returns:
    - response
    - ''', ''.join([item.strip() for item in str(info.get(item_type, '''')).split('','')
      if item])'
    - self.instruct_list
    - ''''''
    class_calls:
    - '''{}''.format'
    - str
    - '''```python\n{}\n```''.format'
    - self.get_info_string
    - self.model_config['prompt_template'].format
    - strategy
    - prompt_template.format
    - len
    - self.llm.tokenize
    - logging.info
    - math.ceil
    - logging.error
    - re.sub
    - self.llm
    - response.replace
    - '''\n''.join'
    - line.lstrip
    - response.split
    - list
    - item.keys
    - item.values
    - self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'],
      instruction_prompt=self.model_config['instruction_prompt']).format
    - item_response.replace
    - item['instruction'].startswith
    - item['instruction'].split
    - any
    - instruction.startswith
    - self.code_qa_list.append
    - instruction.split
    - code_objects_responses.setdefault(code_object, []).append
    - code_objects_responses.setdefault
    - enumerate
    - code_objects_responses.items
    - question_id.endswith
    - info.get
    - self.get_code_qa
    - self.get_response_from_llm
    - clean_and_get_unique_elements
    - str(response).strip
    - self.instruct_list.append
    - ''', ''.join'
    - item.strip
    - str(info.get(item_type, '')).split
    - question_text.format
    - self.process_question
    - self.file_details['classes'].items
    - class_info.items
    - key.startswith
    - self.file_details[self.question_mapping[question_type]].items
    - self.process_question_type
    - self.instruct_list.sort
    class_call_inputs:
      '''{}''.format':
      - str(context)
      str:
      - context
      - self.file_details['file_info']['file_code_simplified']
      - context_size
      - info.get(question_id, '')
      - response
      - info.get(item_type, '')
      '''```python\n{}\n```''.format':
      - str(self.file_details['file_info']['file_code_simplified'])
      - self.get_info_string(self.file_details['file_info'], 'file_summary')
      self.get_info_string:
      - self.file_details['file_info']
      - '''file_summary'''
      - info
      - f'{question_type}_variables'
      - info
      - f'{question_type}_inputs'
      - info
      - f'{question_type}_methods'
      self.model_config['prompt_template'].format: []
      strategy: []
      prompt_template.format: []
      len:
      - self.llm.tokenize(prompt)
      - '''class_method_'''
      - x['input']
      self.llm.tokenize:
      - prompt
      logging.info:
      - '''***Context Size: '' + str(context_size)'
      - 'f''***Overall Response: {response}'''
      - 'f''\n***Itemized Response: {query}\n{item_response}'''
      math.ceil:
      - context_size / 0.7
      logging.error:
      - err_msg
      - 'f''Failed to generate model response: {error}'''
      - 'f''Failed to generate model response: {error}'''
      re.sub:
      - '''\\n\\s*\\n'''
      - '''\n\n'''
      - self.llm(prompt)
      - '''\\n\\s*\\n'''
      - '''\n\n'''
      - self.llm(prompt)
      self.llm:
      - prompt
      - prompt
      response.replace:
      - '''<|im_end|>'''
      - ''''''
      '''\n''.join':
      - '[line.lstrip() for line in response.split(''\n'')]'
      line.lstrip: []
      response.split:
      - '''\n'''
      list:
      - item.keys()
      - item.values()
      item.keys: []
      item.values: []
      ? self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'],
        instruction_prompt=self.model_config['instruction_prompt']).format
      : []
      item_response.replace:
      - '''<|im_end|>'''
      - ''''''
      item['instruction'].startswith:
      - instruct_key
      item['instruction'].split:
      - ''' in Python file:'''
      any:
      - (instruction.startswith(prefix) for prefix in excluded_instructions)
      instruction.startswith:
      - prefix
      self.code_qa_list.append:
      - '{instruction: output}'
      instruction.split:
      - '''`'''
      code_objects_responses.setdefault(code_object, []).append:
      - (code_type, output)
      code_objects_responses.setdefault:
      - code_object
      - '[]'
      enumerate:
      - code_objects_responses.items()
      - type_responses
      code_objects_responses.items: []
      question_id.endswith:
      - ('code_graph', 'docstring')
      - '''file_purpose'''
      info.get:
      - question_id
      - '{}'
      - question_id
      - ''''''
      - item_type
      - ''''''
      self.get_code_qa: []
      self.get_response_from_llm:
      - query
      - context
      clean_and_get_unique_elements:
      - str(info.get(question_id, ''))
      - combined
      str(response).strip: []
      self.instruct_list.append:
      - '{''instruction'': query, ''input'': context, ''output'': response}'
      ''', ''.join':
      - '[item.strip() for item in str(info.get(item_type, '''')).split('','') if
        item]'
      - '[s for s in [variables, inputs] if s]'
      item.strip: []
      str(info.get(item_type, '')).split:
      - ''','''
      question_text.format: []
      self.process_question:
      - question_type
      - question_id
      - query
      - context
      - info
      - question_type
      - question_id
      - query
      - context
      - method_info
      - question_type
      - question_id
      - query
      - context
      - info
      self.file_details['classes'].items: []
      class_info.items: []
      key.startswith:
      - '''class_method_'''
      self.file_details[self.question_mapping[question_type]].items: []
      self.process_question_type:
      - question['type']
      - question['id']
      - question['text']
      self.instruct_list.sort: []
    class_variables:
    - methods_string
    - prompt
    - item_response
    - context_strategies
    - combined
    - code_type
    - max_context_length
    - response
    - instruct_key
    - output
    - mapping
    - info
    - variables
    - context_size
    - inputs
    - code_objects_responses
    - query
    - code_object
    - instruct_value
    - method_name
    - context
    - prompt_template
    - err_msg
    - instruction
    - excluded_instructions
    class_decorators: []
    class_annotations: []
    class_properties:
    - self.model_config
    - self.questions
    - self.instruct_list
    - self.llm
    - self.file_path
    - self.use_llm
    - self.code_qa_list
    - self.base_name
    - self.detailed
    - self.file_details
    - self.question_mapping
    - self.code_qa_response
    class_attributes:
    - file_path
    - file_details
    - base_name
    - questions
    - model_config
    - instruct_list
    - question_mapping
    - code_qa_list
    - code_qa_response
    - llm
    - use_llm
    - detailed
    - llm
    - use_llm
    - detailed
    - code_qa_list
    - code_qa_response
    class_methods:
    - __init__
    - get_response_from_llm
    - get_code_qa
    - process_question
    - get_info_string
    - process_question_type
    - generate
    class_inheritance: []
    class_static_methods:
    - get_info_string
    class_method___init__:
      method_name: __init__
      method_code: "def __init__(self, file_path: str, file_details: Dict, base_name:\
        \ str, questions: List[Dict], model_config: Dict, detailed: bool) -> None:\n\
        \    \"\"\"\n        Initialize the DatasetGenerator class.\n        Args:\n\
        \            file_path (str): The path to the Python file.\n            file_details\
        \ (Dict[str, Any]): Details of the Python file.\n            base_name (str):\
        \ The base name of the Python file.\n            questions (List[Dict[str,\
        \ str]]): Questions for generating responses.\n            model_config (Dict):\
        \ Configuration for the language model.\n        Returns:\n            None\n\
        \        \"\"\"\n    self.file_path = file_path\n    self.file_details = file_details\n\
        \    self.base_name = base_name\n    self.questions = questions\n    self.model_config\
        \ = model_config\n    if model_config is not None:\n        self.llm = model_config['model']\n\
        \        self.use_llm = True\n        self.detailed = detailed\n    else:\n\
        \        self.llm = None\n        self.use_llm = False\n        self.detailed\
        \ = False\n    self.instruct_list = []\n    self.question_mapping = {'file':\
        \ 'file', 'function': 'functions', 'class': 'classes', 'method': 'classes'}\n\
        \    self.code_qa_list = []\n    self.code_qa_response = ''"
      method_docstring: "\n        Initialize the DatasetGenerator class.\n      \
        \  Args:\n            file_path (str): The path to the Python file.\n    \
        \        file_details (Dict[str, Any]): Details of the Python file.\n    \
        \        base_name (str): The base name of the Python file.\n            questions\
        \ (List[Dict[str, str]]): Questions for generating responses.\n          \
        \  model_config (Dict): Configuration for the language model.\n        Returns:\n\
        \            None\n        "
      method_inputs:
      - self
      - file_path
      - file_details
      - base_name
      - questions
      - model_config
      - detailed
      method_defaults: []
      method_returns: []
      method_calls: []
      method_call_inputs: {}
      method_variables:
      - self
      - model_config
      - questions
      - file_details
      - file_path
      - detailed
      - base_name
      method_decorators: []
      method_annotations: []
      method_properties:
      - self.model_config
      - self.questions
      - self.instruct_list
      - self.llm
      - self.file_path
      - self.use_llm
      - self.code_qa_list
      - self.base_name
      - self.detailed
      - self.file_details
      - self.question_mapping
      - self.code_qa_response
    class_method_get_response_from_llm:
      method_name: get_response_from_llm
      method_code: "def get_response_from_llm(self, query: str, context: str) -> str:\n\
        \    \"\"\"\n        Get language model response to query for given context.\n\
        \        Args:\n            query (str): The query to be used for generating\
        \ the response.\n            context (str): The context to be used for generating\
        \ the response.\n        Returns:\n            str: The generated response.\n\
        \        \"\"\"\n    context_strategies = [lambda: '{}'.format(str(context)),\
        \ lambda: '```python\\n{}\\n```'.format(str(self.file_details['file_info']['file_code_simplified'])),\
        \ lambda: '```python\\n{}\\n```'.format(self.get_info_string(self.file_details['file_info'],\
        \ 'file_summary')), lambda: '']\n    max_context_length = self.model_config['inference_model']['model_params']['context_length']\n\
        \    prompt_template = self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'],\
        \ instruction_prompt=self.model_config['instruction_prompt'])\n    for strategy\
        \ in context_strategies:\n        context = strategy()\n        prompt = prompt_template.format(context=context,\
        \ query=query, code_objects=self.code_qa_response)\n        context_size =\
        \ len(self.llm.tokenize(prompt))\n        logging.info('***Context Size: '\
        \ + str(context_size))\n        if context_size <= 0.7 * max_context_length:\n\
        \            break\n    else:\n        err_msg = f'Model response failed,\
        \ increase py2dataset_model_config.yaml context_length > {math.ceil(context_size\
        \ / 0.7)}'\n        logging.error(err_msg)\n        return ''\n    if context\
        \ == '':\n        context = self.code_qa_list\n    response = ''\n    try:\n\
        \        response = re.sub('\\\\n\\\\s*\\\\n', '\\n\\n', self.llm(prompt))\n\
        \        response = response.replace('<|im_end|>', '')\n        response =\
        \ '\\n'.join([line.lstrip() for line in response.split('\\n')])\n        logging.info(f'***Overall\
        \ Response: {response}')\n    except Exception as error:\n        logging.error(f'Failed\
        \ to generate model response: {error}')\n    if self.detailed:\n        for\
        \ item in self.code_qa_list:\n            try:\n                instruct_key\
        \ = list(item.keys())[0]\n                instruct_value = list(item.values())[0]\n\
        \                query = f'Describe the Purpose and Significance of these\
        \ {instruct_key}: [{instruct_value}] and Explain what each of these {instruct_key}\
        \ does in the code.'\n                prompt = self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'],\
        \ instruction_prompt=self.model_config['instruction_prompt']).format(context=f'{context}/nCode\
        \ Summary:/n{response}', query=f'{query}', code_objects=f'{instruct_value}')\n\
        \                item_response = re.sub('\\\\n\\\\s*\\\\n', '\\n\\n', self.llm(prompt))\n\
        \                item_response = item_response.replace('<|im_end|>', '')\n\
        \                logging.info(f'\\n***Itemized Response: {query}\\n{item_response}')\n\
        \                for item in self.instruct_list:\n                    if item['instruction'].startswith(instruct_key):\n\
        \                        output = f'\\n\\nPurpose and Significance:\\n{item_response}'\n\
        \                        item['output'] += output\n                      \
        \  break\n            except Exception as error:\n                logging.error(f'Failed\
        \ to generate model response: {error}')\n    return response"
      method_docstring: "\n        Get language model response to query for given\
        \ context.\n        Args:\n            query (str): The query to be used for\
        \ generating the response.\n            context (str): The context to be used\
        \ for generating the response.\n        Returns:\n            str: The generated\
        \ response.\n        "
      method_inputs:
      - self
      - query
      - context
      method_defaults: []
      method_returns:
      - response
      - ''''''
      method_calls:
      - '''{}''.format'
      - str
      - '''```python\n{}\n```''.format'
      - self.get_info_string
      - self.model_config['prompt_template'].format
      - strategy
      - prompt_template.format
      - len
      - self.llm.tokenize
      - logging.info
      - math.ceil
      - logging.error
      - re.sub
      - self.llm
      - response.replace
      - '''\n''.join'
      - line.lstrip
      - response.split
      - list
      - item.keys
      - item.values
      - self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'],
        instruction_prompt=self.model_config['instruction_prompt']).format
      - item_response.replace
      - item['instruction'].startswith
      method_call_inputs:
        '''{}''.format':
        - str(context)
        str:
        - context
        - self.file_details['file_info']['file_code_simplified']
        - context_size
        '''```python\n{}\n```''.format':
        - str(self.file_details['file_info']['file_code_simplified'])
        - self.get_info_string(self.file_details['file_info'], 'file_summary')
        self.get_info_string:
        - self.file_details['file_info']
        - '''file_summary'''
        self.model_config['prompt_template'].format: []
        strategy: []
        prompt_template.format: []
        len:
        - self.llm.tokenize(prompt)
        self.llm.tokenize:
        - prompt
        logging.info:
        - '''***Context Size: '' + str(context_size)'
        - 'f''***Overall Response: {response}'''
        - 'f''\n***Itemized Response: {query}\n{item_response}'''
        math.ceil:
        - context_size / 0.7
        logging.error:
        - err_msg
        - 'f''Failed to generate model response: {error}'''
        - 'f''Failed to generate model response: {error}'''
        re.sub:
        - '''\\n\\s*\\n'''
        - '''\n\n'''
        - self.llm(prompt)
        - '''\\n\\s*\\n'''
        - '''\n\n'''
        - self.llm(prompt)
        self.llm:
        - prompt
        - prompt
        response.replace:
        - '''<|im_end|>'''
        - ''''''
        '''\n''.join':
        - '[line.lstrip() for line in response.split(''\n'')]'
        line.lstrip: []
        response.split:
        - '''\n'''
        list:
        - item.keys()
        - item.values()
        item.keys: []
        item.values: []
        ? self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'],
          instruction_prompt=self.model_config['instruction_prompt']).format
        : []
        item_response.replace:
        - '''<|im_end|>'''
        - ''''''
        item['instruction'].startswith:
        - instruct_key
      method_variables:
      - self
      - query
      - instruct_value
      - response
      - context_strategies
      - context_size
      - context
      - max_context_length
      - prompt_template
      - err_msg
      - instruct_key
      - prompt
      - item_response
      - output
      method_decorators: []
      method_annotations: []
      method_properties: []
    class_method_get_code_qa:
      method_name: get_code_qa
      method_code: "def get_code_qa(self):\n    \"\"\"\n        Get code responses\
        \ from the instruct_list and update:\n            code_qa_list (List[Dict]):\
        \ List of code question-answer pairs.\n            code_qa_response (str):\
        \ structured text for code question-answer pairs.\n        \"\"\"\n    excluded_instructions\
        \ = {'Call code graph', 'Docstring'}\n    self.code_qa_list = []\n    code_objects_responses\
        \ = {}\n    for item in self.instruct_list:\n        instruction = item['instruction'].split('\
        \ in Python file:')[0]\n        output = item['output']\n        if any((instruction.startswith(prefix)\
        \ for prefix in excluded_instructions)):\n            continue\n        self.code_qa_list.append({instruction:\
        \ output})\n        if '`' in instruction:\n            code_object = instruction.split('`')[1]\n\
        \            code_type = instruction.split()[0]\n            code_objects_responses.setdefault(code_object,\
        \ []).append((code_type, output))\n    self.code_qa_response = ''\n    for\
        \ idx, (code_object, type_responses) in enumerate(code_objects_responses.items(),\
        \ start=1):\n        self.code_qa_response += f'{idx}) {code_object}:\\n'\n\
        \        for subidx, (code_type, response) in enumerate(type_responses, start=1):\n\
        \            self.code_qa_response += f'{idx}.{subidx}. {code_type}: {response}\\\
        n'"
      method_docstring: "\n        Get code responses from the instruct_list and update:\n\
        \            code_qa_list (List[Dict]): List of code question-answer pairs.\n\
        \            code_qa_response (str): structured text for code question-answer\
        \ pairs.\n        "
      method_inputs:
      - self
      method_defaults: []
      method_returns: []
      method_calls:
      - item['instruction'].split
      - any
      - instruction.startswith
      - self.code_qa_list.append
      - instruction.split
      - code_objects_responses.setdefault(code_object, []).append
      - code_objects_responses.setdefault
      - enumerate
      - code_objects_responses.items
      method_call_inputs:
        item['instruction'].split:
        - ''' in Python file:'''
        any:
        - (instruction.startswith(prefix) for prefix in excluded_instructions)
        instruction.startswith:
        - prefix
        self.code_qa_list.append:
        - '{instruction: output}'
        instruction.split:
        - '''`'''
        code_objects_responses.setdefault(code_object, []).append:
        - (code_type, output)
        code_objects_responses.setdefault:
        - code_object
        - '[]'
        enumerate:
        - code_objects_responses.items()
        - type_responses
        code_objects_responses.items: []
      method_variables:
      - self
      - code_object
      - code_type
      - instruction
      - excluded_instructions
      - code_objects_responses
      - output
      method_decorators: []
      method_annotations: []
      method_properties:
      - self.code_qa_response
      - self.code_qa_list
    class_method_process_question:
      method_name: process_question
      method_code: "def process_question(self, question_type: str, question_id: str,\
        \ query: str, context: str, info: Dict) -> None:\n    \"\"\"\n        Process\
        \ question and add the generated response to the instruct_list.\n        Args:\n\
        \            question_type (str): The type of question to be processed.\n\
        \            question_id (str): The ID of the question to be processed.\n\
        \            query (str): The query to be processed.\n            context\
        \ (str): The context to be used for generating the response.\n           \
        \ info (Dict): The information of the Python file.\n        Returns:\n   \
        \         None\n        \"\"\"\n    response = ''\n    if question_id.endswith(('code_graph',\
        \ 'docstring')):\n        response = info.get(question_id, {})\n    elif question_id.endswith('file_purpose'):\n\
        \        self.get_code_qa()\n        if self.use_llm:\n            response\
        \ = self.get_response_from_llm(query, context)\n    else:\n        response\
        \ = clean_and_get_unique_elements(str(info.get(question_id, '')))\n    response\
        \ = str(response).strip()\n    if response and response != 'None':\n     \
        \   context = f'```python\\n{context}\\n```'\n        self.instruct_list.append({'instruction':\
        \ query, 'input': context, 'output': response})"
      method_docstring: "\n        Process question and add the generated response\
        \ to the instruct_list.\n        Args:\n            question_type (str): The\
        \ type of question to be processed.\n            question_id (str): The ID\
        \ of the question to be processed.\n            query (str): The query to\
        \ be processed.\n            context (str): The context to be used for generating\
        \ the response.\n            info (Dict): The information of the Python file.\n\
        \        Returns:\n            None\n        "
      method_inputs:
      - self
      - question_type
      - question_id
      - query
      - context
      - info
      method_defaults: []
      method_returns: []
      method_calls:
      - question_id.endswith
      - info.get
      - self.get_code_qa
      - self.get_response_from_llm
      - clean_and_get_unique_elements
      - str
      - str(response).strip
      - self.instruct_list.append
      method_call_inputs:
        question_id.endswith:
        - ('code_graph', 'docstring')
        - '''file_purpose'''
        info.get:
        - question_id
        - '{}'
        - question_id
        - ''''''
        self.get_code_qa: []
        self.get_response_from_llm:
        - query
        - context
        clean_and_get_unique_elements:
        - str(info.get(question_id, ''))
        str:
        - info.get(question_id, '')
        - response
        str(response).strip: []
        self.instruct_list.append:
        - '{''instruction'': query, ''input'': context, ''output'': response}'
      method_variables:
      - self
      - info
      - query
      - context
      - question_type
      - response
      - question_id
      method_decorators: []
      method_annotations: []
      method_properties: []
    class_method_get_info_string:
      method_name: get_info_string
      method_code: "@staticmethod\ndef get_info_string(info: Dict, item_type: str)\
        \ -> str:\n    \"\"\"\n        Get string from info dictionary.\n        Args:\n\
        \            info (Dict): The information of the Python file.\n          \
        \  item_type (str): The type of item to get the string from.\n        Returns:\n\
        \            str: The string from the info.\n        \"\"\"\n    return ',\
        \ '.join([item.strip() for item in str(info.get(item_type, '')).split(',')\
        \ if item])"
      method_docstring: "\n        Get string from info dictionary.\n        Args:\n\
        \            info (Dict): The information of the Python file.\n          \
        \  item_type (str): The type of item to get the string from.\n        Returns:\n\
        \            str: The string from the info.\n        "
      method_inputs:
      - info
      - item_type
      method_defaults: []
      method_returns:
      - ''', ''.join([item.strip() for item in str(info.get(item_type, '''')).split('','')
        if item])'
      method_calls:
      - ''', ''.join'
      - item.strip
      - str(info.get(item_type, '')).split
      - str
      - info.get
      method_call_inputs:
        ''', ''.join':
        - '[item.strip() for item in str(info.get(item_type, '''')).split('','') if
          item]'
        item.strip: []
        str(info.get(item_type, '')).split:
        - ''','''
        str:
        - info.get(item_type, '')
        info.get:
        - item_type
        - ''''''
      method_variables:
      - info
      - item_type
      method_decorators:
      - staticmethod
      method_annotations: []
      method_properties: []
    class_method_process_question_type:
      method_name: process_question_type
      method_code: "def process_question_type(self, question_type: str, question_id:\
        \ str, question_text: str) -> None:\n    \"\"\"\n        Process questions\
        \ related to a file, function, class, or method.\n        Args:\n        \
        \    question_type (str): The type of question to be processed.\n        \
        \    question_id (str): The ID of the question to be processed.\n        \
        \    question_text (str): The text of the question to be processed.\n    \
        \    Returns:\n            None\n        \"\"\"\n    if question_type == 'file':\n\
        \        query = question_text.format(filename=self.base_name)\n        info\
        \ = self.file_details['file_info']\n        context = self.file_details['file_info']['file_code']\n\
        \        self.process_question(question_type, question_id, query, context,\
        \ info)\n    elif question_type == 'method':\n        for class_name, class_info\
        \ in self.file_details['classes'].items():\n            for key, method_info\
        \ in class_info.items():\n                if key.startswith('class_method_'):\n\
        \                    method_name = f\"{class_name}.{key[len('class_method_'):]}\"\
        \n                    context = method_info['method_code']\n             \
        \       mapping = {'class_name': class_name, 'method_name': method_name}\n\
        \                    query = question_text.format(filename=self.base_name,\
        \ **mapping)\n                    self.process_question(question_type, question_id,\
        \ query, context, method_info)\n    else:\n        for name, info in self.file_details[self.question_mapping[question_type]].items():\n\
        \            context = info[f'{question_type}_code']\n            mapping\
        \ = {f'{question_type}_name': name}\n            if question_id == f'{question_type}_purpose'\
        \ and self.use_llm:\n                variables = self.get_info_string(info,\
        \ f'{question_type}_variables')\n                inputs = self.get_info_string(info,\
        \ f'{question_type}_inputs')\n                combined = ', '.join([s for\
        \ s in [variables, inputs] if s])\n                mapping[f'{question_type}_variables']\
        \ = clean_and_get_unique_elements(combined)\n                if question_type\
        \ == 'class':\n                    methods_string = self.get_info_string(info,\
        \ f'{question_type}_methods')\n                    mapping[f'{question_type}_methods']\
        \ = methods_string\n            query = question_text.format(filename=self.base_name,\
        \ **mapping)\n            self.process_question(question_type, question_id,\
        \ query, context, info)"
      method_docstring: "\n        Process questions related to a file, function,\
        \ class, or method.\n        Args:\n            question_type (str): The type\
        \ of question to be processed.\n            question_id (str): The ID of the\
        \ question to be processed.\n            question_text (str): The text of\
        \ the question to be processed.\n        Returns:\n            None\n    \
        \    "
      method_inputs:
      - self
      - question_type
      - question_id
      - question_text
      method_defaults: []
      method_returns: []
      method_calls:
      - question_text.format
      - self.process_question
      - self.file_details['classes'].items
      - class_info.items
      - key.startswith
      - len
      - self.file_details[self.question_mapping[question_type]].items
      - self.get_info_string
      - ''', ''.join'
      - clean_and_get_unique_elements
      method_call_inputs:
        question_text.format: []
        self.process_question:
        - question_type
        - question_id
        - query
        - context
        - info
        - question_type
        - question_id
        - query
        - context
        - method_info
        - question_type
        - question_id
        - query
        - context
        - info
        self.file_details['classes'].items: []
        class_info.items: []
        key.startswith:
        - '''class_method_'''
        len:
        - '''class_method_'''
        self.file_details[self.question_mapping[question_type]].items: []
        self.get_info_string:
        - info
        - f'{question_type}_variables'
        - info
        - f'{question_type}_inputs'
        - info
        - f'{question_type}_methods'
        ''', ''.join':
        - '[s for s in [variables, inputs] if s]'
        clean_and_get_unique_elements:
        - combined
      method_variables:
      - self
      - info
      - variables
      - query
      - method_name
      - mapping
      - context
      - question_type
      - question_text
      - inputs
      - methods_string
      - question_id
      - combined
      method_decorators: []
      method_annotations: []
      method_properties: []
    class_method_generate:
      method_name: generate
      method_code: "def generate(self) -> Tuple[List[Dict], List[Dict]]:\n    \"\"\
        \"\n        Generate responses for all the questions and returns the instruct_list.\n\
        \        Args:\n            None\n        Returns:\n            Tuple[List[Dict],\
        \ List[Dict]]:  .\n        \"\"\"\n    for question in self.questions:\n \
        \       self.process_question_type(question['type'], question['id'], question['text'])\n\
        \    self.instruct_list.sort(key=lambda x: len(x['input']), reverse=True)\n\
        \    return self.instruct_list"
      method_docstring: "\n        Generate responses for all the questions and returns\
        \ the instruct_list.\n        Args:\n            None\n        Returns:\n\
        \            Tuple[List[Dict], List[Dict]]:  .\n        "
      method_inputs:
      - self
      method_defaults: []
      method_returns:
      - self.instruct_list
      method_calls:
      - self.process_question_type
      - self.instruct_list.sort
      - len
      method_call_inputs:
        self.process_question_type:
        - question['type']
        - question['id']
        - question['text']
        self.instruct_list.sort: []
        len:
        - x['input']
      method_variables:
      - self
      method_decorators: []
      method_annotations: []
      method_properties: []
