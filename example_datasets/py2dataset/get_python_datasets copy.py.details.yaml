file_info:
    file_code: "\"\"\"\nGenerates JSON format question-answer pairs and instructions for a Python file\nRequirements:\n[req01] The `DatasetGenerator` class shall:\n        a. Accept Python file path, file details, base name, list of questions,\n        and model configuration as input during instantiation.\n        b. Initialize and store Python file path, file details, base name,\n        question list, llm, and use_llm flag as class attributes.\n        d. Provide `get_response_from_llm` method to retrieve llm response.\n        e. Provide `process_question` method to process each question, generate\n        corresponding responses, and add to the instruct_list.\n        f. Provide `process_question_type` method to process questions\n        related to the file, functions, classes, and methods.\n        g. Provide `generate` method to generate responses for all questions\n        and return the instruct_list.\n        h. Internally manage question mapping to file details.\n[req02] The `get_python_datasets` function shall:\n        a. Accept a Python file path, file details, base name, questions list,\n        and model config as input.\n        b. Instantiate `DatasetGenerator` class using the provided input.\n        c. Generate instruct_list using `DatasetGenerator` `generate` method.\n        d. Return the generated `instruct_list`.\n[req03] The `clean_and_get_unique_elements` function shall:\n        a. Clean an input string (str) and return a string of unique elements.\n\"\"\"\n\nimport logging\nimport re\nimport math\nfrom typing import Dict, List, Tuple\nimport yaml\n\n\ndef clean_and_get_unique_elements(input_str: str) -> str:\n    \"\"\"\n    Clean an input string (str) and return a string of unique elements.\n    Args:\n        input_str (str): The input string to be cleaned.\n    Returns:\n        str: The cleaned string.\n    \"\"\"\n\n    def element_generator(input_str):\n        start, brace_level = 0, 0\n        for i, char in enumerate(input_str):\n            if char in \"{}\":\n                brace_level += 1 if char == \"{\" else -1\n            elif char == \",\" and brace_level == 0:\n                yield input_str[start:i]\n                start = i + 1\n        yield input_str[start:]\n\n    input_str = input_str.strip(\"[]'\\\"\").strip()\n    cleaned_elements = [\n        element.strip(\"'\\\" \").strip()\n        for element in element_generator(input_str)\n        if element.strip()\n    ]\n    # Modify to not add single quotes around each element\n    returned_elements = \", \".join(cleaned_elements)\n    return returned_elements\n\n\nclass DatasetGenerator:\n    \"\"\"\n    Generate JSON formatted dictionary outputs for a Python file.\n    Attributes:\n        file_path (str): The path to the Python file.\n        file_details (Dict[str, Any]): Details of the Python file.\n        base_name (str): The base name of the Python file.\n        questions (List[Dict[str, str]]): Questions for generating responses.\n        instruct_list (List[Dict[str, str]]): Storage for generated instructions.\n        question_mapping (Dict[str, str]): Mapping of question types to keys in file details.\n        use_llm (bool): Flag indicating if a language model should be used.\n        llm (object): The language model for generating responses.\n        prompt (str): The prompt format for querying the language model.\n    Methods:\n        get_response_from_llm(query: str, context: str) -> str:\n            Get language model response to query for given context.\n        process_question(question_type: str, question_id: str, query: str, context: str, info: Dict) -> None:\n            Process question and add the generated response to the instruct_list.\n        get_info_string(info, item_type) -> str:\n            Get string from info dictionary.\n        process_question_type(question_type: str, question_id: str, question_text: str) -> None:\n            Process questions related to a file, function, class, or method.\n        generate() -> Tuple[List[Dict], List[Dict]]:\n            Generate responses for all the questions and returns the instruct_list.\n    \"\"\"\n\n    def __init__(\n        self,\n        file_path: str,\n        file_details: Dict,\n        base_name: str,\n        questions: List[Dict],\n        model_config: Dict,\n        detailed: bool,\n    ) -> None:\n        \"\"\"\n        Initialize the DatasetGenerator class.\n        Args:\n            file_path (str): The path to the Python file.\n            file_details (Dict[str, Any]): Details of the Python file.\n            base_name (str): The base name of the Python file.\n            questions (List[Dict[str, str]]): Questions for generating responses.\n            model_config (Dict): Configuration for the language model.\n        Returns:\n            None\n        \"\"\"\n        self.file_path = file_path\n        self.file_details = file_details\n        self.base_name = base_name\n        self.questions = questions\n        self.model_config = model_config\n        if model_config is not None:\n            self.llm = model_config[\"model\"]\n            self.use_llm = True\n            self.detailed = detailed\n        else:\n            self.llm = None\n            self.use_llm = False\n            self.detailed = False\n        self.instruct_list = []\n        self.question_mapping = {\n            \"file\": \"file\",\n            \"function\": \"functions\",\n            \"class\": \"classes\",\n            \"method\": \"classes\",\n        }\n        self.code_qa_list = []\n        self.code_qa_response = \"\"\n\n    def get_response_from_llm(self, query: str, context: str) -> str:\n        \"\"\"\n        Get language model response to query for given context.\n        Args:\n            query (str): The query to be used for generating the response.\n            context (str): The context to be used for generating the response.\n        Returns:\n            str: The generated response.\n        \"\"\"\n        context_strategies = [\n            lambda: \"{}\".format(str(context)),\n            lambda: \"```python\\n{}\\n```\".format(\n                str(self.file_details[\"file_info\"][\"file_code_simplified\"])\n            ),\n            lambda: \"```python\\n{}\\n```\".format(\n                self.get_info_string(self.file_details[\"file_info\"], \"file_summary\")\n            ),\n            lambda: \"\",\n        ]\n\n        max_context_length = self.model_config[\"inference_model\"][\"model_params\"][\n            \"context_length\"\n        ]\n        prompt_template = self.model_config[\"prompt_template\"].format(\n            system_prompt=self.model_config[\"system_prompt\"],\n            instruction_prompt=self.model_config[\"instruction_prompt\"],\n        )\n\n        for strategy in context_strategies:\n            context = strategy()\n            prompt = prompt_template.format(\n                context=context, query=query, code_objects=self.code_qa_response\n            )\n            context_size = len(self.llm.tokenize(prompt))\n            logging.info(\"***Context Size: \" + str(context_size))\n            if context_size <= 0.70 * max_context_length:\n                break\n        else:\n            err_msg = f\"Model response failed, increase py2dataset_model_config.yaml context_length > {math.ceil(context_size/0.70)}\"\n            logging.error(err_msg)\n            return \"\"\n        if context == \"\":\n            context = self.code_qa_list\n\n        response = \"\"\n\n        try:\n            response = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", self.llm(prompt))\n            response = response.replace(\"<|im_end|>\", \"\")\n            response = response.replace(\"</|im_end|>\", \"\")\n            response = \"\\n\".join([line.lstrip() for line in response.split(\"\\n\")])\n            logging.info(f\"***Overall Response: {response}\")\n        except Exception as error:\n            logging.error(f\"Failed to generate model response: {error}\")\n\n        if self.detailed:\n         \n            for item in self.code_qa_list:\n                try:\n                    instruct_key = list(item.keys())[0]\n                    instruct_value = list(item.values())[0]\n                    query = f\"Describe the Purpose and Significance of these {instruct_key}: [{instruct_value}] and Explain what each of these {instruct_key} does in the code.\"\n                    prompt = (\n                        self.model_config[\"prompt_template\"]\n                        .format(\n                            system_prompt=self.model_config[\"system_prompt\"],\n                            instruction_prompt=self.model_config[\"instruction_prompt\"],\n                        )\n                        .format(\n                            context=f\"{context}/nCode Summary:/n{response}\",\n                            query=f\"{query}\",\n                            code_objects=f\"{instruct_value}\",\n                        )\n                    )\n                    item_response = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", self.llm(prompt))\n                    item_response = item_response.replace(\"<|im_end|>\", \"\")\n                    item_response = item_response.replace(\"</|im_end|>\", \"\")\n                    logging.info(f\"\\n***Itemized Response: {query}\\n{item_response}\")\n                    for item in self.instruct_list:\n                        if item[\"instruction\"].startswith(instruct_key):\n                            output = f\"\\n\\nPurpose and Significance:\\n{item_response}\"\n                            item[\"output\"] += output\n\n                    if '`' in instruct_key:\n                        dict_key1 = instruct_key.split(\"`\")[1]\n                        dict_key2 = instruct_key.split()[0]\n                    else:\n                        dict_key1 = instruct_key\n                        dict_key2 = \"\"\n                    purpose_dict = {\"Purpose\": item_response.strip()}\n                    if dict_key1 not in self.code_qa_dict:\n                        self.code_qa_dict[dict_key1] = {}\n                    elif not isinstance(self.code_qa_dict[dict_key1], dict):\n                        self.code_qa_dict[dict_key1] = {'Value': self.code_qa_dict[dict_key1]}\n                    if dict_key2:\n                        if not isinstance(self.code_qa_dict[dict_key1], dict):\n                            self.code_qa_dict[dict_key1] = {}\n                        if dict_key2 not in self.code_qa_dict[dict_key1] or not isinstance(self.code_qa_dict[dict_key1][dict_key2], dict):\n                            self.code_qa_dict[dict_key1][dict_key2] = {'Value': self.code_qa_dict[dict_key1].get(dict_key2, '')}\n                        self.code_qa_dict[dict_key1][dict_key2].update(purpose_dict)\n                    else:\n                        self.code_qa_dict[dict_key1].update(purpose_dict)\n\n                except Exception as error:\n                    logging.error(f\"Failed to generate model response: {error}\")\n\n        code_qa_text = yaml.dump(self.code_qa_dict,Dumper=yaml.SafeDumper,width=float(\"inf\"),sort_keys=False,default_flow_style=False).strip()\n        code_qa_text = re.sub(r\"\\n\\s*\\n\", \"\\n\", code_qa_text)\n        self.code_qa_response = code_qa_text\n\n        self.file_details[\"file_info\"][\"code_qa_response\"] = self.code_qa_response\n        self.file_details[\"file_info\"][\"purpose\"] = response\n\n        return response\n\n    def get_code_qa(self):\n        \"\"\"\n        Get code responses from the instruct_list and update:\n            code_qa_list (List[Dict]): List of code question-answer pairs.\n            code_qa_response (str): structured text for code question-answer pairs.\n        \"\"\"\n        excluded_instructions = {\"Call code graph\", \"Docstring\"}\n        self.code_qa_list = []\n        code_objects_responses = {}\n\n        for item in self.instruct_list:\n            instruction = item[\"instruction\"].split(\" in Python file:\")[0]\n            output = item[\"output\"]\n            if any(instruction.startswith(prefix) for prefix in excluded_instructions):\n                continue\n            self.code_qa_list.append({instruction: output})\n            if \"`\" in instruction:\n                code_object = instruction.split(\"`\")[1]\n                code_type = instruction.split()[0]\n                code_objects_responses.setdefault(code_object, []).append(\n                    (code_type, output)\n                )\n            else:\n                code_objects_responses.setdefault(instruction, []).append(\n                    (instruction, output)\n                )\n\n        self.code_qa_dict = {}\n        for idx, (code_object, type_responses) in enumerate(\n            code_objects_responses.items(), start=1\n        ):\n            self.code_qa_dict[code_object] = {}\n            for subidx, (code_type, response) in enumerate(type_responses, start=1):\n                if code_object == code_type:\n                    self.code_qa_dict[code_object] = response\n                else:\n                    self.code_qa_dict.setdefault(code_object, {})[code_type] = response\n\n        self.code_qa_response = yaml.dump(\n            self.code_qa_dict,\n            Dumper=yaml.SafeDumper,\n            width=float(\"inf\"),\n            sort_keys=False,\n            default_flow_style=False,\n        ).strip()\n        self.file_details[\"file_info\"][\"code_qa_response\"] = self.code_qa_response\n\n    def process_question(\n        self, question_type: str, question_id: str, query: str, context: str, info: Dict\n    ) -> None:\n        \"\"\"\n        Process question and add the generated response to the instruct_list.\n        Args:\n            question_type (str): The type of question to be processed.\n            question_id (str): The ID of the question to be processed.\n            query (str): The query to be processed.\n            context (str): The context to be used for generating the response.\n            info (Dict): The information of the Python file.\n        Returns:\n            None\n        \"\"\"\n        response = \"\"\n        if question_id.endswith((\"code_graph\", \"docstring\")):\n            response = info.get(question_id, {})\n        elif question_id.endswith(\"file_purpose\"):  # file_purpose is last question\n            self.get_code_qa()\n            if self.use_llm:\n                response = self.get_response_from_llm(query, context)\n        else:\n            response = clean_and_get_unique_elements(str(info.get(question_id, \"\")))\n        response = str(response).strip()\n\n        if response and response != \"None\":\n            self.instruct_list.append(\n                {\"instruction\": query, \"input\": context, \"output\": response}\n            )\n\n    @staticmethod\n    def get_info_string(info: Dict, item_type: str) -> str:\n        \"\"\"\n        Get string from info dictionary.\n        Args:\n            info (Dict): The information of the Python file.\n            item_type (str): The type of item to get the string from.\n        Returns:\n            str: The string from the info.\n        \"\"\"\n        return \", \".join(\n            [item.strip() for item in str(info.get(item_type, \"\")).split(\",\") if item]\n        )\n\n    def process_question_type(\n        self, question_type: str, question_id: str, question_text: str\n    ) -> None:\n        \"\"\"\n        Process questions related to a file, function, class, or method.\n        Args:\n            question_type (str): The type of question to be processed.\n            question_id (str): The ID of the question to be processed.\n            question_text (str): The text of the question to be processed.\n        Returns:\n            None\n        \"\"\"\n        if question_type == \"file\":\n            query = question_text.format(filename=self.base_name)\n            info = self.file_details[\"file_info\"]\n            context = self.file_details[\"file_info\"][\"file_code\"]\n            context = f\"```python\\n{context}\\n```\"\n            self.process_question(question_type, question_id, query, context, info)\n        elif question_type == \"method\":\n            for class_name, class_info in self.file_details[\"classes\"].items():\n                for key, method_info in class_info.items():\n                    if key.startswith(\"class_method_\"):\n                        method_name = f\"{class_name}.{key[len('class_method_'):]}\"\n                        context = method_info[\"method_code\"]\n                        context = f\"```python\\n{context}\\n```\"\n                        mapping = {\"class_name\": class_name, \"method_name\": method_name}\n                        query = question_text.format(filename=self.base_name, **mapping)\n                        self.process_question(\n                            question_type, question_id, query, context, method_info\n                        )\n        else:  # question_type is 'function' or 'class'\n            for name, info in self.file_details[\n                self.question_mapping[question_type]\n            ].items():\n                context = info[f\"{question_type}_code\"]\n                context = f\"```python\\n{context}\\n```\"\n                mapping = {f\"{question_type}_name\": name}\n                if question_id == f\"{question_type}_purpose\" and self.use_llm:\n                    variables = self.get_info_string(info, f\"{question_type}_variables\")\n                    inputs = self.get_info_string(info, f\"{question_type}_inputs\")\n                    combined = \", \".join([s for s in [variables, inputs] if s])\n                    mapping[f\"{question_type}_variables\"] = (\n                        clean_and_get_unique_elements(combined)\n                    )\n\n                    if question_type == \"class\":\n                        methods_string = self.get_info_string(\n                            info, f\"{question_type}_methods\"\n                        )\n                        mapping[f\"{question_type}_methods\"] = methods_string\n\n                query = question_text.format(filename=self.base_name, **mapping)\n                self.process_question(question_type, question_id, query, context, info)\n\n    def generate(self) -> Tuple[List[Dict], List[Dict]]:\n        \"\"\"\n        Generate responses for all the questions and returns the instruct_list.\n        Args:\n            None\n        Returns:\n            Tuple[List[Dict], List[Dict]]:  .\n        \"\"\"\n        for question in self.questions:\n            self.process_question_type(\n                question[\"type\"], question[\"id\"], question[\"text\"]\n            )\n        self.instruct_list.sort(key=lambda x: len(x[\"input\"]), reverse=True)\n        return self.instruct_list\n\n\ndef get_python_datasets(\n    file_path: str,\n    file_details: Dict,\n    base_name: str,\n    questions: List[Dict],\n    model_config: Dict,\n    detailed: bool,\n) -> Tuple[List[Dict], List[Dict]]:\n    \"\"\"\n    Extract information from a Python file and return it in JSON format.\n    Args:\n        file_path (str): The path to the Python file.\n        file_details (Dict): The details of the file.\n        base_name (str): The base Python code filename.\n        questions (List[Dict]): The list of questions.\n        llm (object): The language model to be used for generating responses.\n        prompt (str): The prompt to be used for generating responses.\n        detailed (bool): Flag indicating if detailed information should be extracted.\n    Returns:\n        Tuple[List[Dict], List[Dict]]: Extracted information in JSON format.\n    \"\"\"\n    return DatasetGenerator(\n        file_path, file_details, base_name, questions, model_config, detailed\n    ).generate()\n"
    file_dependencies:
    - logging
    - math
    - re
    - typing
    - yaml
    file_functions:
    - clean_and_get_unique_elements
    - element_generator
    - get_python_datasets
    file_classes:
    - DatasetGenerator
    file_constants: []
    file_summary: '{dependencies: [logging, math, re, typing, yaml], function_defs: [{clean_and_get_unique_elements: {inputs: [input_str], calls: [enumerate, input_str.strip(''[]\\''\'').strip, input_str.strip, element.strip(''\\''\ '').strip, element.strip, element_generator, '', ''.join], call_inputs: {enumerate: [input_str], input_str.strip(''[]\\''\'').strip: [], input_str.strip: [''[]\\''\''], element.strip(''\\''\ '').strip: [], element.strip: [''\\''\ ''], element_generator: [input_str], '', ''.join: [cleaned_elements]}, returns: [returned_elements]}}, {element_generator: {inputs: [input_str], calls: [enumerate], call_inputs: {enumerate: [input_str]}, returns: []}}, {get_python_datasets: {inputs: [file_path, file_details, base_name, questions, model_config, detailed], calls: [DatasetGenerator(file_path, file_details, base_name, questions, model_config, detailed).generate, DatasetGenerator], call_inputs: {DatasetGenerator(file_path, file_details, base_name, questions, model_config, detailed).generate: [], DatasetGenerator: [file_path, file_details, base_name, questions, model_config, detailed]}, returns: [DatasetGenerator(file_path, file_details, base_name, questions, model_config, detailed).generate()]}}], class_defs: [{DatasetGenerator: {method_defs: {__init__: {inputs: [self, file_path, file_details, base_name, questions, model_config, detailed], calls: [], call_inputs: {}, returns: []}, get_response_from_llm: {inputs: [self, query, context], calls: [''{}''.format, str, ''```python\\n{}\\n```''.format, self.get_info_string, self.model_config[''prompt_template''].format, strategy, prompt_template.format, len, self.llm.tokenize, logging.info, math.ceil, logging.error, re.sub, self.llm, response.replace, ''\\n''.join, line.lstrip, response.split, list, item.keys, item.values, self.model_config[''prompt_template''].format(system_prompt=self.model_config[''system_prompt''], instruction_prompt=self.model_config[''instruction_prompt'']).format, item_response.replace, item[''instruction''].startswith, instruct_key.split, item_response.strip, isinstance, self.code_qa_dict[dict_key1].get, self.code_qa_dict[dict_key1][dict_key2].update, self.code_qa_dict[dict_key1].update, yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float(''inf''), sort_keys=False, default_flow_style=False).strip, yaml.dump, float], call_inputs: {''{}''.format: [str(context)], str: [context, self.file_details[''file_info''][''file_code_simplified''], context_size], ''```python\\n{}\\n```''.format: [str(self.file_details[''file_info''][''file_code_simplified'']), self.get_info_string(self.file_details[''file_info''], ''file_summary'')], self.get_info_string: [self.file_details[''file_info''], ''file_summary''], self.model_config[''prompt_template''].format: [], strategy: [], prompt_template.format: [], len: [self.llm.tokenize(prompt)], self.llm.tokenize: [prompt], logging.info: [''***Context Size: '' + str(context_size), f''***Overall Response: {response}'', f''\\n***Itemized Response: {query}\\n{item_response}''], math.ceil: [context_size / 0.7], logging.error: [err_msg, f''Failed to generate model response: {error}'', f''Failed to generate model response: {error}''], re.sub: [''\\\\n\\\\s*\\\\n'', ''\\n\\n'', self.llm(prompt), ''\\\\n\\\\s*\\\\n'', ''\\n\\n'', self.llm(prompt), ''\\\\n\\\\s*\\\\n'', ''\\n'', code_qa_text], self.llm: [prompt, prompt], response.replace: [''<|im_end|>'', '''', ''</|im_end|>'', ''''], ''\\n''.join: [[line.lstrip() for line in response.split(''\\n'')]], line.lstrip: [], response.split: [''\\n''], list: [item.keys(), item.values()], item.keys: [], item.values: [], self.model_config[''prompt_template''].format(system_prompt=self.model_config[''system_prompt''], instruction_prompt=self.model_config[''instruction_prompt'']).format: [], item_response.replace: [''<|im_end|>'', '''', ''</|im_end|>'', ''''], item[''instruction''].startswith: [instruct_key], instruct_key.split: [''`''], item_response.strip: [], isinstance: [self.code_qa_dict[dict_key1], dict, self.code_qa_dict[dict_key1], dict, self.code_qa_dict[dict_key1][dict_key2], dict], self.code_qa_dict[dict_key1].get: [dict_key2, ''''], self.code_qa_dict[dict_key1][dict_key2].update: [purpose_dict], self.code_qa_dict[dict_key1].update: [purpose_dict], yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float(''inf''), sort_keys=False, default_flow_style=False).strip: [], yaml.dump: [self.code_qa_dict], float: [''inf'']}, returns: [response, '''']}, get_code_qa: {inputs: [self], calls: [item[''instruction''].split, any, instruction.startswith, self.code_qa_list.append, instruction.split, code_objects_responses.setdefault(code_object, []).append, code_objects_responses.setdefault, code_objects_responses.setdefault(instruction, []).append, enumerate, code_objects_responses.items, self.code_qa_dict.setdefault, yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float(''inf''), sort_keys=False, default_flow_style=False).strip, yaml.dump, float], call_inputs: {item[''instruction''].split: ['' in Python file:''], any: [(instruction.startswith(prefix) for prefix in excluded_instructions)], instruction.startswith: [prefix], self.code_qa_list.append: [{instruction: output}], instruction.split: [''`''], code_objects_responses.setdefault(code_object, []).append: [(code_type, output)], code_objects_responses.setdefault: [code_object, [], instruction, []], code_objects_responses.setdefault(instruction, []).append: [(instruction, output)], enumerate: [code_objects_responses.items(), type_responses], code_objects_responses.items: [], self.code_qa_dict.setdefault: [code_object, {}], yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float(''inf''), sort_keys=False, default_flow_style=False).strip: [], yaml.dump: [self.code_qa_dict], float: [''inf'']}, returns: []}, process_question: {inputs: [self, question_type, question_id, query, context, info], calls: [question_id.endswith, info.get, self.get_code_qa, self.get_response_from_llm, clean_and_get_unique_elements, str, str(response).strip, self.instruct_list.append], call_inputs: {question_id.endswith: [(''code_graph'', ''docstring''), ''file_purpose''], info.get: [question_id, {}, question_id, ''''], self.get_code_qa: [], self.get_response_from_llm: [query, context], clean_and_get_unique_elements: [str(info.get(question_id, ''''))], str: [info.get(question_id, ''''), response], str(response).strip: [], self.instruct_list.append: [{''instruction'': query, ''input'': context, ''output'': response}]}, returns: []}, get_info_string: {inputs: [info, item_type], calls: ['', ''.join, item.strip, str(info.get(item_type, '''')).split, str, info.get], call_inputs: {'', ''.join: [[item.strip() for item in str(info.get(item_type, '''')).split('','') if item]], item.strip: [], str(info.get(item_type, '''')).split: ['',''], str: [info.get(item_type, '''')], info.get: [item_type, '''']}, returns: ['', ''.join([item.strip() for item in str(info.get(item_type, '''')).split('','') if item])]}, process_question_type: {inputs: [self, question_type, question_id, question_text], calls: [question_text.format, self.process_question, self.file_details[''classes''].items, class_info.items, key.startswith, len, self.file_details[self.question_mapping[question_type]].items, self.get_info_string, '', ''.join, clean_and_get_unique_elements], call_inputs: {question_text.format: [], self.process_question: [question_type, question_id, query, context, info, question_type, question_id, query, context, method_info, question_type, question_id, query, context, info], self.file_details[''classes''].items: [], class_info.items: [], key.startswith: [''class_method_''], len: [''class_method_''], self.file_details[self.question_mapping[question_type]].items: [], self.get_info_string: [info, f''{question_type}_variables'', info, f''{question_type}_inputs'', info, f''{question_type}_methods''], '', ''.join: [[s for s in [variables, inputs] if s]], clean_and_get_unique_elements: [combined]}, returns: []}, generate: {inputs: [self], calls: [self.process_question_type, self.instruct_list.sort, len], call_inputs: {self.process_question_type: [question[''type''], question[''id''], question[''text'']], self.instruct_list.sort: [], len: [x[''input'']]}, returns: [self.instruct_list]}}}}]}'
    file_code_simplified: "import logging\nimport re\nimport math\nfrom typing import Dict, List, Tuple\nimport yaml\n\ndef clean_and_get_unique_elements(input_str: str) -> str:\n\n    def element_generator(input_str):\n        start, brace_level = (0, 0)\n        for i, char in enumerate(input_str):\n            if char in '{}':\n                brace_level += 1 if char == '{' else -1\n            elif char == ',' and brace_level == 0:\n                yield input_str[start:i]\n                start = i + 1\n        yield input_str[start:]\n    input_str = input_str.strip('[]\\'\"').strip()\n    cleaned_elements = [element.strip('\\'\" ').strip() for element in element_generator(input_str) if element.strip()]\n    returned_elements = ', '.join(cleaned_elements)\n    return returned_elements\n\nclass DatasetGenerator:\n\n    def __init__(self, file_path: str, file_details: Dict, base_name: str, questions: List[Dict], model_config: Dict, detailed: bool) -> None:\n        self.file_path = file_path\n        self.file_details = file_details\n        self.base_name = base_name\n        self.questions = questions\n        self.model_config = model_config\n        if model_config is not None:\n            self.llm = model_config['model']\n            self.use_llm = True\n            self.detailed = detailed\n        else:\n            self.llm = None\n            self.use_llm = False\n            self.detailed = False\n        self.instruct_list = []\n        self.question_mapping = {'file': 'file', 'function': 'functions', 'class': 'classes', 'method': 'classes'}\n        self.code_qa_list = []\n        self.code_qa_response = ''\n\n    def get_response_from_llm(self, query: str, context: str) -> str:\n        context_strategies = [lambda: '{}'.format(str(context)), lambda: '```python\\n{}\\n```'.format(str(self.file_details['file_info']['file_code_simplified'])), lambda: '```python\\n{}\\n```'.format(self.get_info_string(self.file_details['file_info'], 'file_summary')), lambda: '']\n        max_context_length = self.model_config['inference_model']['model_params']['context_length']\n        prompt_template = self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'], instruction_prompt=self.model_config['instruction_prompt'])\n        for strategy in context_strategies:\n            context = strategy()\n            prompt = prompt_template.format(context=context, query=query, code_objects=self.code_qa_response)\n            context_size = len(self.llm.tokenize(prompt))\n            logging.info('***Context Size: ' + str(context_size))\n            if context_size <= 0.7 * max_context_length:\n                break\n        else:\n            err_msg = f'Model response failed, increase py2dataset_model_config.yaml context_length > {math.ceil(context_size / 0.7)}'\n            logging.error(err_msg)\n            return ''\n        if context == '':\n            context = self.code_qa_list\n        response = ''\n        try:\n            response = re.sub('\\\\n\\\\s*\\\\n', '\\n\\n', self.llm(prompt))\n            response = response.replace('<|im_end|>', '')\n            response = response.replace('</|im_end|>', '')\n            response = '\\n'.join([line.lstrip() for line in response.split('\\n')])\n            logging.info(f'***Overall Response: {response}')\n        except Exception as error:\n            logging.error(f'Failed to generate model response: {error}')\n        if self.detailed:\n            for item in self.code_qa_list:\n                try:\n                    instruct_key = list(item.keys())[0]\n                    instruct_value = list(item.values())[0]\n                    query = f'Describe the Purpose and Significance of these {instruct_key}: [{instruct_value}] and Explain what each of these {instruct_key} does in the code.'\n                    prompt = self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'], instruction_prompt=self.model_config['instruction_prompt']).format(context=f'{context}/nCode Summary:/n{response}', query=f'{query}', code_objects=f'{instruct_value}')\n                    item_response = re.sub('\\\\n\\\\s*\\\\n', '\\n\\n', self.llm(prompt))\n                    item_response = item_response.replace('<|im_end|>', '')\n                    item_response = item_response.replace('</|im_end|>', '')\n                    logging.info(f'\\n***Itemized Response: {query}\\n{item_response}')\n                    for item in self.instruct_list:\n                        if item['instruction'].startswith(instruct_key):\n                            output = f'\\n\\nPurpose and Significance:\\n{item_response}'\n                            item['output'] += output\n                    if '`' in instruct_key:\n                        dict_key1 = instruct_key.split('`')[1]\n                        dict_key2 = instruct_key.split()[0]\n                    else:\n                        dict_key1 = instruct_key\n                        dict_key2 = ''\n                    purpose_dict = {'Purpose': item_response.strip()}\n                    if dict_key1 not in self.code_qa_dict:\n                        self.code_qa_dict[dict_key1] = {}\n                    elif not isinstance(self.code_qa_dict[dict_key1], dict):\n                        self.code_qa_dict[dict_key1] = {'Value': self.code_qa_dict[dict_key1]}\n                    if dict_key2:\n                        if not isinstance(self.code_qa_dict[dict_key1], dict):\n                            self.code_qa_dict[dict_key1] = {}\n                        if dict_key2 not in self.code_qa_dict[dict_key1] or not isinstance(self.code_qa_dict[dict_key1][dict_key2], dict):\n                            self.code_qa_dict[dict_key1][dict_key2] = {'Value': self.code_qa_dict[dict_key1].get(dict_key2, '')}\n                        self.code_qa_dict[dict_key1][dict_key2].update(purpose_dict)\n                    else:\n                        self.code_qa_dict[dict_key1].update(purpose_dict)\n                except Exception as error:\n                    logging.error(f'Failed to generate model response: {error}')\n        code_qa_text = yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False).strip()\n        code_qa_text = re.sub('\\\\n\\\\s*\\\\n', '\\n', code_qa_text)\n        self.code_qa_response = code_qa_text\n        self.file_details['file_info']['code_qa_response'] = self.code_qa_response\n        self.file_details['file_info']['purpose'] = response\n        return response\n\n    def get_code_qa(self):\n        excluded_instructions = {'Call code graph', 'Docstring'}\n        self.code_qa_list = []\n        code_objects_responses = {}\n        for item in self.instruct_list:\n            instruction = item['instruction'].split(' in Python file:')[0]\n            output = item['output']\n            if any((instruction.startswith(prefix) for prefix in excluded_instructions)):\n                continue\n            self.code_qa_list.append({instruction: output})\n            if '`' in instruction:\n                code_object = instruction.split('`')[1]\n                code_type = instruction.split()[0]\n                code_objects_responses.setdefault(code_object, []).append((code_type, output))\n            else:\n                code_objects_responses.setdefault(instruction, []).append((instruction, output))\n        self.code_qa_dict = {}\n        for idx, (code_object, type_responses) in enumerate(code_objects_responses.items(), start=1):\n            self.code_qa_dict[code_object] = {}\n            for subidx, (code_type, response) in enumerate(type_responses, start=1):\n                if code_object == code_type:\n                    self.code_qa_dict[code_object] = response\n                else:\n                    self.code_qa_dict.setdefault(code_object, {})[code_type] = response\n        self.code_qa_response = yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False).strip()\n        self.file_details['file_info']['code_qa_response'] = self.code_qa_response\n\n    def process_question(self, question_type: str, question_id: str, query: str, context: str, info: Dict) -> None:\n        response = ''\n        if question_id.endswith(('code_graph', 'docstring')):\n            response = info.get(question_id, {})\n        elif question_id.endswith('file_purpose'):\n            self.get_code_qa()\n            if self.use_llm:\n                response = self.get_response_from_llm(query, context)\n        else:\n            response = clean_and_get_unique_elements(str(info.get(question_id, '')))\n        response = str(response).strip()\n        if response and response != 'None':\n            self.instruct_list.append({'instruction': query, 'input': context, 'output': response})\n\n    @staticmethod\n    def get_info_string(info: Dict, item_type: str) -> str:\n        return ', '.join([item.strip() for item in str(info.get(item_type, '')).split(',') if item])\n\n    def process_question_type(self, question_type: str, question_id: str, question_text: str) -> None:\n        if question_type == 'file':\n            query = question_text.format(filename=self.base_name)\n            info = self.file_details['file_info']\n            context = self.file_details['file_info']['file_code']\n            context = f'```python\\n{context}\\n```'\n            self.process_question(question_type, question_id, query, context, info)\n        elif question_type == 'method':\n            for class_name, class_info in self.file_details['classes'].items():\n                for key, method_info in class_info.items():\n                    if key.startswith('class_method_'):\n                        method_name = f\"{class_name}.{key[len('class_method_'):]}\"\n                        context = method_info['method_code']\n                        context = f'```python\\n{context}\\n```'\n                        mapping = {'class_name': class_name, 'method_name': method_name}\n                        query = question_text.format(filename=self.base_name, **mapping)\n                        self.process_question(question_type, question_id, query, context, method_info)\n        else:\n            for name, info in self.file_details[self.question_mapping[question_type]].items():\n                context = info[f'{question_type}_code']\n                context = f'```python\\n{context}\\n```'\n                mapping = {f'{question_type}_name': name}\n                if question_id == f'{question_type}_purpose' and self.use_llm:\n                    variables = self.get_info_string(info, f'{question_type}_variables')\n                    inputs = self.get_info_string(info, f'{question_type}_inputs')\n                    combined = ', '.join([s for s in [variables, inputs] if s])\n                    mapping[f'{question_type}_variables'] = clean_and_get_unique_elements(combined)\n                    if question_type == 'class':\n                        methods_string = self.get_info_string(info, f'{question_type}_methods')\n                        mapping[f'{question_type}_methods'] = methods_string\n                query = question_text.format(filename=self.base_name, **mapping)\n                self.process_question(question_type, question_id, query, context, info)\n\n    def generate(self) -> Tuple[List[Dict], List[Dict]]:\n        for question in self.questions:\n            self.process_question_type(question['type'], question['id'], question['text'])\n        self.instruct_list.sort(key=lambda x: len(x['input']), reverse=True)\n        return self.instruct_list\n\ndef get_python_datasets(file_path: str, file_details: Dict, base_name: str, questions: List[Dict], model_config: Dict, detailed: bool) -> Tuple[List[Dict], List[Dict]]:\n    return DatasetGenerator(file_path, file_details, base_name, questions, model_config, detailed).generate()"
    entire_code_graph:
        nodes:
        - DatasetGenerator
        - DatasetGenerator.__init__
        - DatasetGenerator.get_response_from_llm
        - DatasetGenerator.get_code_qa
        - DatasetGenerator.process_question
        - DatasetGenerator.get_info_string
        - DatasetGenerator.process_question_type
        - DatasetGenerator.generate
        - clean_and_get_unique_elements
        - element_generator
        - get_python_datasets
        - enumerate
        - input_str.strip('[]\'"').strip
        - input_str.strip
        - element.strip('\'" ').strip
        - element.strip
        - ''', ''.join'
        - DatasetGenerator(file_path, file_details, base_name, questions, model_config, detailed).generate
        - '''{}''.format'
        - str
        - '''```python\n{}\n```''.format'
        - self.model_config['prompt_template'].format
        - strategy
        - prompt_template.format
        - len
        - self.llm.tokenize
        - logging.info
        - math.ceil
        - logging.error
        - re.sub
        - self.llm
        - response.replace
        - '''\n''.join'
        - line.lstrip
        - response.split
        - list
        - item.keys
        - item.values
        - self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'], instruction_prompt=self.model_config['instruction_prompt']).format
        - item_response.replace
        - item['instruction'].startswith
        - instruct_key.split
        - item_response.strip
        - isinstance
        - self.code_qa_dict[dict_key1].get
        - self.code_qa_dict[dict_key1][dict_key2].update
        - self.code_qa_dict[dict_key1].update
        - yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False).strip
        - yaml.dump
        - float
        - item['instruction'].split
        - any
        - instruction.startswith
        - self.code_qa_list.append
        - instruction.split
        - code_objects_responses.setdefault(code_object, []).append
        - code_objects_responses.setdefault
        - code_objects_responses.setdefault(instruction, []).append
        - code_objects_responses.items
        - self.code_qa_dict.setdefault
        - question_id.endswith
        - info.get
        - str(response).strip
        - self.instruct_list.append
        - item.strip
        - str(info.get(item_type, '')).split
        - question_text.format
        - self.file_details['classes'].items
        - class_info.items
        - key.startswith
        - self.file_details[self.question_mapping[question_type]].items
        - self.instruct_list.sort
        edges:
        -   source: DatasetGenerator
            target: DatasetGenerator.__init__
            target_inputs:
            - self
            - file_path
            - file_details
            - base_name
            - questions
            - model_config
            - detailed
            target_returns: []
        -   source: DatasetGenerator
            target: DatasetGenerator.get_response_from_llm
            target_inputs:
            - self
            - query
            - context
            target_returns:
            - response
            - ''''''
        -   source: DatasetGenerator
            target: DatasetGenerator.get_code_qa
            target_inputs:
            - self
            target_returns: []
        -   source: DatasetGenerator
            target: DatasetGenerator.process_question
            target_inputs:
            - self
            - question_type
            - question_id
            - query
            - context
            - info
            target_returns: []
        -   source: DatasetGenerator
            target: DatasetGenerator.get_info_string
            target_inputs:
            - info
            - item_type
            target_returns:
            - ''', ''.join([item.strip() for item in str(info.get(item_type, '''')).split('','') if item])'
        -   source: DatasetGenerator
            target: DatasetGenerator.process_question_type
            target_inputs:
            - self
            - question_type
            - question_id
            - question_text
            target_returns: []
        -   source: DatasetGenerator
            target: DatasetGenerator.generate
            target_inputs:
            - self
            target_returns:
            - self.instruct_list
        -   source: DatasetGenerator.get_response_from_llm
            target: '''{}''.format'
            target_inputs:
            - str(context)
        -   source: DatasetGenerator.get_response_from_llm
            target: str
            target_inputs:
            - context
            - self.file_details['file_info']['file_code_simplified']
            - context_size
        -   source: DatasetGenerator.get_response_from_llm
            target: '''```python\n{}\n```''.format'
            target_inputs:
            - str(self.file_details['file_info']['file_code_simplified'])
            - self.get_info_string(self.file_details['file_info'], 'file_summary')
        -   source: DatasetGenerator.get_response_from_llm
            target: DatasetGenerator.get_info_string
            target_inputs:
            - info
            - item_type
            target_returns:
            - ''', ''.join([item.strip() for item in str(info.get(item_type, '''')).split('','') if item])'
        -   source: DatasetGenerator.get_response_from_llm
            target: self.model_config['prompt_template'].format
            target_inputs: []
        -   source: DatasetGenerator.get_response_from_llm
            target: strategy
            target_inputs: []
        -   source: DatasetGenerator.get_response_from_llm
            target: prompt_template.format
            target_inputs: []
        -   source: DatasetGenerator.get_response_from_llm
            target: len
            target_inputs:
            - self.llm.tokenize(prompt)
        -   source: DatasetGenerator.get_response_from_llm
            target: self.llm.tokenize
            target_inputs:
            - prompt
        -   source: DatasetGenerator.get_response_from_llm
            target: logging.info
            target_inputs:
            - '''***Context Size: '' + str(context_size)'
            - 'f''***Overall Response: {response}'''
            - 'f''\n***Itemized Response: {query}\n{item_response}'''
        -   source: DatasetGenerator.get_response_from_llm
            target: math.ceil
            target_inputs:
            - context_size / 0.7
        -   source: DatasetGenerator.get_response_from_llm
            target: logging.error
            target_inputs:
            - err_msg
            - 'f''Failed to generate model response: {error}'''
            - 'f''Failed to generate model response: {error}'''
        -   source: DatasetGenerator.get_response_from_llm
            target: re.sub
            target_inputs:
            - '''\\n\\s*\\n'''
            - '''\n\n'''
            - self.llm(prompt)
            - '''\\n\\s*\\n'''
            - '''\n\n'''
            - self.llm(prompt)
            - '''\\n\\s*\\n'''
            - '''\n'''
            - code_qa_text
        -   source: DatasetGenerator.get_response_from_llm
            target: self.llm
            target_inputs:
            - prompt
            - prompt
        -   source: DatasetGenerator.get_response_from_llm
            target: response.replace
            target_inputs:
            - '''<|im_end|>'''
            - ''''''
            - '''</|im_end|>'''
            - ''''''
        -   source: DatasetGenerator.get_response_from_llm
            target: '''\n''.join'
            target_inputs:
            - '[line.lstrip() for line in response.split(''\n'')]'
        -   source: DatasetGenerator.get_response_from_llm
            target: line.lstrip
            target_inputs: []
        -   source: DatasetGenerator.get_response_from_llm
            target: response.split
            target_inputs:
            - '''\n'''
        -   source: DatasetGenerator.get_response_from_llm
            target: list
            target_inputs:
            - item.keys()
            - item.values()
        -   source: DatasetGenerator.get_response_from_llm
            target: item.keys
            target_inputs: []
        -   source: DatasetGenerator.get_response_from_llm
            target: item.values
            target_inputs: []
        -   source: DatasetGenerator.get_response_from_llm
            target: self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'], instruction_prompt=self.model_config['instruction_prompt']).format
            target_inputs: []
        -   source: DatasetGenerator.get_response_from_llm
            target: item_response.replace
            target_inputs:
            - '''<|im_end|>'''
            - ''''''
            - '''</|im_end|>'''
            - ''''''
        -   source: DatasetGenerator.get_response_from_llm
            target: item['instruction'].startswith
            target_inputs:
            - instruct_key
        -   source: DatasetGenerator.get_response_from_llm
            target: instruct_key.split
            target_inputs:
            - '''`'''
        -   source: DatasetGenerator.get_response_from_llm
            target: item_response.strip
            target_inputs: []
        -   source: DatasetGenerator.get_response_from_llm
            target: isinstance
            target_inputs:
            - self.code_qa_dict[dict_key1]
            - dict
            - self.code_qa_dict[dict_key1]
            - dict
            - self.code_qa_dict[dict_key1][dict_key2]
            - dict
        -   source: DatasetGenerator.get_response_from_llm
            target: self.code_qa_dict[dict_key1].get
            target_inputs:
            - dict_key2
            - ''''''
        -   source: DatasetGenerator.get_response_from_llm
            target: self.code_qa_dict[dict_key1][dict_key2].update
            target_inputs:
            - purpose_dict
        -   source: DatasetGenerator.get_response_from_llm
            target: self.code_qa_dict[dict_key1].update
            target_inputs:
            - purpose_dict
        -   source: DatasetGenerator.get_response_from_llm
            target: yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False).strip
            target_inputs: []
        -   source: DatasetGenerator.get_response_from_llm
            target: yaml.dump
            target_inputs:
            - self.code_qa_dict
        -   source: DatasetGenerator.get_response_from_llm
            target: float
            target_inputs:
            - '''inf'''
        -   source: DatasetGenerator.get_code_qa
            target: item['instruction'].split
            target_inputs:
            - ''' in Python file:'''
        -   source: DatasetGenerator.get_code_qa
            target: any
            target_inputs:
            - (instruction.startswith(prefix) for prefix in excluded_instructions)
        -   source: DatasetGenerator.get_code_qa
            target: instruction.startswith
            target_inputs:
            - prefix
        -   source: DatasetGenerator.get_code_qa
            target: self.code_qa_list.append
            target_inputs:
            - '{instruction: output}'
        -   source: DatasetGenerator.get_code_qa
            target: instruction.split
            target_inputs:
            - '''`'''
        -   source: DatasetGenerator.get_code_qa
            target: code_objects_responses.setdefault(code_object, []).append
            target_inputs:
            - (code_type, output)
        -   source: DatasetGenerator.get_code_qa
            target: code_objects_responses.setdefault
            target_inputs:
            - code_object
            - '[]'
            - instruction
            - '[]'
        -   source: DatasetGenerator.get_code_qa
            target: code_objects_responses.setdefault(instruction, []).append
            target_inputs:
            - (instruction, output)
        -   source: DatasetGenerator.get_code_qa
            target: enumerate
            target_inputs:
            - code_objects_responses.items()
            - type_responses
        -   source: DatasetGenerator.get_code_qa
            target: code_objects_responses.items
            target_inputs: []
        -   source: DatasetGenerator.get_code_qa
            target: self.code_qa_dict.setdefault
            target_inputs:
            - code_object
            - '{}'
        -   source: DatasetGenerator.get_code_qa
            target: yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False).strip
            target_inputs: []
        -   source: DatasetGenerator.get_code_qa
            target: yaml.dump
            target_inputs:
            - self.code_qa_dict
        -   source: DatasetGenerator.get_code_qa
            target: float
            target_inputs:
            - '''inf'''
        -   source: DatasetGenerator.process_question
            target: question_id.endswith
            target_inputs:
            - ('code_graph', 'docstring')
            - '''file_purpose'''
        -   source: DatasetGenerator.process_question
            target: info.get
            target_inputs:
            - question_id
            - '{}'
            - question_id
            - ''''''
        -   source: DatasetGenerator.process_question
            target: DatasetGenerator.get_code_qa
            target_inputs:
            - self
            target_returns: []
        -   source: DatasetGenerator.process_question
            target: DatasetGenerator.get_response_from_llm
            target_inputs:
            - self
            - query
            - context
            target_returns:
            - response
            - ''''''
        -   source: DatasetGenerator.process_question
            target: clean_and_get_unique_elements
            target_inputs:
            - str(info.get(question_id, ''))
            target_returns:
            - returned_elements
        -   source: DatasetGenerator.process_question
            target: str
            target_inputs:
            - info.get(question_id, '')
            - response
        -   source: DatasetGenerator.process_question
            target: str(response).strip
            target_inputs: []
        -   source: DatasetGenerator.process_question
            target: self.instruct_list.append
            target_inputs:
            - '{''instruction'': query, ''input'': context, ''output'': response}'
        -   source: DatasetGenerator.get_info_string
            target: ''', ''.join'
            target_inputs:
            - '[item.strip() for item in str(info.get(item_type, '''')).split('','') if item]'
        -   source: DatasetGenerator.get_info_string
            target: item.strip
            target_inputs: []
        -   source: DatasetGenerator.get_info_string
            target: str(info.get(item_type, '')).split
            target_inputs:
            - ''','''
        -   source: DatasetGenerator.get_info_string
            target: str
            target_inputs:
            - info.get(item_type, '')
        -   source: DatasetGenerator.get_info_string
            target: info.get
            target_inputs:
            - item_type
            - ''''''
        -   source: DatasetGenerator.process_question_type
            target: question_text.format
            target_inputs: []
        -   source: DatasetGenerator.process_question_type
            target: DatasetGenerator.process_question
            target_inputs:
            - self
            - question_type
            - question_id
            - query
            - context
            - info
            target_returns: []
        -   source: DatasetGenerator.process_question_type
            target: self.file_details['classes'].items
            target_inputs: []
        -   source: DatasetGenerator.process_question_type
            target: class_info.items
            target_inputs: []
        -   source: DatasetGenerator.process_question_type
            target: key.startswith
            target_inputs:
            - '''class_method_'''
        -   source: DatasetGenerator.process_question_type
            target: len
            target_inputs:
            - '''class_method_'''
        -   source: DatasetGenerator.process_question_type
            target: self.file_details[self.question_mapping[question_type]].items
            target_inputs: []
        -   source: DatasetGenerator.process_question_type
            target: DatasetGenerator.get_info_string
            target_inputs:
            - info
            - item_type
            target_returns:
            - ''', ''.join([item.strip() for item in str(info.get(item_type, '''')).split('','') if item])'
        -   source: DatasetGenerator.process_question_type
            target: ''', ''.join'
            target_inputs:
            - '[s for s in [variables, inputs] if s]'
        -   source: DatasetGenerator.process_question_type
            target: clean_and_get_unique_elements
            target_inputs:
            - combined
            target_returns:
            - returned_elements
        -   source: DatasetGenerator.generate
            target: DatasetGenerator.process_question_type
            target_inputs:
            - self
            - question_type
            - question_id
            - question_text
            target_returns: []
        -   source: DatasetGenerator.generate
            target: self.instruct_list.sort
            target_inputs: []
        -   source: DatasetGenerator.generate
            target: len
            target_inputs:
            - x['input']
        -   source: clean_and_get_unique_elements
            target: enumerate
            target_inputs:
            - input_str
        -   source: clean_and_get_unique_elements
            target: input_str.strip('[]\'"').strip
            target_inputs: []
        -   source: clean_and_get_unique_elements
            target: input_str.strip
            target_inputs:
            - '''[]\''"'''
        -   source: clean_and_get_unique_elements
            target: element.strip('\'" ').strip
            target_inputs: []
        -   source: clean_and_get_unique_elements
            target: element.strip
            target_inputs:
            - '''\''" '''
        -   source: clean_and_get_unique_elements
            target: element_generator
            target_inputs:
            - input_str
            target_returns: []
        -   source: clean_and_get_unique_elements
            target: ''', ''.join'
            target_inputs:
            - cleaned_elements
        -   source: element_generator
            target: enumerate
            target_inputs:
            - input_str
        -   source: get_python_datasets
            target: DatasetGenerator(file_path, file_details, base_name, questions, model_config, detailed).generate
            target_inputs: []
        -   source: get_python_datasets
            target: DatasetGenerator
            target_inputs:
            - file_path
            - file_details
            - base_name
            - questions
            - model_config
            - detailed
            target_returns: []
    control_flow_structure:
    -   ? 'def get_python_datasets(file_path: str, file_details: Dict, base_name: str, questions: List[Dict], model_config: Dict, detailed: bool)'
        :   -   return:
                - DatasetGenerator(file_path, file_details, base_name, questions, model_config, detailed).generate()
    - import logging
    - import re
    - import math
    - from typing import Dict, List, Tuple
    - import yaml
    -   'def clean_and_get_unique_elements(input_str: str)':
        -   def element_generator(input_str):
            - start, brace_level = (0, 0)
            -   for (i, char) in enumerate(input_str):
                -   if char in '{}':
                    - brace_level += 1 if char == '{' else -1
                    elif char == ',' and brace_level == 0:
                    - yield input_str[start:i]
                    - start = i + 1
            - yield input_str[start:]
        - input_str = input_str.strip('[]\'"').strip()
        - cleaned_elements = [element.strip('\'" ').strip() for element in element_generator(input_str) if element.strip()]
        - returned_elements = ', '.join(cleaned_elements)
        -   return:
            - returned_elements
    -   class DatasetGenerator:
        -   ? 'def __init__(self, file_path: str, file_details: Dict, base_name: str, questions: List[Dict], model_config: Dict, detailed: bool)'
            :   - self.file_path = file_path
                - self.file_details = file_details
                - self.base_name = base_name
                - self.questions = questions
                - self.model_config = model_config
                -   if model_config is not None:
                    - self.llm = model_config['model']
                    - self.use_llm = True
                    - self.detailed = detailed
                    else:
                    - self.llm = None
                    - self.use_llm = False
                    - self.detailed = False
                - self.instruct_list = []
                - 'self.question_mapping = {''file'': ''file'', ''function'': ''functions'', ''class'': ''classes'', ''method'': ''classes''}'
                - self.code_qa_list = []
                - self.code_qa_response = ''
        -   'def get_response_from_llm(self, query: str, context: str)':
            - 'context_strategies = [lambda: ''{}''.format(str(context)), lambda: ''```python\n{}\n```''.format(str(self.file_details[''file_info''][''file_code_simplified''])), lambda: ''```python\n{}\n```''.format(self.get_info_string(self.file_details[''file_info''], ''file_summary'')), lambda: '''']'
            - max_context_length = self.model_config['inference_model']['model_params']['context_length']
            - prompt_template = self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'], instruction_prompt=self.model_config['instruction_prompt'])
            -   for strategy in context_strategies:
                - context = strategy()
                - prompt = prompt_template.format(context=context, query=query, code_objects=self.code_qa_response)
                - context_size = len(self.llm.tokenize(prompt))
                - 'logging.info(''***Context Size: '' + str(context_size))'
                -   if context_size <= 0.7 * max_context_length:
                    - break
            -   if context == '':
                - context = self.code_qa_list
            - response = ''
            -   try:
                - response = re.sub('\\n\\s*\\n', '\n\n', self.llm(prompt))
                - response = response.replace('<|im_end|>', '')
                - response = response.replace('</|im_end|>', '')
                - response = '\n'.join([line.lstrip() for line in response.split('\n')])
                - 'logging.info(f''***Overall Response: {response}'')'
                except:
                -   'except Exception as :':
                    - 'logging.error(f''Failed to generate model response: {error}'')'
            -   if self.detailed:
                -   for item in self.code_qa_list:
                    -   try:
                        - instruct_key = list(item.keys())[0]
                        - instruct_value = list(item.values())[0]
                        - 'query = f''Describe the Purpose and Significance of these {instruct_key}: [{instruct_value}] and Explain what each of these {instruct_key} does in the code.'''
                        - prompt = self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'], instruction_prompt=self.model_config['instruction_prompt']).format(context=f'{context}/nCode Summary:/n{response}', query=f'{query}', code_objects=f'{instruct_value}')
                        - item_response = re.sub('\\n\\s*\\n', '\n\n', self.llm(prompt))
                        - item_response = item_response.replace('<|im_end|>', '')
                        - item_response = item_response.replace('</|im_end|>', '')
                        - 'logging.info(f''\n***Itemized Response: {query}\n{item_response}'')'
                        -   for item in self.instruct_list:
                            -   if item['instruction'].startswith(instruct_key):
                                - output = f'\n\nPurpose and Significance:\n{item_response}'
                                - item['output'] += output
                        -   if '`' in instruct_key:
                            - dict_key1 = instruct_key.split('`')[1]
                            - dict_key2 = instruct_key.split()[0]
                            else:
                            - dict_key1 = instruct_key
                            - dict_key2 = ''
                        - 'purpose_dict = {''Purpose'': item_response.strip()}'
                        -   if dict_key1 not in self.code_qa_dict:
                            - self.code_qa_dict[dict_key1] = {}
                            elif not isinstance(self.code_qa_dict[dict_key1], dict):
                            - 'self.code_qa_dict[dict_key1] = {''Value'': self.code_qa_dict[dict_key1]}'
                        -   if dict_key2:
                            -   if not isinstance(self.code_qa_dict[dict_key1], dict):
                                - self.code_qa_dict[dict_key1] = {}
                            -   if dict_key2 not in self.code_qa_dict[dict_key1] or not isinstance(self.code_qa_dict[dict_key1][dict_key2], dict):
                                - 'self.code_qa_dict[dict_key1][dict_key2] = {''Value'': self.code_qa_dict[dict_key1].get(dict_key2, '''')}'
                            - self.code_qa_dict[dict_key1][dict_key2].update(purpose_dict)
                            else:
                            - self.code_qa_dict[dict_key1].update(purpose_dict)
                        except:
                        -   'except Exception as :':
                            - 'logging.error(f''Failed to generate model response: {error}'')'
            - code_qa_text = yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False).strip()
            - code_qa_text = re.sub('\\n\\s*\\n', '\n', code_qa_text)
            - self.code_qa_response = code_qa_text
            - self.file_details['file_info']['code_qa_response'] = self.code_qa_response
            - self.file_details['file_info']['purpose'] = response
            -   return:
                - response
        -   def get_code_qa(self):
            - excluded_instructions = {'Call code graph', 'Docstring'}
            - self.code_qa_list = []
            - code_objects_responses = {}
            -   for item in self.instruct_list:
                - instruction = item['instruction'].split(' in Python file:')[0]
                - output = item['output']
                -   if any((instruction.startswith(prefix) for prefix in excluded_instructions)):
                    - continue
                - 'self.code_qa_list.append({instruction: output})'
                -   if '`' in instruction:
                    - code_object = instruction.split('`')[1]
                    - code_type = instruction.split()[0]
                    - code_objects_responses.setdefault(code_object, []).append((code_type, output))
                    else:
                    - code_objects_responses.setdefault(instruction, []).append((instruction, output))
            - self.code_qa_dict = {}
            -   for (idx, (code_object, type_responses)) in enumerate(code_objects_responses.items(), start=1):
                - self.code_qa_dict[code_object] = {}
                -   for (subidx, (code_type, response)) in enumerate(type_responses, start=1):
                    -   if code_object == code_type:
                        - self.code_qa_dict[code_object] = response
                        else:
                        - self.code_qa_dict.setdefault(code_object, {})[code_type] = response
            - self.code_qa_response = yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False).strip()
            - self.file_details['file_info']['code_qa_response'] = self.code_qa_response
        -   'def process_question(self, question_type: str, question_id: str, query: str, context: str, info: Dict)':
            - response = ''
            -   if question_id.endswith(('code_graph', 'docstring')):
                - response = info.get(question_id, {})
                elif question_id.endswith('file_purpose'):
                - self.get_code_qa()
                -   if self.use_llm:
                    - response = self.get_response_from_llm(query, context)
                else:
                - response = clean_and_get_unique_elements(str(info.get(question_id, '')))
            - response = str(response).strip()
            -   if response and response != 'None':
                - 'self.instruct_list.append({''instruction'': query, ''input'': context, ''output'': response})'
        -   'def get_info_string(info: Dict, item_type: str)':
            -   return:
                - ''', ''.join([item.strip() for item in str(info.get(item_type, '''')).split('','') if item])'
        -   'def process_question_type(self, question_type: str, question_id: str, question_text: str)':
            -   if question_type == 'file':
                - query = question_text.format(filename=self.base_name)
                - info = self.file_details['file_info']
                - context = self.file_details['file_info']['file_code']
                - context = f'```python\n{context}\n```'
                - self.process_question(question_type, question_id, query, context, info)
                elif question_type == 'method':
                -   for (class_name, class_info) in self.file_details['classes'].items():
                    -   for (key, method_info) in class_info.items():
                        -   if key.startswith('class_method_'):
                            - method_name = f"{class_name}.{key[len('class_method_'):]}"
                            - context = method_info['method_code']
                            - context = f'```python\n{context}\n```'
                            - 'mapping = {''class_name'': class_name, ''method_name'': method_name}'
                            - query = question_text.format(filename=self.base_name, **mapping)
                            - self.process_question(question_type, question_id, query, context, method_info)
                else:
                -   for (name, info) in self.file_details[self.question_mapping[question_type]].items():
                    - context = info[f'{question_type}_code']
                    - context = f'```python\n{context}\n```'
                    - 'mapping = {f''{question_type}_name'': name}'
                    -   if question_id == f'{question_type}_purpose' and self.use_llm:
                        - variables = self.get_info_string(info, f'{question_type}_variables')
                        - inputs = self.get_info_string(info, f'{question_type}_inputs')
                        - combined = ', '.join([s for s in [variables, inputs] if s])
                        - mapping[f'{question_type}_variables'] = clean_and_get_unique_elements(combined)
                        -   if question_type == 'class':
                            - methods_string = self.get_info_string(info, f'{question_type}_methods')
                            - mapping[f'{question_type}_methods'] = methods_string
                    - query = question_text.format(filename=self.base_name, **mapping)
                    - self.process_question(question_type, question_id, query, context, info)
        -   def generate(self):
            -   for question in self.questions:
                - self.process_question_type(question['type'], question['id'], question['text'])
            - 'self.instruct_list.sort(key=lambda x: len(x[''input'']), reverse=True)'
            -   return:
                - self.instruct_list
    plant_uml: "@startuml\n  : def get_python_datasets(file_path: str, file_details: Dict, base_name: str, questions: List[Dict], model_config: Dict, detailed: bool);\n      : return;\n  : import logging;\n  : import re;\n  : import math;\n  : from typing import Dict, List, Tuple;\n  : import yaml;\n  : def clean_and_get_unique_elements(input_str: str);\n      : def element_generator(input_str);\n          : start, brace_level = (0, 0);\n          if ((i, char) in enumerate(input_str)) then (yes)\n              if (char in '{}') then (yes)\n                  : brace_level += 1 if char == '{' else -1;\n              endif\n          endif\n          : yield input_str[start:];\n      : input_str = input_str.strip('[]\\'\"').strip();\n      : cleaned_elements = [element.strip('\\'\" ').strip() for element in element_generator(input_str) if element.strip()];\n      : returned_elements = ', '.join(cleaned_elements);\n      : return;\n  : class DatasetGenerator;\n      : def __init__(self, file_path: str, file_details: Dict, base_name: str, questions: List[Dict], model_config: Dict, detailed: bool);\n          : self.file_path = file_path;\n          : self.file_details = file_details;\n          : self.base_name = base_name;\n          : self.questions = questions;\n          : self.model_config = model_config;\n          if (model_config is not None) then (yes)\n              : self.llm = model_config['model'];\n              : self.use_llm = True;\n              : self.detailed = detailed;\n          endif\n          : self.instruct_list = [];\n          : self.question_mapping = {'file': 'file', 'function': 'functions', 'class': 'classes', 'method': 'classes'};\n          : self.code_qa_list = [];\n          : self.code_qa_response = '';\n      : def get_response_from_llm(self, query: str, context: str);\n          : context_strategies = [lambda: '{}'.format(str(context)), lambda: '```python\\n{}\\n```'.format(str(self.file_details['file_info']['file_code_simplified'])), lambda: '```python\\n{}\\n```'.format(self.get_info_string(self.file_details['file_info'], 'file_summary')), lambda: ''];\n          : max_context_length = self.model_config['inference_model']['model_params']['context_length'];\n          : prompt_template = self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'], instruction_prompt=self.model_config['instruction_prompt']);\n          if (strategy in context_strategies) then (yes)\n              : context = strategy();\n              : prompt = prompt_template.format(context=context, query=query, code_objects=self.code_qa_response);\n              : context_size = len(self.llm.tokenize(prompt));\n              : logging.info('***Context Size: ' + str(context_size));\n              if (context_size <= 0.7 * max_context_length) then (yes)\n                  : break;\n              endif\n          endif\n          if (context == '') then (yes)\n              : context = self.code_qa_list;\n          endif\n          : response = '';\n          partition \"try\" {\n              : response = re.sub('\\\\n\\\\s*\\\\n', '\\n\\n', self.llm(prompt));\n              : response = response.replace('<|im_end|>', '');\n              : response = response.replace('</|im_end|>', '');\n              : response = '\\n'.join([line.lstrip() for line in response.split('\\n')]);\n              : logging.info(f'***Overall Response: {response}');\n          }\n          if (self.detailed) then (yes)\n              if (item in self.code_qa_list) then (yes)\n                  partition \"try\" {\n                      : instruct_key = list(item.keys())[0];\n                      : instruct_value = list(item.values())[0];\n                      : query = f'Describe the Purpose and Significance of these {instruct_key}: [{instruct_value}] and Explain what each of these {instruct_key} does in the code.';\n                      : prompt = self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'], instruction_prompt=self.model_config['instruction_prompt']).format(context=f'{context}/nCode Summary:/n{response}', query=f'{query}', code_objects=f'{instruct_value}');\n                      : item_response = re.sub('\\\\n\\\\s*\\\\n', '\\n\\n', self.llm(prompt));\n                      : item_response = item_response.replace('<|im_end|>', '');\n                      : item_response = item_response.replace('</|im_end|>', '');\n                      : logging.info(f'\\n***Itemized Response: {query}\\n{item_response}');\n                      if (item in self.instruct_list) then (yes)\n                          if (item['instruction'].startswith(instruct_key)) then (yes)\n                              : output = f'\\n\\nPurpose and Significance:\\n{item_response}';\n                              : item['output'] += output;\n                          endif\n                      endif\n                      if ('`' in instruct_key) then (yes)\n                          : dict_key1 = instruct_key.split('`')[1];\n                          : dict_key2 = instruct_key.split()[0];\n                      endif\n                      : purpose_dict = {'Purpose': item_response.strip()};\n                      if (dict_key1 not in self.code_qa_dict) then (yes)\n                          : self.code_qa_dict[dict_key1] = {};\n                      endif\n                      if (dict_key2) then (yes)\n                          if (not isinstance(self.code_qa_dict[dict_key1], dict)) then (yes)\n                              : self.code_qa_dict[dict_key1] = {};\n                          endif\n                          if (dict_key2 not in self.code_qa_dict[dict_key1] or not isinstance(self.code_qa_dict[dict_key1][dict_key2], dict)) then (yes)\n                              : self.code_qa_dict[dict_key1][dict_key2] = {'Value': self.code_qa_dict[dict_key1].get(dict_key2, '')};\n                          endif\n                          : self.code_qa_dict[dict_key1][dict_key2].update(purpose_dict);\n                      endif\n                  }\n              endif\n          endif\n          : code_qa_text = yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False).strip();\n          : code_qa_text = re.sub('\\\\n\\\\s*\\\\n', '\\n', code_qa_text);\n          : self.code_qa_response = code_qa_text;\n          : self.file_details['file_info']['code_qa_response'] = self.code_qa_response;\n          : self.file_details['file_info']['purpose'] = response;\n          : return;\n      : def get_code_qa(self);\n          : excluded_instructions = {'Call code graph', 'Docstring'};\n          : self.code_qa_list = [];\n          : code_objects_responses = {};\n          if (item in self.instruct_list) then (yes)\n              : instruction = item['instruction'].split(' in Python file:')[0];\n              : output = item['output'];\n              if (any((instruction.startswith(prefix) prefix in excluded_instructions))) then (yes)\n                  : continue;\n              endif\n              : self.code_qa_list.append({instruction: output});\n              if ('`' in instruction) then (yes)\n                  : code_object = instruction.split('`')[1];\n                  : code_type = instruction.split()[0];\n                  : code_objects_responses.setdefault(code_object, []).append((code_type, output));\n              endif\n          endif\n          : self.code_qa_dict = {};\n          if ((idx, (code_object, type_responses)) in enumerate(code_objects_responses.items(), start=1)) then (yes)\n              : self.code_qa_dict[code_object] = {};\n              if ((subidx, (code_type, response)) in enumerate(type_responses, start=1)) then (yes)\n                  if (code_object == code_type) then (yes)\n                      : self.code_qa_dict[code_object] = response;\n                  endif\n              endif\n          endif\n          : self.code_qa_response = yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False).strip();\n          : self.file_details['file_info']['code_qa_response'] = self.code_qa_response;\n      : def process_question(self, question_type: str, question_id: str, query: str, context: str, info: Dict);\n          : response = '';\n          if (question_id.endswith(('code_graph', 'docstring'))) then (yes)\n              : response = info.get(question_id, {});\n          endif\n          : response = str(response).strip();\n          if (response and response != 'None') then (yes)\n              : self.instruct_list.append({'instruction': query, 'input': context, 'output': response});\n          endif\n      : def get_info_string(info: Dict, item_type: str);\n          : return;\n      : def process_question_type(self, question_type: str, question_id: str, question_text: str);\n          if (question_type == 'file') then (yes)\n              : query = question_text.format(filename=self.base_name);\n              : info = self.file_details['file_info'];\n              : context = self.file_details['file_info']['file_code'];\n              : context = f'```python\\n{context}\\n```';\n              : self.process_question(question_type, question_id, query, context, info);\n          endif\n      : def generate(self);\n          if (question in self.questions) then (yes)\n              : self.process_question_type(question['type'], question['id'], question['text']);\n          endif\n          : self.instruct_list.sort(key=lambda x: len(x['input']), reverse=True);\n          : return;\nend\n@enduml"
    code_qa_response: "py2dataset/get_python_datasets copy.py:\n  Dependencies: logging, math, re, typing, yaml\n  Functions: clean_and_get_unique_elements, element_generator, get_python_datasets\n  Classes: DatasetGenerator\n  clean_and_get_unique_elements:\n    Inputs: input_str\n    Calls: enumerate, input_str.strip(\\'[]\\\\\\'\"\\').strip, input_str.strip, element.strip(\\'\\\\\\'\" \\').strip, element.strip, element_generator, .join\n    Variables: start, cleaned_elements, returned_elements, input_str\n    Returns: returned_elements\n  element_generator:\n    Inputs: input_str\n    Calls: enumerate\n    Variables: start, input_str\n  get_python_datasets:\n    Inputs: file_path, file_details, base_name, questions, model_config, detailed\n    Calls: DatasetGenerator(file_path, file_details, base_name, questions, model_config, detailed).generate, DatasetGenerator\n    Variables: model_config, questions, file_path, detailed, base_name, file_details\n    Returns: DatasetGenerator(file_path, file_details, base_name, questions, model_config, detailed).generate()\n  DatasetGenerator:\n    Methods: __init__, get_response_from_llm, get_code_qa, process_question, get_info_string, process_question_type, generate\n    Attributes: file_path, file_details, base_name, questions, model_config, instruct_list, question_mapping, code_qa_list, code_qa_response, llm, use_llm, detailed, llm, use_llm, detailed, code_qa_response, code_qa_list, code_qa_dict, code_qa_response\n  DatasetGenerator.__init__:\n    Inputs: self, file_path, file_details, base_name, questions, model_config, detailed\n    Variables: model_config, questions, file_path, detailed, self, base_name, file_details\n  DatasetGenerator.get_response_from_llm:\n    Inputs: self, query, context\n    Calls: '{}'.format, str, ```python\\\\n{}\\\\n```'.format, self.get_info_string, self.model_config['prompt_template'].format, strategy, prompt_template.format, len, self.llm.tokenize, logging.info, math.ceil, logging.error, re.sub, self.llm, response.replace, \\\\n'.join, line.lstrip, response.split, list, item.keys, item.values, self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'], instruction_prompt=self.model_config['instruction_prompt']).format, item_response.replace, item['instruction'].startswith, instruct_key.split, item_response.strip, isinstance, self.code_qa_dict[dict_key1].get, self.code_qa_dict[dict_key1][dict_key2].update, self.code_qa_dict[dict_key1].update, yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False).strip, yaml.dump, float'\n    Variables: context_strategies, context_size, instruct_key, prompt, code_qa_text, output, dict_key2, context, query, max_context_length, response, prompt_template, item_response, dict_key1, err_msg, self, instruct_value, purpose_dict\n    Returns: response\n  DatasetGenerator.get_code_qa:\n    Inputs: self\n    Calls: item['instruction'].split, any, instruction.startswith, self.code_qa_list.append, instruction.split, code_objects_responses.setdefault(code_object, []).append, code_objects_responses.setdefault, code_objects_responses.setdefault(instruction, []).append, enumerate, code_objects_responses.items, self.code_qa_dict.setdefault, yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False).strip, yaml.dump, float\n    Variables: output, code_type, instruction, excluded_instructions, self, code_object, code_objects_responses\n  DatasetGenerator.process_question:\n    Inputs: self, question_type, question_id, query, context, info\n    Calls: question_id.endswith, info.get, self.get_code_qa, self.get_response_from_llm, clean_and_get_unique_elements, str, str(response).strip, self.instruct_list.append\n    Variables: info, question_id, context, query, response, question_type, self\n  DatasetGenerator.get_info_string:\n    Inputs: info, item_type\n    Calls: .join, item.strip, str(info.get(item_type, )).split, str, info.get\n    Variables: info, item_type\n    Returns: .join([item.strip() for item in str(info.get(item_type, )).split(, ) if item])\n  DatasetGenerator.process_question_type:\n    Inputs: self, question_type, question_id, question_text\n    Calls: question_text.format, self.process_question, self.file_details['classes'].items, class_info.items, key.startswith, len, self.file_details[self.question_mapping[question_type]].items, self.get_info_string, .join, clean_and_get_unique_elements\n    Variables: question_text, variables, info, combined, question_id, methods_string, context, query, inputs, method_name, question_type, self, mapping\n  DatasetGenerator.generate:\n    Inputs: self\n    Calls: self.process_question_type, self.instruct_list.sort, len\n    Variables: self\n    Returns: self.instruct_list"
functions:
    clean_and_get_unique_elements:
        function_name: clean_and_get_unique_elements
        function_code: "def clean_and_get_unique_elements(input_str: str) -> str:\n    \"\"\"\n    Clean an input string (str) and return a string of unique elements.\n    Args:\n        input_str (str): The input string to be cleaned.\n    Returns:\n        str: The cleaned string.\n    \"\"\"\n\n    def element_generator(input_str):\n        start, brace_level = (0, 0)\n        for i, char in enumerate(input_str):\n            if char in '{}':\n                brace_level += 1 if char == '{' else -1\n            elif char == ',' and brace_level == 0:\n                yield input_str[start:i]\n                start = i + 1\n        yield input_str[start:]\n    input_str = input_str.strip('[]\\'\"').strip()\n    cleaned_elements = [element.strip('\\'\" ').strip() for element in element_generator(input_str) if element.strip()]\n    returned_elements = ', '.join(cleaned_elements)\n    return returned_elements"
        function_docstring: "\n    Clean an input string (str) and return a string of unique elements.\n    Args:\n        input_str (str): The input string to be cleaned.\n    Returns:\n        str: The cleaned string.\n    "
        function_inputs:
        - input_str
        function_defaults: []
        function_returns:
        - returned_elements
        function_calls:
        - enumerate
        - input_str.strip('[]\'"').strip
        - input_str.strip
        - element.strip('\'" ').strip
        - element.strip
        - element_generator
        - ''', ''.join'
        function_call_inputs:
            enumerate:
            - input_str
            input_str.strip('[]\'"').strip: []
            input_str.strip:
            - '''[]\''"'''
            element.strip('\'" ').strip: []
            element.strip:
            - '''\''" '''
            element_generator:
            - input_str
            ''', ''.join':
            - cleaned_elements
        function_variables:
        - start
        - cleaned_elements
        - returned_elements
        - input_str
        function_decorators: []
        function_annotations: []
        function_properties: []
    element_generator:
        function_name: element_generator
        function_code: "def element_generator(input_str):\n    start, brace_level = (0, 0)\n    for i, char in enumerate(input_str):\n        if char in '{}':\n            brace_level += 1 if char == '{' else -1\n        elif char == ',' and brace_level == 0:\n            yield input_str[start:i]\n            start = i + 1\n    yield input_str[start:]"
        function_docstring: null
        function_inputs:
        - input_str
        function_defaults: []
        function_returns: []
        function_calls:
        - enumerate
        function_call_inputs:
            enumerate:
            - input_str
        function_variables:
        - start
        - input_str
        function_decorators: []
        function_annotations: []
        function_properties: []
    get_python_datasets:
        function_name: get_python_datasets
        function_code: "def get_python_datasets(file_path: str, file_details: Dict, base_name: str, questions: List[Dict], model_config: Dict, detailed: bool) -> Tuple[List[Dict], List[Dict]]:\n    \"\"\"\n    Extract information from a Python file and return it in JSON format.\n    Args:\n        file_path (str): The path to the Python file.\n        file_details (Dict): The details of the file.\n        base_name (str): The base Python code filename.\n        questions (List[Dict]): The list of questions.\n        llm (object): The language model to be used for generating responses.\n        prompt (str): The prompt to be used for generating responses.\n        detailed (bool): Flag indicating if detailed information should be extracted.\n    Returns:\n        Tuple[List[Dict], List[Dict]]: Extracted information in JSON format.\n    \"\"\"\n    return DatasetGenerator(file_path, file_details, base_name, questions, model_config, detailed).generate()"
        function_docstring: "\n    Extract information from a Python file and return it in JSON format.\n    Args:\n        file_path (str): The path to the Python file.\n        file_details (Dict): The details of the file.\n        base_name (str): The base Python code filename.\n        questions (List[Dict]): The list of questions.\n        llm (object): The language model to be used for generating responses.\n        prompt (str): The prompt to be used for generating responses.\n        detailed (bool): Flag indicating if detailed information should be extracted.\n    Returns:\n        Tuple[List[Dict], List[Dict]]: Extracted information in JSON format.\n    "
        function_inputs:
        - file_path
        - file_details
        - base_name
        - questions
        - model_config
        - detailed
        function_defaults: []
        function_returns:
        - DatasetGenerator(file_path, file_details, base_name, questions, model_config, detailed).generate()
        function_calls:
        - DatasetGenerator(file_path, file_details, base_name, questions, model_config, detailed).generate
        - DatasetGenerator
        function_call_inputs:
            DatasetGenerator(file_path, file_details, base_name, questions, model_config, detailed).generate: []
            DatasetGenerator:
            - file_path
            - file_details
            - base_name
            - questions
            - model_config
            - detailed
        function_variables:
        - model_config
        - questions
        - file_path
        - detailed
        - base_name
        - file_details
        function_decorators: []
        function_annotations: []
        function_properties: []
classes:
    DatasetGenerator:
        class_name: DatasetGenerator
        class_code: "class DatasetGenerator:\n    \"\"\"\n    Generate JSON formatted dictionary outputs for a Python file.\n    Attributes:\n        file_path (str): The path to the Python file.\n        file_details (Dict[str, Any]): Details of the Python file.\n        base_name (str): The base name of the Python file.\n        questions (List[Dict[str, str]]): Questions for generating responses.\n        instruct_list (List[Dict[str, str]]): Storage for generated instructions.\n        question_mapping (Dict[str, str]): Mapping of question types to keys in file details.\n        use_llm (bool): Flag indicating if a language model should be used.\n        llm (object): The language model for generating responses.\n        prompt (str): The prompt format for querying the language model.\n    Methods:\n        get_response_from_llm(query: str, context: str) -> str:\n            Get language model response to query for given context.\n        process_question(question_type: str, question_id: str, query: str, context: str, info: Dict) -> None:\n            Process question and add the generated response to the instruct_list.\n        get_info_string(info, item_type) -> str:\n            Get string from info dictionary.\n        process_question_type(question_type: str, question_id: str, question_text: str) -> None:\n            Process questions related to a file, function, class, or method.\n        generate() -> Tuple[List[Dict], List[Dict]]:\n            Generate responses for all the questions and returns the instruct_list.\n    \"\"\"\n\n    def __init__(self, file_path: str, file_details: Dict, base_name: str, questions: List[Dict], model_config: Dict, detailed: bool) -> None:\n        \"\"\"\n        Initialize the DatasetGenerator class.\n        Args:\n            file_path (str): The path to the Python file.\n            file_details (Dict[str, Any]): Details of the Python file.\n            base_name (str): The base name of the Python file.\n            questions (List[Dict[str, str]]): Questions for generating responses.\n            model_config (Dict): Configuration for the language model.\n        Returns:\n            None\n        \"\"\"\n        self.file_path = file_path\n        self.file_details = file_details\n        self.base_name = base_name\n        self.questions = questions\n        self.model_config = model_config\n        if model_config is not None:\n            self.llm = model_config['model']\n            self.use_llm = True\n            self.detailed = detailed\n        else:\n            self.llm = None\n            self.use_llm = False\n            self.detailed = False\n        self.instruct_list = []\n        self.question_mapping = {'file': 'file', 'function': 'functions', 'class': 'classes', 'method': 'classes'}\n        self.code_qa_list = []\n        self.code_qa_response = ''\n\n    def get_response_from_llm(self, query: str, context: str) -> str:\n        \"\"\"\n        Get language model response to query for given context.\n        Args:\n            query (str): The query to be used for generating the response.\n            context (str): The context to be used for generating the response.\n        Returns:\n            str: The generated response.\n        \"\"\"\n        context_strategies = [lambda: '{}'.format(str(context)), lambda: '```python\\n{}\\n```'.format(str(self.file_details['file_info']['file_code_simplified'])), lambda: '```python\\n{}\\n```'.format(self.get_info_string(self.file_details['file_info'], 'file_summary')), lambda: '']\n        max_context_length = self.model_config['inference_model']['model_params']['context_length']\n        prompt_template = self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'], instruction_prompt=self.model_config['instruction_prompt'])\n        for strategy in context_strategies:\n            context = strategy()\n            prompt = prompt_template.format(context=context, query=query, code_objects=self.code_qa_response)\n            context_size = len(self.llm.tokenize(prompt))\n            logging.info('***Context Size: ' + str(context_size))\n            if context_size <= 0.7 * max_context_length:\n                break\n        else:\n            err_msg = f'Model response failed, increase py2dataset_model_config.yaml context_length > {math.ceil(context_size / 0.7)}'\n            logging.error(err_msg)\n            return ''\n        if context == '':\n            context = self.code_qa_list\n        response = ''\n        try:\n            response = re.sub('\\\\n\\\\s*\\\\n', '\\n\\n', self.llm(prompt))\n            response = response.replace('<|im_end|>', '')\n            response = response.replace('</|im_end|>', '')\n            response = '\\n'.join([line.lstrip() for line in response.split('\\n')])\n            logging.info(f'***Overall Response: {response}')\n        except Exception as error:\n            logging.error(f'Failed to generate model response: {error}')\n        if self.detailed:\n            for item in self.code_qa_list:\n                try:\n                    instruct_key = list(item.keys())[0]\n                    instruct_value = list(item.values())[0]\n                    query = f'Describe the Purpose and Significance of these {instruct_key}: [{instruct_value}] and Explain what each of these {instruct_key} does in the code.'\n                    prompt = self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'], instruction_prompt=self.model_config['instruction_prompt']).format(context=f'{context}/nCode Summary:/n{response}', query=f'{query}', code_objects=f'{instruct_value}')\n                    item_response = re.sub('\\\\n\\\\s*\\\\n', '\\n\\n', self.llm(prompt))\n                    item_response = item_response.replace('<|im_end|>', '')\n                    item_response = item_response.replace('</|im_end|>', '')\n                    logging.info(f'\\n***Itemized Response: {query}\\n{item_response}')\n                    for item in self.instruct_list:\n                        if item['instruction'].startswith(instruct_key):\n                            output = f'\\n\\nPurpose and Significance:\\n{item_response}'\n                            item['output'] += output\n                    if '`' in instruct_key:\n                        dict_key1 = instruct_key.split('`')[1]\n                        dict_key2 = instruct_key.split()[0]\n                    else:\n                        dict_key1 = instruct_key\n                        dict_key2 = ''\n                    purpose_dict = {'Purpose': item_response.strip()}\n                    if dict_key1 not in self.code_qa_dict:\n                        self.code_qa_dict[dict_key1] = {}\n                    elif not isinstance(self.code_qa_dict[dict_key1], dict):\n                        self.code_qa_dict[dict_key1] = {'Value': self.code_qa_dict[dict_key1]}\n                    if dict_key2:\n                        if not isinstance(self.code_qa_dict[dict_key1], dict):\n                            self.code_qa_dict[dict_key1] = {}\n                        if dict_key2 not in self.code_qa_dict[dict_key1] or not isinstance(self.code_qa_dict[dict_key1][dict_key2], dict):\n                            self.code_qa_dict[dict_key1][dict_key2] = {'Value': self.code_qa_dict[dict_key1].get(dict_key2, '')}\n                        self.code_qa_dict[dict_key1][dict_key2].update(purpose_dict)\n                    else:\n                        self.code_qa_dict[dict_key1].update(purpose_dict)\n                except Exception as error:\n                    logging.error(f'Failed to generate model response: {error}')\n        code_qa_text = yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False).strip()\n        code_qa_text = re.sub('\\\\n\\\\s*\\\\n', '\\n', code_qa_text)\n        self.code_qa_response = code_qa_text\n        self.file_details['file_info']['code_qa_response'] = self.code_qa_response\n        self.file_details['file_info']['purpose'] = response\n        return response\n\n    def get_code_qa(self):\n        \"\"\"\n        Get code responses from the instruct_list and update:\n            code_qa_list (List[Dict]): List of code question-answer pairs.\n            code_qa_response (str): structured text for code question-answer pairs.\n        \"\"\"\n        excluded_instructions = {'Call code graph', 'Docstring'}\n        self.code_qa_list = []\n        code_objects_responses = {}\n        for item in self.instruct_list:\n            instruction = item['instruction'].split(' in Python file:')[0]\n            output = item['output']\n            if any((instruction.startswith(prefix) for prefix in excluded_instructions)):\n                continue\n            self.code_qa_list.append({instruction: output})\n            if '`' in instruction:\n                code_object = instruction.split('`')[1]\n                code_type = instruction.split()[0]\n                code_objects_responses.setdefault(code_object, []).append((code_type, output))\n            else:\n                code_objects_responses.setdefault(instruction, []).append((instruction, output))\n        self.code_qa_dict = {}\n        for idx, (code_object, type_responses) in enumerate(code_objects_responses.items(), start=1):\n            self.code_qa_dict[code_object] = {}\n            for subidx, (code_type, response) in enumerate(type_responses, start=1):\n                if code_object == code_type:\n                    self.code_qa_dict[code_object] = response\n                else:\n                    self.code_qa_dict.setdefault(code_object, {})[code_type] = response\n        self.code_qa_response = yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False).strip()\n        self.file_details['file_info']['code_qa_response'] = self.code_qa_response\n\n    def process_question(self, question_type: str, question_id: str, query: str, context: str, info: Dict) -> None:\n        \"\"\"\n        Process question and add the generated response to the instruct_list.\n        Args:\n            question_type (str): The type of question to be processed.\n            question_id (str): The ID of the question to be processed.\n            query (str): The query to be processed.\n            context (str): The context to be used for generating the response.\n            info (Dict): The information of the Python file.\n        Returns:\n            None\n        \"\"\"\n        response = ''\n        if question_id.endswith(('code_graph', 'docstring')):\n            response = info.get(question_id, {})\n        elif question_id.endswith('file_purpose'):\n            self.get_code_qa()\n            if self.use_llm:\n                response = self.get_response_from_llm(query, context)\n        else:\n            response = clean_and_get_unique_elements(str(info.get(question_id, '')))\n        response = str(response).strip()\n        if response and response != 'None':\n            self.instruct_list.append({'instruction': query, 'input': context, 'output': response})\n\n    @staticmethod\n    def get_info_string(info: Dict, item_type: str) -> str:\n        \"\"\"\n        Get string from info dictionary.\n        Args:\n            info (Dict): The information of the Python file.\n            item_type (str): The type of item to get the string from.\n        Returns:\n            str: The string from the info.\n        \"\"\"\n        return ', '.join([item.strip() for item in str(info.get(item_type, '')).split(',') if item])\n\n    def process_question_type(self, question_type: str, question_id: str, question_text: str) -> None:\n        \"\"\"\n        Process questions related to a file, function, class, or method.\n        Args:\n            question_type (str): The type of question to be processed.\n            question_id (str): The ID of the question to be processed.\n            question_text (str): The text of the question to be processed.\n        Returns:\n            None\n        \"\"\"\n        if question_type == 'file':\n            query = question_text.format(filename=self.base_name)\n            info = self.file_details['file_info']\n            context = self.file_details['file_info']['file_code']\n            context = f'```python\\n{context}\\n```'\n            self.process_question(question_type, question_id, query, context, info)\n        elif question_type == 'method':\n            for class_name, class_info in self.file_details['classes'].items():\n                for key, method_info in class_info.items():\n                    if key.startswith('class_method_'):\n                        method_name = f\"{class_name}.{key[len('class_method_'):]}\"\n                        context = method_info['method_code']\n                        context = f'```python\\n{context}\\n```'\n                        mapping = {'class_name': class_name, 'method_name': method_name}\n                        query = question_text.format(filename=self.base_name, **mapping)\n                        self.process_question(question_type, question_id, query, context, method_info)\n        else:\n            for name, info in self.file_details[self.question_mapping[question_type]].items():\n                context = info[f'{question_type}_code']\n                context = f'```python\\n{context}\\n```'\n                mapping = {f'{question_type}_name': name}\n                if question_id == f'{question_type}_purpose' and self.use_llm:\n                    variables = self.get_info_string(info, f'{question_type}_variables')\n                    inputs = self.get_info_string(info, f'{question_type}_inputs')\n                    combined = ', '.join([s for s in [variables, inputs] if s])\n                    mapping[f'{question_type}_variables'] = clean_and_get_unique_elements(combined)\n                    if question_type == 'class':\n                        methods_string = self.get_info_string(info, f'{question_type}_methods')\n                        mapping[f'{question_type}_methods'] = methods_string\n                query = question_text.format(filename=self.base_name, **mapping)\n                self.process_question(question_type, question_id, query, context, info)\n\n    def generate(self) -> Tuple[List[Dict], List[Dict]]:\n        \"\"\"\n        Generate responses for all the questions and returns the instruct_list.\n        Args:\n            None\n        Returns:\n            Tuple[List[Dict], List[Dict]]:  .\n        \"\"\"\n        for question in self.questions:\n            self.process_question_type(question['type'], question['id'], question['text'])\n        self.instruct_list.sort(key=lambda x: len(x['input']), reverse=True)\n        return self.instruct_list"
        class_docstring: "\n    Generate JSON formatted dictionary outputs for a Python file.\n    Attributes:\n        file_path (str): The path to the Python file.\n        file_details (Dict[str, Any]): Details of the Python file.\n        base_name (str): The base name of the Python file.\n        questions (List[Dict[str, str]]): Questions for generating responses.\n        instruct_list (List[Dict[str, str]]): Storage for generated instructions.\n        question_mapping (Dict[str, str]): Mapping of question types to keys in file details.\n        use_llm (bool): Flag indicating if a language model should be used.\n        llm (object): The language model for generating responses.\n        prompt (str): The prompt format for querying the language model.\n    Methods:\n        get_response_from_llm(query: str, context: str) -> str:\n            Get language model response to query for given context.\n        process_question(question_type: str, question_id: str, query: str, context: str, info: Dict) -> None:\n            Process question and add the generated response to the instruct_list.\n        get_info_string(info, item_type) -> str:\n            Get string from info dictionary.\n        process_question_type(question_type: str, question_id: str, question_text: str) -> None:\n            Process questions related to a file, function, class, or method.\n        generate() -> Tuple[List[Dict], List[Dict]]:\n            Generate responses for all the questions and returns the instruct_list.\n    "
        class_inputs: null
        class_defaults: null
        class_returns:
        - response
        - ''', ''.join([item.strip() for item in str(info.get(item_type, '''')).split('','') if item])'
        - self.instruct_list
        - ''''''
        class_calls:
        - '''{}''.format'
        - str
        - '''```python\n{}\n```''.format'
        - self.get_info_string
        - self.model_config['prompt_template'].format
        - strategy
        - prompt_template.format
        - len
        - self.llm.tokenize
        - logging.info
        - math.ceil
        - logging.error
        - re.sub
        - self.llm
        - response.replace
        - '''\n''.join'
        - line.lstrip
        - response.split
        - list
        - item.keys
        - item.values
        - self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'], instruction_prompt=self.model_config['instruction_prompt']).format
        - item_response.replace
        - item['instruction'].startswith
        - instruct_key.split
        - item_response.strip
        - isinstance
        - self.code_qa_dict[dict_key1].get
        - self.code_qa_dict[dict_key1][dict_key2].update
        - self.code_qa_dict[dict_key1].update
        - yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False).strip
        - yaml.dump
        - float
        - item['instruction'].split
        - any
        - instruction.startswith
        - self.code_qa_list.append
        - instruction.split
        - code_objects_responses.setdefault(code_object, []).append
        - code_objects_responses.setdefault
        - code_objects_responses.setdefault(instruction, []).append
        - enumerate
        - code_objects_responses.items
        - self.code_qa_dict.setdefault
        - question_id.endswith
        - info.get
        - self.get_code_qa
        - self.get_response_from_llm
        - clean_and_get_unique_elements
        - str(response).strip
        - self.instruct_list.append
        - ''', ''.join'
        - item.strip
        - str(info.get(item_type, '')).split
        - question_text.format
        - self.process_question
        - self.file_details['classes'].items
        - class_info.items
        - key.startswith
        - self.file_details[self.question_mapping[question_type]].items
        - self.process_question_type
        - self.instruct_list.sort
        class_call_inputs:
            '''{}''.format':
            - str(context)
            str:
            - context
            - self.file_details['file_info']['file_code_simplified']
            - context_size
            - info.get(question_id, '')
            - response
            - info.get(item_type, '')
            '''```python\n{}\n```''.format':
            - str(self.file_details['file_info']['file_code_simplified'])
            - self.get_info_string(self.file_details['file_info'], 'file_summary')
            self.get_info_string:
            - self.file_details['file_info']
            - '''file_summary'''
            - info
            - f'{question_type}_variables'
            - info
            - f'{question_type}_inputs'
            - info
            - f'{question_type}_methods'
            self.model_config['prompt_template'].format: []
            strategy: []
            prompt_template.format: []
            len:
            - self.llm.tokenize(prompt)
            - '''class_method_'''
            - x['input']
            self.llm.tokenize:
            - prompt
            logging.info:
            - '''***Context Size: '' + str(context_size)'
            - 'f''***Overall Response: {response}'''
            - 'f''\n***Itemized Response: {query}\n{item_response}'''
            math.ceil:
            - context_size / 0.7
            logging.error:
            - err_msg
            - 'f''Failed to generate model response: {error}'''
            - 'f''Failed to generate model response: {error}'''
            re.sub:
            - '''\\n\\s*\\n'''
            - '''\n\n'''
            - self.llm(prompt)
            - '''\\n\\s*\\n'''
            - '''\n\n'''
            - self.llm(prompt)
            - '''\\n\\s*\\n'''
            - '''\n'''
            - code_qa_text
            self.llm:
            - prompt
            - prompt
            response.replace:
            - '''<|im_end|>'''
            - ''''''
            - '''</|im_end|>'''
            - ''''''
            '''\n''.join':
            - '[line.lstrip() for line in response.split(''\n'')]'
            line.lstrip: []
            response.split:
            - '''\n'''
            list:
            - item.keys()
            - item.values()
            item.keys: []
            item.values: []
            ? self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'], instruction_prompt=self.model_config['instruction_prompt']).format
            : []
            item_response.replace:
            - '''<|im_end|>'''
            - ''''''
            - '''</|im_end|>'''
            - ''''''
            item['instruction'].startswith:
            - instruct_key
            instruct_key.split:
            - '''`'''
            item_response.strip: []
            isinstance:
            - self.code_qa_dict[dict_key1]
            - dict
            - self.code_qa_dict[dict_key1]
            - dict
            - self.code_qa_dict[dict_key1][dict_key2]
            - dict
            self.code_qa_dict[dict_key1].get:
            - dict_key2
            - ''''''
            self.code_qa_dict[dict_key1][dict_key2].update:
            - purpose_dict
            self.code_qa_dict[dict_key1].update:
            - purpose_dict
            yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False).strip: []
            yaml.dump:
            - self.code_qa_dict
            - self.code_qa_dict
            float:
            - '''inf'''
            - '''inf'''
            item['instruction'].split:
            - ''' in Python file:'''
            any:
            - (instruction.startswith(prefix) for prefix in excluded_instructions)
            instruction.startswith:
            - prefix
            self.code_qa_list.append:
            - '{instruction: output}'
            instruction.split:
            - '''`'''
            code_objects_responses.setdefault(code_object, []).append:
            - (code_type, output)
            code_objects_responses.setdefault:
            - code_object
            - '[]'
            - instruction
            - '[]'
            code_objects_responses.setdefault(instruction, []).append:
            - (instruction, output)
            enumerate:
            - code_objects_responses.items()
            - type_responses
            code_objects_responses.items: []
            self.code_qa_dict.setdefault:
            - code_object
            - '{}'
            question_id.endswith:
            - ('code_graph', 'docstring')
            - '''file_purpose'''
            info.get:
            - question_id
            - '{}'
            - question_id
            - ''''''
            - item_type
            - ''''''
            self.get_code_qa: []
            self.get_response_from_llm:
            - query
            - context
            clean_and_get_unique_elements:
            - str(info.get(question_id, ''))
            - combined
            str(response).strip: []
            self.instruct_list.append:
            - '{''instruction'': query, ''input'': context, ''output'': response}'
            ''', ''.join':
            - '[item.strip() for item in str(info.get(item_type, '''')).split('','') if item]'
            - '[s for s in [variables, inputs] if s]'
            item.strip: []
            str(info.get(item_type, '')).split:
            - ''','''
            question_text.format: []
            self.process_question:
            - question_type
            - question_id
            - query
            - context
            - info
            - question_type
            - question_id
            - query
            - context
            - method_info
            - question_type
            - question_id
            - query
            - context
            - info
            self.file_details['classes'].items: []
            class_info.items: []
            key.startswith:
            - '''class_method_'''
            self.file_details[self.question_mapping[question_type]].items: []
            self.process_question_type:
            - question['type']
            - question['id']
            - question['text']
            self.instruct_list.sort: []
        class_variables:
        - context_size
        - output
        - dict_key2
        - combined
        - max_context_length
        - response
        - instruction
        - method_name
        - instruct_value
        - code_object
        - code_type
        - prompt_template
        - excluded_instructions
        - item_response
        - variables
        - context_strategies
        - code_qa_text
        - context
        - query
        - dict_key1
        - purpose_dict
        - mapping
        - prompt
        - info
        - methods_string
        - inputs
        - err_msg
        - instruct_key
        - code_objects_responses
        class_decorators: []
        class_annotations: []
        class_properties:
        - self.questions
        - self.code_qa_list
        - self.base_name
        - self.instruct_list
        - self.model_config
        - self.code_qa_dict
        - self.llm
        - self.code_qa_response
        - self.detailed
        - self.file_path
        - self.file_details
        - self.use_llm
        - self.question_mapping
        class_attributes:
        - file_path
        - file_details
        - base_name
        - questions
        - model_config
        - instruct_list
        - question_mapping
        - code_qa_list
        - code_qa_response
        - llm
        - use_llm
        - detailed
        - llm
        - use_llm
        - detailed
        - code_qa_response
        - code_qa_list
        - code_qa_dict
        - code_qa_response
        class_methods:
        - __init__
        - get_response_from_llm
        - get_code_qa
        - process_question
        - get_info_string
        - process_question_type
        - generate
        class_inheritance: []
        class_static_methods:
        - get_info_string
        class_method___init__:
            method_name: __init__
            method_code: "def __init__(self, file_path: str, file_details: Dict, base_name: str, questions: List[Dict], model_config: Dict, detailed: bool) -> None:\n    \"\"\"\n        Initialize the DatasetGenerator class.\n        Args:\n            file_path (str): The path to the Python file.\n            file_details (Dict[str, Any]): Details of the Python file.\n            base_name (str): The base name of the Python file.\n            questions (List[Dict[str, str]]): Questions for generating responses.\n            model_config (Dict): Configuration for the language model.\n        Returns:\n            None\n        \"\"\"\n    self.file_path = file_path\n    self.file_details = file_details\n    self.base_name = base_name\n    self.questions = questions\n    self.model_config = model_config\n    if model_config is not None:\n        self.llm = model_config['model']\n        self.use_llm = True\n        self.detailed = detailed\n    else:\n        self.llm = None\n        self.use_llm = False\n        self.detailed = False\n    self.instruct_list = []\n    self.question_mapping = {'file': 'file', 'function': 'functions', 'class': 'classes', 'method': 'classes'}\n    self.code_qa_list = []\n    self.code_qa_response = ''"
            method_docstring: "\n        Initialize the DatasetGenerator class.\n        Args:\n            file_path (str): The path to the Python file.\n            file_details (Dict[str, Any]): Details of the Python file.\n            base_name (str): The base name of the Python file.\n            questions (List[Dict[str, str]]): Questions for generating responses.\n            model_config (Dict): Configuration for the language model.\n        Returns:\n            None\n        "
            method_inputs:
            - self
            - file_path
            - file_details
            - base_name
            - questions
            - model_config
            - detailed
            method_defaults: []
            method_returns: []
            method_calls: []
            method_call_inputs: {}
            method_variables:
            - model_config
            - questions
            - file_path
            - detailed
            - self
            - base_name
            - file_details
            method_decorators: []
            method_annotations: []
            method_properties:
            - self.questions
            - self.code_qa_list
            - self.base_name
            - self.instruct_list
            - self.model_config
            - self.llm
            - self.code_qa_response
            - self.detailed
            - self.file_path
            - self.file_details
            - self.use_llm
            - self.question_mapping
        class_method_get_response_from_llm:
            method_name: get_response_from_llm
            method_code: "def get_response_from_llm(self, query: str, context: str) -> str:\n    \"\"\"\n        Get language model response to query for given context.\n        Args:\n            query (str): The query to be used for generating the response.\n            context (str): The context to be used for generating the response.\n        Returns:\n            str: The generated response.\n        \"\"\"\n    context_strategies = [lambda: '{}'.format(str(context)), lambda: '```python\\n{}\\n```'.format(str(self.file_details['file_info']['file_code_simplified'])), lambda: '```python\\n{}\\n```'.format(self.get_info_string(self.file_details['file_info'], 'file_summary')), lambda: '']\n    max_context_length = self.model_config['inference_model']['model_params']['context_length']\n    prompt_template = self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'], instruction_prompt=self.model_config['instruction_prompt'])\n    for strategy in context_strategies:\n        context = strategy()\n        prompt = prompt_template.format(context=context, query=query, code_objects=self.code_qa_response)\n        context_size = len(self.llm.tokenize(prompt))\n        logging.info('***Context Size: ' + str(context_size))\n        if context_size <= 0.7 * max_context_length:\n            break\n    else:\n        err_msg = f'Model response failed, increase py2dataset_model_config.yaml context_length > {math.ceil(context_size / 0.7)}'\n        logging.error(err_msg)\n        return ''\n    if context == '':\n        context = self.code_qa_list\n    response = ''\n    try:\n        response = re.sub('\\\\n\\\\s*\\\\n', '\\n\\n', self.llm(prompt))\n        response = response.replace('<|im_end|>', '')\n        response = response.replace('</|im_end|>', '')\n        response = '\\n'.join([line.lstrip() for line in response.split('\\n')])\n        logging.info(f'***Overall Response: {response}')\n    except Exception as error:\n        logging.error(f'Failed to generate model response: {error}')\n    if self.detailed:\n        for item in self.code_qa_list:\n            try:\n                instruct_key = list(item.keys())[0]\n                instruct_value = list(item.values())[0]\n                query = f'Describe the Purpose and Significance of these {instruct_key}: [{instruct_value}] and Explain what each of these {instruct_key} does in the code.'\n                prompt = self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'], instruction_prompt=self.model_config['instruction_prompt']).format(context=f'{context}/nCode Summary:/n{response}', query=f'{query}', code_objects=f'{instruct_value}')\n                item_response = re.sub('\\\\n\\\\s*\\\\n', '\\n\\n', self.llm(prompt))\n                item_response = item_response.replace('<|im_end|>', '')\n                item_response = item_response.replace('</|im_end|>', '')\n                logging.info(f'\\n***Itemized Response: {query}\\n{item_response}')\n                for item in self.instruct_list:\n                    if item['instruction'].startswith(instruct_key):\n                        output = f'\\n\\nPurpose and Significance:\\n{item_response}'\n                        item['output'] += output\n                if '`' in instruct_key:\n                    dict_key1 = instruct_key.split('`')[1]\n                    dict_key2 = instruct_key.split()[0]\n                else:\n                    dict_key1 = instruct_key\n                    dict_key2 = ''\n                purpose_dict = {'Purpose': item_response.strip()}\n                if dict_key1 not in self.code_qa_dict:\n                    self.code_qa_dict[dict_key1] = {}\n                elif not isinstance(self.code_qa_dict[dict_key1], dict):\n                    self.code_qa_dict[dict_key1] = {'Value': self.code_qa_dict[dict_key1]}\n                if dict_key2:\n                    if not isinstance(self.code_qa_dict[dict_key1], dict):\n                        self.code_qa_dict[dict_key1] = {}\n                    if dict_key2 not in self.code_qa_dict[dict_key1] or not isinstance(self.code_qa_dict[dict_key1][dict_key2], dict):\n                        self.code_qa_dict[dict_key1][dict_key2] = {'Value': self.code_qa_dict[dict_key1].get(dict_key2, '')}\n                    self.code_qa_dict[dict_key1][dict_key2].update(purpose_dict)\n                else:\n                    self.code_qa_dict[dict_key1].update(purpose_dict)\n            except Exception as error:\n                logging.error(f'Failed to generate model response: {error}')\n    code_qa_text = yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False).strip()\n    code_qa_text = re.sub('\\\\n\\\\s*\\\\n', '\\n', code_qa_text)\n    self.code_qa_response = code_qa_text\n    self.file_details['file_info']['code_qa_response'] = self.code_qa_response\n    self.file_details['file_info']['purpose'] = response\n    return response"
            method_docstring: "\n        Get language model response to query for given context.\n        Args:\n            query (str): The query to be used for generating the response.\n            context (str): The context to be used for generating the response.\n        Returns:\n            str: The generated response.\n        "
            method_inputs:
            - self
            - query
            - context
            method_defaults: []
            method_returns:
            - response
            - ''''''
            method_calls:
            - '''{}''.format'
            - str
            - '''```python\n{}\n```''.format'
            - self.get_info_string
            - self.model_config['prompt_template'].format
            - strategy
            - prompt_template.format
            - len
            - self.llm.tokenize
            - logging.info
            - math.ceil
            - logging.error
            - re.sub
            - self.llm
            - response.replace
            - '''\n''.join'
            - line.lstrip
            - response.split
            - list
            - item.keys
            - item.values
            - self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'], instruction_prompt=self.model_config['instruction_prompt']).format
            - item_response.replace
            - item['instruction'].startswith
            - instruct_key.split
            - item_response.strip
            - isinstance
            - self.code_qa_dict[dict_key1].get
            - self.code_qa_dict[dict_key1][dict_key2].update
            - self.code_qa_dict[dict_key1].update
            - yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False).strip
            - yaml.dump
            - float
            method_call_inputs:
                '''{}''.format':
                - str(context)
                str:
                - context
                - self.file_details['file_info']['file_code_simplified']
                - context_size
                '''```python\n{}\n```''.format':
                - str(self.file_details['file_info']['file_code_simplified'])
                - self.get_info_string(self.file_details['file_info'], 'file_summary')
                self.get_info_string:
                - self.file_details['file_info']
                - '''file_summary'''
                self.model_config['prompt_template'].format: []
                strategy: []
                prompt_template.format: []
                len:
                - self.llm.tokenize(prompt)
                self.llm.tokenize:
                - prompt
                logging.info:
                - '''***Context Size: '' + str(context_size)'
                - 'f''***Overall Response: {response}'''
                - 'f''\n***Itemized Response: {query}\n{item_response}'''
                math.ceil:
                - context_size / 0.7
                logging.error:
                - err_msg
                - 'f''Failed to generate model response: {error}'''
                - 'f''Failed to generate model response: {error}'''
                re.sub:
                - '''\\n\\s*\\n'''
                - '''\n\n'''
                - self.llm(prompt)
                - '''\\n\\s*\\n'''
                - '''\n\n'''
                - self.llm(prompt)
                - '''\\n\\s*\\n'''
                - '''\n'''
                - code_qa_text
                self.llm:
                - prompt
                - prompt
                response.replace:
                - '''<|im_end|>'''
                - ''''''
                - '''</|im_end|>'''
                - ''''''
                '''\n''.join':
                - '[line.lstrip() for line in response.split(''\n'')]'
                line.lstrip: []
                response.split:
                - '''\n'''
                list:
                - item.keys()
                - item.values()
                item.keys: []
                item.values: []
                ? self.model_config['prompt_template'].format(system_prompt=self.model_config['system_prompt'], instruction_prompt=self.model_config['instruction_prompt']).format
                : []
                item_response.replace:
                - '''<|im_end|>'''
                - ''''''
                - '''</|im_end|>'''
                - ''''''
                item['instruction'].startswith:
                - instruct_key
                instruct_key.split:
                - '''`'''
                item_response.strip: []
                isinstance:
                - self.code_qa_dict[dict_key1]
                - dict
                - self.code_qa_dict[dict_key1]
                - dict
                - self.code_qa_dict[dict_key1][dict_key2]
                - dict
                self.code_qa_dict[dict_key1].get:
                - dict_key2
                - ''''''
                self.code_qa_dict[dict_key1][dict_key2].update:
                - purpose_dict
                self.code_qa_dict[dict_key1].update:
                - purpose_dict
                yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False).strip: []
                yaml.dump:
                - self.code_qa_dict
                float:
                - '''inf'''
            method_variables:
            - context_strategies
            - context_size
            - instruct_key
            - prompt
            - code_qa_text
            - output
            - dict_key2
            - context
            - query
            - max_context_length
            - response
            - prompt_template
            - item_response
            - dict_key1
            - err_msg
            - self
            - instruct_value
            - purpose_dict
            method_decorators: []
            method_annotations: []
            method_properties:
            - self.code_qa_response
        class_method_get_code_qa:
            method_name: get_code_qa
            method_code: "def get_code_qa(self):\n    \"\"\"\n        Get code responses from the instruct_list and update:\n            code_qa_list (List[Dict]): List of code question-answer pairs.\n            code_qa_response (str): structured text for code question-answer pairs.\n        \"\"\"\n    excluded_instructions = {'Call code graph', 'Docstring'}\n    self.code_qa_list = []\n    code_objects_responses = {}\n    for item in self.instruct_list:\n        instruction = item['instruction'].split(' in Python file:')[0]\n        output = item['output']\n        if any((instruction.startswith(prefix) for prefix in excluded_instructions)):\n            continue\n        self.code_qa_list.append({instruction: output})\n        if '`' in instruction:\n            code_object = instruction.split('`')[1]\n            code_type = instruction.split()[0]\n            code_objects_responses.setdefault(code_object, []).append((code_type, output))\n        else:\n            code_objects_responses.setdefault(instruction, []).append((instruction, output))\n    self.code_qa_dict = {}\n    for idx, (code_object, type_responses) in enumerate(code_objects_responses.items(), start=1):\n        self.code_qa_dict[code_object] = {}\n        for subidx, (code_type, response) in enumerate(type_responses, start=1):\n            if code_object == code_type:\n                self.code_qa_dict[code_object] = response\n            else:\n                self.code_qa_dict.setdefault(code_object, {})[code_type] = response\n    self.code_qa_response = yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False).strip()\n    self.file_details['file_info']['code_qa_response'] = self.code_qa_response"
            method_docstring: "\n        Get code responses from the instruct_list and update:\n            code_qa_list (List[Dict]): List of code question-answer pairs.\n            code_qa_response (str): structured text for code question-answer pairs.\n        "
            method_inputs:
            - self
            method_defaults: []
            method_returns: []
            method_calls:
            - item['instruction'].split
            - any
            - instruction.startswith
            - self.code_qa_list.append
            - instruction.split
            - code_objects_responses.setdefault(code_object, []).append
            - code_objects_responses.setdefault
            - code_objects_responses.setdefault(instruction, []).append
            - enumerate
            - code_objects_responses.items
            - self.code_qa_dict.setdefault
            - yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False).strip
            - yaml.dump
            - float
            method_call_inputs:
                item['instruction'].split:
                - ''' in Python file:'''
                any:
                - (instruction.startswith(prefix) for prefix in excluded_instructions)
                instruction.startswith:
                - prefix
                self.code_qa_list.append:
                - '{instruction: output}'
                instruction.split:
                - '''`'''
                code_objects_responses.setdefault(code_object, []).append:
                - (code_type, output)
                code_objects_responses.setdefault:
                - code_object
                - '[]'
                - instruction
                - '[]'
                code_objects_responses.setdefault(instruction, []).append:
                - (instruction, output)
                enumerate:
                - code_objects_responses.items()
                - type_responses
                code_objects_responses.items: []
                self.code_qa_dict.setdefault:
                - code_object
                - '{}'
                yaml.dump(self.code_qa_dict, Dumper=yaml.SafeDumper, width=float('inf'), sort_keys=False, default_flow_style=False).strip: []
                yaml.dump:
                - self.code_qa_dict
                float:
                - '''inf'''
            method_variables:
            - output
            - code_type
            - instruction
            - excluded_instructions
            - self
            - code_object
            - code_objects_responses
            method_decorators: []
            method_annotations: []
            method_properties:
            - self.code_qa_dict
            - self.code_qa_list
            - self.code_qa_response
        class_method_process_question:
            method_name: process_question
            method_code: "def process_question(self, question_type: str, question_id: str, query: str, context: str, info: Dict) -> None:\n    \"\"\"\n        Process question and add the generated response to the instruct_list.\n        Args:\n            question_type (str): The type of question to be processed.\n            question_id (str): The ID of the question to be processed.\n            query (str): The query to be processed.\n            context (str): The context to be used for generating the response.\n            info (Dict): The information of the Python file.\n        Returns:\n            None\n        \"\"\"\n    response = ''\n    if question_id.endswith(('code_graph', 'docstring')):\n        response = info.get(question_id, {})\n    elif question_id.endswith('file_purpose'):\n        self.get_code_qa()\n        if self.use_llm:\n            response = self.get_response_from_llm(query, context)\n    else:\n        response = clean_and_get_unique_elements(str(info.get(question_id, '')))\n    response = str(response).strip()\n    if response and response != 'None':\n        self.instruct_list.append({'instruction': query, 'input': context, 'output': response})"
            method_docstring: "\n        Process question and add the generated response to the instruct_list.\n        Args:\n            question_type (str): The type of question to be processed.\n            question_id (str): The ID of the question to be processed.\n            query (str): The query to be processed.\n            context (str): The context to be used for generating the response.\n            info (Dict): The information of the Python file.\n        Returns:\n            None\n        "
            method_inputs:
            - self
            - question_type
            - question_id
            - query
            - context
            - info
            method_defaults: []
            method_returns: []
            method_calls:
            - question_id.endswith
            - info.get
            - self.get_code_qa
            - self.get_response_from_llm
            - clean_and_get_unique_elements
            - str
            - str(response).strip
            - self.instruct_list.append
            method_call_inputs:
                question_id.endswith:
                - ('code_graph', 'docstring')
                - '''file_purpose'''
                info.get:
                - question_id
                - '{}'
                - question_id
                - ''''''
                self.get_code_qa: []
                self.get_response_from_llm:
                - query
                - context
                clean_and_get_unique_elements:
                - str(info.get(question_id, ''))
                str:
                - info.get(question_id, '')
                - response
                str(response).strip: []
                self.instruct_list.append:
                - '{''instruction'': query, ''input'': context, ''output'': response}'
            method_variables:
            - info
            - question_id
            - context
            - query
            - response
            - question_type
            - self
            method_decorators: []
            method_annotations: []
            method_properties: []
        class_method_get_info_string:
            method_name: get_info_string
            method_code: "@staticmethod\ndef get_info_string(info: Dict, item_type: str) -> str:\n    \"\"\"\n        Get string from info dictionary.\n        Args:\n            info (Dict): The information of the Python file.\n            item_type (str): The type of item to get the string from.\n        Returns:\n            str: The string from the info.\n        \"\"\"\n    return ', '.join([item.strip() for item in str(info.get(item_type, '')).split(',') if item])"
            method_docstring: "\n        Get string from info dictionary.\n        Args:\n            info (Dict): The information of the Python file.\n            item_type (str): The type of item to get the string from.\n        Returns:\n            str: The string from the info.\n        "
            method_inputs:
            - info
            - item_type
            method_defaults: []
            method_returns:
            - ''', ''.join([item.strip() for item in str(info.get(item_type, '''')).split('','') if item])'
            method_calls:
            - ''', ''.join'
            - item.strip
            - str(info.get(item_type, '')).split
            - str
            - info.get
            method_call_inputs:
                ''', ''.join':
                - '[item.strip() for item in str(info.get(item_type, '''')).split('','') if item]'
                item.strip: []
                str(info.get(item_type, '')).split:
                - ''','''
                str:
                - info.get(item_type, '')
                info.get:
                - item_type
                - ''''''
            method_variables:
            - info
            - item_type
            method_decorators:
            - staticmethod
            method_annotations: []
            method_properties: []
        class_method_process_question_type:
            method_name: process_question_type
            method_code: "def process_question_type(self, question_type: str, question_id: str, question_text: str) -> None:\n    \"\"\"\n        Process questions related to a file, function, class, or method.\n        Args:\n            question_type (str): The type of question to be processed.\n            question_id (str): The ID of the question to be processed.\n            question_text (str): The text of the question to be processed.\n        Returns:\n            None\n        \"\"\"\n    if question_type == 'file':\n        query = question_text.format(filename=self.base_name)\n        info = self.file_details['file_info']\n        context = self.file_details['file_info']['file_code']\n        context = f'```python\\n{context}\\n```'\n        self.process_question(question_type, question_id, query, context, info)\n    elif question_type == 'method':\n        for class_name, class_info in self.file_details['classes'].items():\n            for key, method_info in class_info.items():\n                if key.startswith('class_method_'):\n                    method_name = f\"{class_name}.{key[len('class_method_'):]}\"\n                    context = method_info['method_code']\n                    context = f'```python\\n{context}\\n```'\n                    mapping = {'class_name': class_name, 'method_name': method_name}\n                    query = question_text.format(filename=self.base_name, **mapping)\n                    self.process_question(question_type, question_id, query, context, method_info)\n    else:\n        for name, info in self.file_details[self.question_mapping[question_type]].items():\n            context = info[f'{question_type}_code']\n            context = f'```python\\n{context}\\n```'\n            mapping = {f'{question_type}_name': name}\n            if question_id == f'{question_type}_purpose' and self.use_llm:\n                variables = self.get_info_string(info, f'{question_type}_variables')\n                inputs = self.get_info_string(info, f'{question_type}_inputs')\n                combined = ', '.join([s for s in [variables, inputs] if s])\n                mapping[f'{question_type}_variables'] = clean_and_get_unique_elements(combined)\n                if question_type == 'class':\n                    methods_string = self.get_info_string(info, f'{question_type}_methods')\n                    mapping[f'{question_type}_methods'] = methods_string\n            query = question_text.format(filename=self.base_name, **mapping)\n            self.process_question(question_type, question_id, query, context, info)"
            method_docstring: "\n        Process questions related to a file, function, class, or method.\n        Args:\n            question_type (str): The type of question to be processed.\n            question_id (str): The ID of the question to be processed.\n            question_text (str): The text of the question to be processed.\n        Returns:\n            None\n        "
            method_inputs:
            - self
            - question_type
            - question_id
            - question_text
            method_defaults: []
            method_returns: []
            method_calls:
            - question_text.format
            - self.process_question
            - self.file_details['classes'].items
            - class_info.items
            - key.startswith
            - len
            - self.file_details[self.question_mapping[question_type]].items
            - self.get_info_string
            - ''', ''.join'
            - clean_and_get_unique_elements
            method_call_inputs:
                question_text.format: []
                self.process_question:
                - question_type
                - question_id
                - query
                - context
                - info
                - question_type
                - question_id
                - query
                - context
                - method_info
                - question_type
                - question_id
                - query
                - context
                - info
                self.file_details['classes'].items: []
                class_info.items: []
                key.startswith:
                - '''class_method_'''
                len:
                - '''class_method_'''
                self.file_details[self.question_mapping[question_type]].items: []
                self.get_info_string:
                - info
                - f'{question_type}_variables'
                - info
                - f'{question_type}_inputs'
                - info
                - f'{question_type}_methods'
                ''', ''.join':
                - '[s for s in [variables, inputs] if s]'
                clean_and_get_unique_elements:
                - combined
            method_variables:
            - question_text
            - variables
            - info
            - combined
            - question_id
            - methods_string
            - context
            - query
            - inputs
            - method_name
            - question_type
            - self
            - mapping
            method_decorators: []
            method_annotations: []
            method_properties: []
        class_method_generate:
            method_name: generate
            method_code: "def generate(self) -> Tuple[List[Dict], List[Dict]]:\n    \"\"\"\n        Generate responses for all the questions and returns the instruct_list.\n        Args:\n            None\n        Returns:\n            Tuple[List[Dict], List[Dict]]:  .\n        \"\"\"\n    for question in self.questions:\n        self.process_question_type(question['type'], question['id'], question['text'])\n    self.instruct_list.sort(key=lambda x: len(x['input']), reverse=True)\n    return self.instruct_list"
            method_docstring: "\n        Generate responses for all the questions and returns the instruct_list.\n        Args:\n            None\n        Returns:\n            Tuple[List[Dict], List[Dict]]:  .\n        "
            method_inputs:
            - self
            method_defaults: []
            method_returns:
            - self.instruct_list
            method_calls:
            - self.process_question_type
            - self.instruct_list.sort
            - len
            method_call_inputs:
                self.process_question_type:
                - question['type']
                - question['id']
                - question['text']
                self.instruct_list.sort: []
                len:
                - x['input']
            method_variables:
            - self
            method_decorators: []
            method_annotations: []
            method_properties: []
