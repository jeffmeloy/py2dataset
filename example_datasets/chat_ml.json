[
    {
        "conversation": [
            {
                "from": "system",
                "value": "code documentation: 1) Description: The given Python file 'get_code_graph.py' aims to analyze source code details extracted from a file summarized as dictionary format, generate a directed graph representing relationships among functions and classes within it, extract control flow tree from Abstract Syntax Tree (AST), reorganize the control flow structure according to the generated graph, and produce PlantUML activity diagram code for visualization purposes. It consists of several interconnected functions that perform specific tasks related to creating a code graph, parsing AST nodes for control flow extraction, restructuring control flow based on the graph, and generating PlantUML representation from the extracted elements.\n\n2) API Signatures:\n- `code_graph(file_summary: Dict[str, Union[Dict, str]] -> Dict[str, Union[List[str], Dict[str, List[str]]]:` Extracts function and class method details from the file summary dictionary, creates a directed graph representing code relationships using NetworkX DiGraph, adds nodes and edges with related data. Returns a dictionary containing nodes and edges representing the code structure.\n- `get_edge_data_from_details(target_details: dict, source_details: dict, target: str) -> dict`: Extracts edge data from details of targeted and source objects based on their information retrieved during graph construction.\n- `add_edge_with_data(source: str, target: str, init_method: Optional[str] = None) -> None`: Adds edges to the graph with associated data between nodes in the code graph.\n- `add_edges_for_calls(source_name: str, calls: List[Any]) -> None`: Iterates through function definitions and class method calls adding them as nodes and edges if applicable.\n- `extract_control_flow_tree(nodes: List[ast.AST]) -> List[Union[str, dict]]`: Parses AST nodes to create a control flow tree structure representation of the code logic.\n- `reorganize_control_flow(code_graph: Dict[str, Union[List[str], Dict[str, List[str]], control_flow_structure) -> List[dict]]`: Gets starting points from the code graph and recursively rearranges the control flow structure to match it.\n- `get_plantUML_element(element, indentation=\"\") -> str`: Generates PlantUML code for each element in the control flow structure with given indentation level.\n- `get_plantUML(control_flow_structure) -> str`: Produces overall PlantUML activity diagram code from the reorganized control flow structure.\n- `get_code_graph(file_summary: Dict[str, Union[Dict, str], file_ast) -> Tuple[Dict[str, List[dict], List[dict], str]`: Combines entire code graph creation with control flow extraction and PlantUML generation.\n\n3) Functional Requirements:\n- [req01] `code_graph()` creates a lookup dictionary for function and class method details from file summary, builds a directed graph with nodes representing classes/functions as sources/targets of method calls adding edges with data where needed; generates node list and edge list for later usage.\n- [req02] `extract_control_flow_tree()` traverses AST nodes extracting code structure information in terms of 'if', 'while', 'try-catch' blocks, etc., producing control flow tree elements.\n- [req03] `reorganize_control_flow()` locates starting points from the generated graph and restructures control flow elements accordingly.\n- [req04] `get_plantUML_element()` generates PlantUML code for each element recursively with indentation levels, forming activity diagrams for visualization purposes.\n- [req05] `get_plantUML()` collects all PlantUML codes from reorganized control flow structure elements and concatenates them into a single string representation.\n- [req06] `get_code_graph()` invokes code graph creation, control flow extraction using AST traversal, control flow reorganization, and PlantUML generation altogether returning results as tuples.\n\n4) Variables Explanation:\n- `function_details_lookup`, `class_method_details_lookup`: Dictionaries storing function/class method details for graph construction.\n- `G`: NetworkX DiGraph object representing the code relationships.\n- `nodes`, `edges`: Lists containing nodes and edges in the generated graph.\n- `source_name`, `target`, `init_method`: Identifiers referring to function/class names for edge building.\n- `starting_points`, `control_flow_tree`, `code_graph`, `plantUML`, `element`, `indentation`: Used in different functions for specific purposes related to graph traversal, control flow manipulation and PlantUML generation.\n- `node_keywords_map`: Mapping Python keywords to their respective PlantUML representations."
            },
            {
                "from": "human",
                "value": "Output the Python code described by the code documentation."
            },
            {
                "from": "gpt",
                "value": "```python\n\"\"\"\nRequirements\n[req01] The code_graph function performs the following:\n        a. Extracts the function and class method details from the file summary.\n        b. Creates a lookup dictionary for function and class method details.\n        c. Creates a directed graph with nodes and edges representing the relationships in the code.\n        d. Adds edges for function and class method calls.\n        e. Adds edge data to edges.\n        f. Returns a dictionary with nodes and edges representing the relationships in the code.\n[req02] The extract_control_flow_tree function performs the following:\n        a. Extracts control flow tree from AST.\n        b. Returns control flow tree.\n[req03] The reorganize_control_flow function performs the following:\n        a. Gets starting points from the code graph.\n        b. Reorganizes the control flow structure recursively.\n        c. Returns reorganized control flow structure.\n[req04] The get_plantUML_element function performs the following:\n        a. Gets plantUML code for each element.\n        b. Returns plantUML code for each element.\n[req05] The get_plantUML function performs the following: \n        a. Gets plantUML code for entire file.\n        b. Returns plantUML code for entire file.\n[req06] The get_code_graph function performs the following:\n        a. Gets the entire code graph\n        b. Gets the control flow structure\n        c. Gets the plantUML code\n        d. Returns entire code graph, control flow structure and plantUML code\n\"\"\"\nimport ast\nimport json\nfrom typing import Dict, List, Optional, Union\nimport networkx as nx\n\n\ndef code_graph(\n    file_summary: Dict[str, Union[Dict, str]],\n) -> Dict[str, Union[List[str], Dict[str, List[str]]]]:\n    \"\"\"\n    Create a dictionary representation of file details.\n    Args:\n        file_summary: Dict[str, Union[Dict, str]]: The details extracted from the file.\n    Returns:\n        dict: A dictionary with nodes and edges representing the relationships in the code.\n    \"\"\"\n    G = nx.DiGraph()\n\n    # Create lookup dictionaries for function and class method details\n    function_details_lookup = {}\n    for function_def in file_summary[\"function_defs\"]:\n        function_details_lookup.update(function_def)\n    class_method_details_lookup = {}\n    for class_def in file_summary[\"class_defs\"]:\n        for (\n            class_name,\n            class_details,\n        ) in class_def.items():  # Extract class name and details\n            G.add_node(class_name)  # Add class as a graph node\n            for method_name, method_details in class_details[\"method_defs\"].items():\n                qualified_method_name = (\n                    f\"{class_name}.{method_name}\"  # Create method fully qualified name\n                )\n                G.add_node(qualified_method_name)  # Add method as a graph node\n                class_method_details_lookup[\n                    qualified_method_name\n                ] = method_details  # Store method details\n                G.add_edge(\n                    class_name, qualified_method_name\n                )  # Add edge from class to method\n\n    # Helper function to extract edge data from target details\n    def get_edge_data_from_details(\n        target_details: dict, source_details: dict, target: str\n    ) -> dict:\n        edge_data = {}\n        if target_details:\n            edge_data[\"target_inputs\"] = target_details.get(\"inputs\")\n            edge_data[\"target_returns\"] = list(set(target_details.get(\"returns\", [])))\n        if (\n            source_details\n            and \"call_inputs\" in source_details\n            and target in source_details[\"call_inputs\"]\n        ):\n            edge_data[\"target_inputs\"] = source_details[\"call_inputs\"][target]\n        return edge_data\n\n    # Helper function to add edge with data\n    def add_edge_with_data(\n        source: str, target: str, init_method: Optional[str] = None\n    ) -> None:\n        target_details = class_method_details_lookup.get(\n            init_method or target\n        ) or function_details_lookup.get(target)\n        source_details = function_details_lookup.get(\n            source\n        ) or class_method_details_lookup.get(source)\n        G.add_edge(\n            source,\n            target,\n            **get_edge_data_from_details(target_details, source_details, target),\n        )\n\n    # Helper function to add edges for function or class method calls\n    def add_edges_for_calls(source_name, calls):\n        class_names = [\n            list(class_def.keys())[0] for class_def in file_summary[\"class_defs\"]\n        ]\n        for called in calls:\n            called_class_name = called.split(\".\")[0]\n            if called.startswith(\"self.\"):\n                method_name = called.replace(\"self.\", \"\")\n                fully_qualified_name = f\"{source_name.split('.')[0]}.{method_name}\"\n                if fully_qualified_name in class_method_details_lookup:\n                    add_edge_with_data(source_name, fully_qualified_name)\n                    continue\n            if (\n                called in function_details_lookup\n                or called in class_method_details_lookup\n                or f\"{source_name.split('.')[0]}.{called}\"\n                in class_method_details_lookup\n            ):\n                add_edge_with_data(source_name, called)\n            elif called_class_name in class_names:\n                init_method = None\n                init_method_name = f\"{called}.__init__\"\n                if init_method_name in class_method_details_lookup:\n                    init_method = init_method_name\n                add_edge_with_data(source_name, called, init_method)\n            else:\n                G.add_node(called)\n                add_edge_with_data(source_name, called)\n\n    # Add function nodes to graph and edges for function calls\n    for function_name in function_details_lookup:\n        G.add_node(function_name)\n    for func_name, details in function_details_lookup.items():\n        add_edges_for_calls(func_name, details[\"calls\"])\n\n    # Add edges for method calls\n    for qualified_method_name, details in class_method_details_lookup.items():\n        add_edges_for_calls(qualified_method_name, details[\"calls\"])\n\n    # Add edge data to edges and create node and edges to return\n    for edge in G.edges:\n        source, target = edge\n        target_details = function_details_lookup.get(\n            target\n        ) or class_method_details_lookup.get(target)\n        source_details = function_details_lookup.get(\n            source\n        ) or class_method_details_lookup.get(source)\n        edge_data = get_edge_data_from_details(target_details, source_details, target)\n        G[source][target].update(edge_data)\n    nodes = list(G.nodes)\n    edges = [\n        {\"source\": edge[0], \"target\": edge[1], **edge[2]} for edge in G.edges.data()\n    ]\n\n    # remove any nodes that are not either a source of an edge or a target of an edge\n    nodes_to_remove = []\n    for node in nodes:\n        if node not in [edge[\"source\"] for edge in edges] and node not in [\n            edge[\"target\"] for edge in edges\n        ]:\n            nodes_to_remove.append(node)\n\n    return {\"nodes\": nodes, \"edges\": edges}\n\n\ndef extract_control_flow_tree(nodes: List[ast.AST]) -> List[Union[str, dict]]:\n    \"\"\"\n    Extract control flow tree from AST.\n    Args:\n        nodes: AST nodes\n    Returns:\n        control_flow_tree: control flow tree\n    \"\"\"\n    control_flow_tree = []\n    node_keywords_map = {\n        ast.FunctionDef: \"def\",\n        ast.AsyncFunctionDef: \"def\",\n        ast.ClassDef: \"class\",\n        ast.While: \"while\",\n        ast.For: \"for\",\n        ast.AsyncFor: \"for\",\n        ast.With: \"with\",\n        ast.AsyncWith: \"with\",\n        ast.Return: \"return\",\n        ast.If: \"if\",\n        ast.Try: \"try\",\n    }\n\n    for node in nodes:\n        node_type = type(node)\n        if node_type not in node_keywords_map:\n            control_flow_tree.append(ast.unparse(node))\n            continue\n        keyword = node_keywords_map[node_type]\n\n        if keyword == \"def\":\n            args_str = \", \".join([ast.unparse(arg) for arg in node.args.args])\n            key = f\"def {node.name}({args_str})\"\n            value = extract_control_flow_tree(node.body)\n            control_flow_tree.append({key: value})\n        elif keyword == \"class\":\n            key = f\"class {node.name}\"\n            value = extract_control_flow_tree(node.body)\n            control_flow_tree.append({key: value})\n        elif keyword == \"while\":\n            key = f\"while {ast.unparse(node.test)}\"\n            value = extract_control_flow_tree(node.body)\n            control_flow_tree.append({key: value})\n        elif keyword == \"for\":\n            key = f\"for {ast.unparse(node.target)} in {ast.unparse(node.iter)}\"\n            value = extract_control_flow_tree(node.body)\n            control_flow_tree.append({key: value})\n        elif keyword == \"with\":\n            key = f\"with {', '.join([ast.unparse(item) for item in node.items])}\"\n            value = extract_control_flow_tree(node.body)\n            control_flow_tree.append({key: value})\n        elif keyword == \"return\":\n            key = \"return\"\n            value = [ast.unparse(node.value)] if node.value is not None else []\n            control_flow_tree.append({key: value})\n        elif keyword == \"if\":\n            key = f\"if {ast.unparse(node.test)}\"\n            value = extract_control_flow_tree(node.body)\n            if_block = {key: value}\n            orelse = node.orelse\n            while orelse and isinstance(orelse[0], ast.If):\n                key = f\"elif {ast.unparse(orelse[0].test)}\"\n                value = extract_control_flow_tree(orelse[0].body)\n                if_block[key] = value\n                orelse = orelse[0].orelse\n            if orelse:\n                key = \"else\"\n                value = extract_control_flow_tree(orelse)\n                if_block[key] = value\n            control_flow_tree.append(if_block)\n        elif keyword == \"try\":\n            try_block = extract_control_flow_tree(node.body)\n            except_block = []\n            for handler in node.handlers:\n                h_type = ast.unparse(handler.type) if handler.type is not None else \"\"\n                h_name = (\n                    ast.unparse(handler.name)\n                    if isinstance(handler.name, ast.Name)\n                    else \"\"\n                )\n                key = f\"except {h_type} as {h_name}:\"\n                value = extract_control_flow_tree(handler.body)\n                except_block.append({key: value})\n            control_flow_dict = {\"try\": try_block, \"except\": except_block}\n            if node.orelse:\n                else_block = extract_control_flow_tree(node.orelse)\n                control_flow_dict[\"else\"] = else_block\n            if node.finalbody:\n                finally_block = extract_control_flow_tree(node.finalbody)\n                control_flow_dict[\"finally\"] = finally_block\n            control_flow_tree.append(control_flow_dict)\n        else:\n            control_flow_tree.append(ast.unparse(node))\n\n    return control_flow_tree\n\n\ndef reorganize_control_flow(code_graph, control_flow_structure):\n    \"\"\"\n    Reorganize control flow structure to match the code graph.\n    Args:\n        file_details: file details\n        control_flow_structure: control flow structure\n    Returns:\n        reorganized_control_flow_structure: reorganized control flow structure\n    \"\"\"\n    # Get starting points from the code graph as those\n    targets = [edge[\"target\"] for edge in code_graph[\"edges\"]]\n    starting_points = [node for node in code_graph[\"nodes\"] if node not in targets]\n\n    # Define a function to reorganize the structure recursively\n    def reorganize_structure(structure, start_points):\n        organized, seen = [], set()\n\n        # Iterate through each start point and find matching elements in structure\n        for start in start_points:\n            for element in structure:\n                if isinstance(element, dict):\n                    key = next(iter(element))  # Get the first key of the dictionary\n                    if (\n                        start in key\n                    ):  # Add element if it matches the start point and hasn't been seen\n                        element_id = json.dumps(element)\n                        if element_id not in seen:\n                            organized.append(element)\n                            seen.add(element_id)\n                elif (\n                    isinstance(element, str) and start in element\n                ):  # Handle string elements\n                    organized.append(element)\n\n        # Append elements not included in the organized list\n        remaining = [elem for elem in structure if json.dumps(elem) not in seen]\n        organized.extend(remaining)\n        return organized\n\n    # Reorganize the control flow structure recursively\n    return reorganize_structure(control_flow_structure, starting_points)\n\n\ndef get_plantUML_element(element, indentation=\"\"):\n    \"\"\"\n    Get plantUML code for each element.\n    Args:\n        element: element\n        indentation: current indentation level\n    Returns:\n        plantuml_str: plantUML code for each element\n    \"\"\"\n    plantuml_str = \"\"\n    if isinstance(element, dict):\n        key = next(iter(element))\n        value = element[key]\n        if key.startswith(\"def \") or key.startswith(\"async def \"):\n            plantuml_str += f\"{indentation}def {value} {{\\n\"\n            inner_indentation = indentation + \"  \"\n        elif key.startswith(\"class \"):\n            plantuml_str += f\"{indentation}class {value} {{\\n\"\n            inner_indentation = indentation + \"  \"\n        elif key.startswith(\"if \"):\n            plantuml_str += f\"{indentation}if ({value}) {{\\n\"\n            inner_indentation = indentation + \"  \"\n        elif (\n            key.startswith(\"for \")\n            or key.startswith(\"while \")\n            or key.startswith(\"asyncfor \")\n        ):\n            plantuml_str += f\"{indentation}while ({value}) {{\\n\"\n            inner_indentation = indentation + \"  \"\n        elif key.startswith(\"try \"):\n            plantuml_str += f\"{indentation}try {{\\n\"\n            inner_indentation = indentation + \"  \"\n        elif key.startswith(\"except \"):\n            plantuml_str += f\"{indentation}catch ({value}) {{\\n\"\n            inner_indentation = indentation + \"  \"\n        else:\n            plantuml_str += f\"{indentation}:{key};\\n\"\n            inner_indentation = indentation\n\n        if isinstance(value, list):\n            for child in value:\n                plantuml_str += get_plantUML_element(child, inner_indentation)\n\n        if key.startswith(\"def \") or key.startswith(\"class \"):\n            plantuml_str += f\"{indentation}}}\\n\"\n        elif (\n            key.startswith(\"if \")\n            or key.startswith(\"for \")\n            or key.startswith(\"while \")\n            or key.startswith(\"asyncfor \")\n            or key.startswith(\"try \")\n            or key.startswith(\"except \")\n        ):\n            plantuml_str += f\"{indentation}}}\\n\"\n\n    elif isinstance(element, str):\n        plantuml_str += f\"{indentation}:{element};\\n\"\n\n    return plantuml_str\n\n\ndef get_plantUML(control_flow_structure):\n    \"\"\"\n    Get plantUML activity diagram code for entire file.\n    Args:\n        file_details: file details\n    Returns:\n        plantuml_str: plantUML code for entire file\n    \"\"\"\n    plantuml_str = \"@startuml\\n\"\n    for element in control_flow_structure:\n        plantuml_str += get_plantUML_element(element, \"  \")\n    plantuml_str += \"end\\n@enduml\"\n    return plantuml_str\n\n\ndef get_code_graph(file_summary, file_ast):\n    \"\"\"\n    Add code graph and control flow to file details.\n    Args:\n        file_details: file details\n    Returns:\n        file_details: file details\n    \"\"\"\n    try:\n        entire_code_graph = code_graph(file_summary)\n        control_flow_tree = extract_control_flow_tree(file_ast.body)\n        control_flow_structure = reorganize_control_flow(\n            entire_code_graph, control_flow_tree\n        )\n        plantUML = get_plantUML(control_flow_structure)\n    except Exception as e:\n        control_flow_structure = [str(e)]\n        plantUML = str(e)\n\n    #print('file_summary:', file_summary)\n    #print('entire_code_graph:', entire_code_graph)\n    #print('control_flow_structure:', control_flow_structure)\n    #print('plantUML:', plantUML)\n\n    return entire_code_graph, control_flow_structure, plantUML\n\n```"
            }
        ],
        "nbytes": 21054,
        "source": "py2dataset/get_code_graph.py"
    },
    {
        "conversation": [
            {
                "from": "system",
                "value": "code documentation: 1. get_start_dir - Purpose: Returns an appropriate start directory based on user input or defaults to current working directory. It validates if the given path exists or creates it if necessary.\n- Inputs: start_dir (optional string)\n- Calls: Path().is_dir(), Path(), os.getcwd(), os.path.abspath()\n- Variables: start_dir\n- Returns: Absolute path of provided start_dir or current working directory depending on its validity\n\n2. get_output_dir - Purpose: Determines the output directory where files will be saved during program execution. If a specific path is given, it ensures its existence and accessibility. Otherwise defaults to 'OUTPUT_DIR'.\n- Inputs: output_dir (optional string)\n- Calls: os.path.abspath(), os.makedirs(), logging.info()\n- Variables: output_dir\n- Returns: Absolute path of provided output_dir or default OUTPUT_DIR depending on its validity\n\n3. get_questions - Purpose: Retrieves a list of questions from either user-specified file or defaults to hardcoded ones in JSON format.\n- Inputs: questions_pathname (optional string)\n- Calls: os.path.join(), open(), json.load(), logging.info(), get_default_questions()\n- Variables: questions_pathname, questions\n- Returns: Question list in JSON format retrieved from user input file or hardcoded values\n\n4. instantiate_model - Purpose: Instantiates a Language Model using parameters from provided model configuration dictionary and relevant imports. Handles importing the model module/class dynamically.\n- Inputs: model_config (dictionary)\n- Calls: model_config['model_import_path'].rsplit(), getattr(), importlib.import_module(), model_params.pop(), logging.info()\n- Variables: ModelClass, inference_function_name, model_params, llm\n- Returns: Instantiated Language Model object or None if error occurs during instantiation\n\n5. get_model - Purpose: Loads a model configuration from user-specified file or defaults to hardcoded values and instantiates the Language Model using it.\n- Inputs: model_config_pathname (optional string)\n- Calls: os.path.join(), open(), yaml.safe_load(), logging.info(), get_default_model_config(), instantiate_model()\n- Variables: model_config_pathname, model_config\n- Returns: Model configuration dictionary with an instantiated Language Model object and prompt template string\n\n6. write_questions_file - Purpose: Writes a file with question definitions stored either as input from a given filepath in JSON format or the hardcoded values default questions_filename variable in Python file named \"py2dataset_questions.json\" of the same folder,\nindentation being applied as 4 spaces.\n- Inputs: output_dir (optional string)\n- Calls: get_default_questions(), Path().is_dir(), os.getcwd(), os.path.join(), open(), json.dump()\n- Variables: output_dir, questions\n- Returns: None\n\n7. write_model_config_file - Purpose: Writes model configuration to a file named \"py2dataset_model_config.yaml\" or default path in Python file with indentation applied as 4 spaces in YAML format.\n- Inputs: output_dir (optional string)\n- Calls: get_default_model_config(), Path().is_dir(), os.getcwd(), os.path.join(), open(), yaml.dump()\n- Variables: output_dir, model_config\n- Returns: None\n\n8. get_default_questions - Purpose: Provides a predefined list of questions used for software auditing purposes in Python files.\n- Variables: questions (list)\n- Returns: List of default questions as defined constants\n\n9. get_default_model_config - Purpose: Defines the default configuration parameters for the Language Model utilized in the program including its prompt template structure.\n- Variables: model_config (dictionary)\n- Returns: Default model configuration dictionary"
            },
            {
                "from": "human",
                "value": "Output the Python code described by the code documentation."
            },
            {
                "from": "gpt",
                "value": "```python\n\"\"\"\nObtain data parameter and model from the py2dataset functions.\nRequirements:\n[req01] The `get_default_questions` function shall:\n        a. Return a list of default questions.\n        b. Ensure each question in the list is a dictionary.\n        c. Ensure each dictionary has the keys: id, text, and type.\n[req02] The `get_default_model_config` function shall:\n        a. Return a dictionary representing the default model configuration.\n[req03] The `get_output_dir` function shall:\n        a. Accept an optional output_dir argument.\n        b. Return the absolute path of the provided output_dir if it exists or can be created.\n        c. Return the default OUTPUT_DIR if the provided output_dir argument is not provided or invalid.\n[req04] The `get_questions` function shall:\n        a. Accept an optional questions_pathname argument.\n        b. Validate the question file provided by the questions_pathname.\n        c. Return the questions from the provided questions_pathname if valid.\n        d. Return default questions if the questions_pathname is not provided or invalid.\n[req05] The `instantiate_model` function shall:\n        a. Accept a model_config dictionary as an argument.\n        b. Import the specified module and class from the model_config.\n        c. Instantiate and return the model using the provided configuration.\n[req06] The `get_model` function shall:\n        a. Accept an optional model_config_pathname argument.\n        b. Validate the model config file provided by the model_config_pathname.\n        c. Return an instantiated model and a prompt template based on the provided configuration.\n        d. Return an instantiated model and a prompt template based on the default model configuration if the model_config_pathname is not provided or invalid.\n[req07] The `write_questions_file` function shall:\n        a. Accept an optional output_dir argument.\n        b. Write the default questions to the QUESTIONS_FILE in the specified directory.\n        c. Write the default questions to the QUESTIONS_FILE in the current working directory if the output_dir argument is not provided or invalid.\n[req08] The `write_model_config_file` function shall:\n        a. Accept an optional output_dir argument.\n        b. Write the default model configuration to the MODEL_CONFIG_FILE in the specified directory.\n        c. Write the default model configuration to the MODEL_CONFIG_FILE in the current working directory if the output_dir argument is not provided or invalid.\n\"\"\"\nimport os\nimport json\nimport logging\nimport importlib\nfrom typing import Dict, List\nfrom pathlib import Path\nimport yaml\n\n# Setting up a basic logger\nlogging.basicConfig(level=logging.INFO)\n\n# defaults if provided inputs fail\nQUESTIONS_FILE = \"py2dataset_questions.json\"\nMODEL_CONFIG_FILE = \"py2dataset_model_config.yaml\"\nOUTPUT_DIR = \"datasets\"\n\n\ndef get_default_questions() -> List[Dict]:\n    \"\"\"Return default question list\n    Args:\n        None\n    Returns:\n        List[Dict]: The default question list\n    \"\"\"\n    questions = [\n        {\n            \"id\": \"file_dependencies\",\n            \"text\": \"Dependencies in Python file: `{filename}`?\",\n            \"type\": \"file\",\n        },\n        {\n            \"id\": \"entire_code_graph\",\n            \"text\": \"Call code graph in Python file: `{filename}`?\",\n            \"type\": \"file\",\n        },\n        {\n            \"id\": \"file_functions\",\n            \"text\": \"Functions in Python file: `{filename}`?\",\n            \"type\": \"file\",\n        },\n        {\n            \"id\": \"file_classes\",\n            \"text\": \"Classes in Python file: `{filename}`?\",\n            \"type\": \"file\",\n        },\n        {\n            \"id\": \"function_inputs\",\n            \"text\": \"Inputs to `{function_name}` in Python file: `{filename}`?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"function_docstring\",\n            \"text\": \"Docstring of `{function_name}` in Python file: `{filename}`?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"function_calls\",\n            \"text\": \"Calls made in `{function_name}` in Python file: `{filename}`?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"function_variables\",\n            \"text\": \"Variables in `{function_name}` in Python file: `{filename}`?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"function_returns\",\n            \"text\": \"Returns from `{function_name}` in Python file: `{filename}`?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"class_methods\",\n            \"text\": \"Methods in `{class_name}` in Python file: `{filename}`?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"class_docstring\",\n            \"text\": \"Docstring of `{class_name}` in Python file: `{filename}`?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"class_attributes\",\n            \"text\": \"Attributes of `{class_name}` in Python file: `{filename}`?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"class_inheritance\",\n            \"text\": \"Inheritance of `{class_name}` in Python file: `{filename}`?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"method_inputs\",\n            \"text\": \"Inputs to `{method_name}` in Python file: `{filename}`?\",\n            \"type\": \"method\",\n        },\n        {\n            \"id\": \"method_docstring\",\n            \"text\": \"Docstring of `{method_name}` in Python file: `{filename}`?\",\n            \"type\": \"method\",\n        },\n        {\n            \"id\": \"method_calls\",\n            \"text\": \"Calls made in `{method_name}` in Python file: `{filename}`?\",\n            \"type\": \"method\",\n        },\n        {\n            \"id\": \"method_variables\",\n            \"text\": \"Variables in `{method_name}` in Python file: `{filename}`?\",\n            \"type\": \"method\",\n        },\n        {\n            \"id\": \"method_returns\",\n            \"text\": \"Returns from `{method_name}` in Python file: `{filename}`?\",\n            \"type\": \"method\",\n        },\n        {\n            \"id\": \"file_purpose\",\n            \"text\": \"1) Describe the Purpose and Processing summary of Python file: `{filename}`; 2) Summarize the Significance of applicable Function, Class, and Method; 3) Explain what each Input, Output, and Variable does in the code.\",\n            \"type\": \"file\",\n        },\n    ]\n    return questions\n\n\ndef get_default_model_config() -> Dict:\n    \"\"\"Return default model config dict\n    Args:\n        None\n    Returns:\n        Dict: The default model config dictionary\n    \"\"\"\n    model_config = {\n        \"system_prompt\": \"Provide complete structured response for a formal software audit, given this Context:\\n'{context}'\\n\",\n        \"instruction_prompt\": \"\\nPlease provide a very detailed, accurate, and insightful Response to this Instruction and include your reasoning step by step.\\n{query}\\n\",\n        \"prompt_template\": \"{system_prompt} ### Instruction: {instruction_prompt} ### Response: \",\n        \"inference_model\": {\n            \"model_import_path\": \"ctransformers.AutoModelForCausalLM\",\n            \"model_inference_function\": \"from_pretrained\",\n            \"model_params\": {\n                \"model_path\": \"TheBloke/WizardCoder-Python-13B-V1.0-GGUF\",\n                \"model_file\": \"wizardcoder-python-13b-v1.0.Q5_K_S.gguf\",\n                \"model_type\": \"llama\",\n                \"local_files_only\": False,\n                ## MODEL CONFIGURATION PARAMETERS (set for current model with: GPU 4090-24GB VRAM, CPU 5950x-32 threads, 64GB RAM)\n                # avx2 and gpu_layers are not compatible\n                # \"lib\": \"avx2\",\n                \"threads\": 16,\n                \"batch_size\": 128,\n                \"context_length\": 12000,\n                \"max_new_tokens\": 12000,\n                \"gpu_layers\": 100,\n                \"reset\": True,\n            },\n        },\n    }\n    return model_config\n\n\ndef get_start_dir(start_dir: str = \"\") -> str:\n    \"\"\"\n    Returns the appropriate start directory.\n    Args:\n        start_dir (str): The directory to start the search from.\n    Returns:\n        str: The absolute path of the provided start_dir if it exists or can be created.\n    \"\"\"\n    if start_dir and not Path(start_dir).is_dir():\n        logging.info(f\"Setting Start Dir : {start_dir}\")\n        start_dir = os.getcwd()\n    else:\n        start_dir = os.path.abspath(start_dir)\n    return start_dir\n\n\ndef get_output_dir(output_dir: str = \"\") -> str:\n    \"\"\"Returns the appropriate output directory.\n    Args:\n        output_dir (str): The directory to write the output to.\n    Returns:\n        str: The absolute path of the provided output_dir if it exists or can be created.\n    \"\"\"\n    output_dir = os.path.abspath(output_dir or OUTPUT_DIR)\n    os.makedirs(output_dir, exist_ok=True)\n    logging.info(f\"Using output directory: {output_dir}\")\n    return output_dir\n\n\ndef get_questions(questions_pathname: str) -> List[Dict]:\n    \"\"\"\n    Get questions from file or default\n    Args:\n        questions_pathname (str): The pathname of the questions file\n    Returns:\n        List[Dict]: The list of questions\n    \"\"\"\n    try:  # get questions from provided or default configuration file\n        if not questions_pathname:\n            questions_pathname = os.path.join(os.getcwd(), QUESTIONS_FILE)\n        with open(questions_pathname, \"r\") as f:\n            questions = json.load(f)\n        logging.info(f\"Using questions from file: {questions_pathname}\")\n    except (FileNotFoundError, json.decoder.JSONDecodeError):\n        logging.info(\n            f\"Questions file not valid: {questions_pathname} Using default questions\"\n        )\n        questions = get_default_questions()\n    return questions\n\n\ndef instantiate_model(model_config: Dict) -> object:\n    \"\"\"\n    Imports and instantiates a model based on the provided configuration.\n    Args:\n        model_config (dict): model configuration dictionary.\n    Returns:\n        object: An instance of the specified model class, or None if error.\n    \"\"\"\n    try:\n        module_name, class_name = model_config[\"model_import_path\"].rsplit(\".\", 1)\n        ModelClass = getattr(importlib.import_module(module_name), class_name)\n        model_params = model_config[\"model_params\"]\n        inference_function_name = model_config[\"model_inference_function\"]\n        if inference_function_name != \"\":\n            inference_function = getattr(ModelClass, inference_function_name)\n            llm = inference_function(model_params.pop(\"model_path\"), **model_params)\n        else:\n            llm = ModelClass(model_params.pop(\"model_path\"), **model_params)\n        return llm\n    except (ImportError, AttributeError, Exception) as e:\n        logging.info(f\"Failed to instantiate the model. Error: {e}\")\n        return None\n\n\ndef get_model(model_config_pathname: str) -> object:\n    \"\"\"\n    Returns an instantiated model and prompt template based on the model configuration.\n    Agrs:\n        model_config_pathname (str): The pathname of the model config file\n    Returns:\n        Tuple[object, str]: The instantiated model\n    \"\"\"\n    try:\n        if not model_config_pathname:\n            model_config_pathname = os.path.join(os.getcwd(), MODEL_CONFIG_FILE)\n        with open(model_config_pathname, \"r\") as config_file:\n            model_config = yaml.safe_load(config_file)\n        logging.info(f\"Using model config from file: {model_config_pathname}\")\n    except (FileNotFoundError, yaml.YAMLError):\n        logging.info(\n            f\"Model config file not valid: {model_config_pathname} Using default model config\"\n        )\n        model_config = get_default_model_config()\n    model_config[\"model\"] = instantiate_model(model_config[\"inference_model\"])\n\n    return model_config\n\n\ndef write_questions_file(output_dir: str = \"\") -> None:\n    \"\"\"\n    Writes the default questions to a file in JSON format.\n    Args:\n        output_dir (str): The directory to write the questions file to.\n    Returns:\n        None\n    \"\"\"\n    questions = get_default_questions()\n    output_dir = output_dir if output_dir and Path(output_dir).is_dir() else os.getcwd()\n    with open(os.path.join(output_dir, QUESTIONS_FILE), \"w\") as file:\n        json.dump(questions, file, indent=4)\n\n\ndef write_model_config_file(output_dir: str = \"\") -> None:\n    \"\"\"\n    Writes the default model config to a file in YAML format.\n    Args:\n        output_dir (str): The directory to write the model config file to.\n    Returns:\n        None\n    \"\"\"\n    model_config = get_default_model_config()\n    output_dir = output_dir if output_dir and Path(output_dir).is_dir() else os.getcwd()\n    with open(os.path.join(output_dir, MODEL_CONFIG_FILE), \"w\") as file:\n        yaml.dump(model_config, file)\n\n```"
            }
        ],
        "nbytes": 16426,
        "source": "py2dataset/get_params.py"
    },
    {
        "conversation": [
            {
                "from": "system",
                "value": "code documentation: 1. Overall Purpose and Processing Approach of Python file: 'get_python_datasets.py' File primarily aims at extracting structured JSON format question-answer pairs for given input Python files using a provided dataset generator class named DatasetGenerator. It accepts various inputs such as Python file path, file details dictionary, base name of the Python file, list of questions, and model configuration to generate instruct_list containing instructions related to diverse aspects of the Python code. The function utilizes other methods within DatasetGenerator class for processing different types of queries like file details, functions, classes, methods, etc., ultimately returning a tuple containing extracted information in JSON format.\n\n2. API Signatures:\n- clean_and_get_unique_elements(input_str): Cleans an input string by removing extra characters and returns a string with unique elements. It calls various built-in functions like 'enumerate', 'strip', 'join' etc., uses variables such as 'cleaned_elements', 'start', 'returned_elements'.\n- get_python_datasets(file_path, file_details, base_name, questions, model_config, detailed): Accepts inputs related to Python file path, file details dictionary, list of questions, model configuration, and a boolean flag for detailed information extraction. It instantiates DatasetGenerator class using provided inputs, calls its generate method, and returns extracted data as a tuple containing instruct_list generated by DatasetGenerator instance.\n- DatasetGenerator(__init__): Initializes the DatasetGenerator class with attributes like file path, file details dictionary, list of questions, model configuration etc., initializing variables such as llm flag, detailed flag, question mapping, code_qa_list, code_qa_response. It also defines methods like get_response_from_llm(), get_code_qa(), process_question(), get_info_string(), process_question_type() and generate().\n- DatasetGenerator.get_response_from_llm(query, context): This method queries a language model with the given context to produce response considering tokenized limits from config parameters, processes excluded instructions ('Call code graph'/'Docstring') while creating response using various function calls such as 'str', 'enumerate', 'self.model_config['prompt_template'], 'self.llm', 'logging'. It also uses variables like query, context, response, context_size, prompt_template, err_msg, instruction_prompt, code_objects_responses etc.\n- DatasetGenerator.get_code_qa(): Gathers code responses from instruct_list and updates code_qa_list and code_qa_response variables by iterating through excluded instructions and code objects responses.\n- DatasetGenerator.process_question(question_type, question_id, query, context, info): Processes questions related to file, function, class or method types using process_question_type() method with appropriate inputs for each type. It calls methods like process_question_type(), 'info.get', get_response_from_llm(), clean_and_get_unique_elements('str') and adds processed query results into instruct_list in order sorted by input size using variable 'self', 'self' respectively.\n- DatasetGenerator.get_info_string(info, item_type): Returns a string from info dictionary based on given item type using calls like '.join', 'item.strip', 'str(info.get)', 'info.get'. It uses variables such as 'info', 'item_type'.\n- DatasetGenerator.process_question_type(question_type, question_id, question_text): Processes questions related to file, method or class instances based on the question type with respective input splitting and formatting operations using query parameters like 'self', 'info', 'context', 'output', 'item_response'. It calls get_info_string(), element generator methods and string manipulation functions.\n- DatasetGenerator.generate(): Generates responses for all questions in the list, sorts instruct_list by input size length, and returns it as a tuple containing extracted information in JSON format using variables like 'self'.\n\n3. Functional Requirements:\n- clean_and_get_unique_elements() removes extra characters from input string to obtain unique elements stored in returned_elements variable.\n- get_python_datasets() returns information as tuple following datasetGenerator generation via necessary methods/input utilization detailed before instantiation (mentioned previously). It deals with high level implementation incorporating input datums within specified requirements (path, details of the Python file, list of questions and configuration settings).\n- DatasetGenerator() instantiates object of this class for querying model responses with configurable language models or no LLM usage depending on input availability.\n- get_response_from_llm() retrieves LLM response considering context size limitations while handling exceptions using logging mechanism. It manipulates prompts and queries to generate desired output stored in 'response'.\n- get_code_qa() organizes code question-answer pairs within DatasetGenerator object after iterating over instruct_list for necessary variables/classes, methods information.\n- process_question() manages distinct types of questions using corresponding method invocations, updates instruct_list with processed query results in sorted order.\n- get_info_string() extracts string data from info dictionary based on item type by joining unique elements and stripping whitespaces.\n- process_question_type() tackles various query types concerning Python files ('file', 'method' or 'function'), uses formatted prompts/templates with class related data handling within method calls as needed variables ('query', 'info').\n- generate() generates responses for all questions and returns instruct_list containing JSON format information after processing each question.\n\n4. Variables Description:\n- clean_and_get_unique_elements(): 'input_str', 'cleaned_elements', 'start', 'returned_elements'.\n- get_python_datasets(): 'file_path', 'file_details', 'base_name', 'questions', 'model_config', 'detailed' ,generated instructions(output tuples/list/list) - 'dataset_Generator Instance Method calls result '.\n- DatasetGenerator(__init__): 'llm','questions','code_qa_list','code_qa_response', 'question_mapping', 'use_llm', 'detailed', 'code_qa_list', 'code_qa_response'.\n- get_response_from_llm(): 'context_strategies', 'context_size', 'prompt_template', 'context', 'query', 'context_length', 'error_msg', 'context_size', 'context', 'query', 'item', 'instruct_key', 'instruction', 'response', 'output'.\n- DatasetGenerator.get_code_qa(): 'self','code_object', 'code_type', 'excluded_instructions', 'output', 'instruction', 'code_objects_responses', 'start', 'enumerate', 'subidx', 'mapping', 'item', 'instruction'.\n- DatasetGenerator.process_question(): 'self','info', 'query', 'context', 'response', 'question_type', 'question_id', 'combined', 'variables', 'inputs', 'methods_string'.\n- get_info_string(): 'info', 'item_type', 'item'.\n- DatasetGenerator.process_question_type(): 'self','query', 'context', 'info', 'class_name', 'method_name', 'mapping', 'inputs','variables','context','methods_string'.\n- getCodeQAGenerate() has internal instances calls relating to attributes named here(variables: file path, detail object specific variables), used as process function of DatasetGenerator class."
            },
            {
                "from": "human",
                "value": "Output the Python code described by the code documentation."
            },
            {
                "from": "gpt",
                "value": "```python\n\"\"\"\nGenerates JSON format question-answer pairs and instructions for a Python file\nRequirements:\n[req01] The `DatasetGenerator` class shall:\n        a. Accept Python file path, file details, base name, list of questions,\n        and model configuration as input during instantiation.\n        b. Initialize and store Python file path, file details, base name,\n        question list, llm, and use_llm flag as class attributes.\n        d. Provide `get_response_from_llm` method to retrieve llm response.\n        e. Provide `process_question` method to process each question, generate\n        corresponding responses, and add to the instruct_list.\n        f. Provide `process_question_type` method to process questions\n        related to the file, functions, classes, and methods.\n        g. Provide `generate` method to generate responses for all questions\n        and return the instruct_list.\n        h. Internally manage question mapping to file details.\n[req02] The `get_python_datasets` function shall:\n        a. Accept a Python file path, file details, base name, questions list,\n        and model config as input.\n        b. Instantiate `DatasetGenerator` class using the provided input.\n        c. Generate instruct_list using `DatasetGenerator` `generate` method.\n        d. Return the generated `instruct_list`.\n[req03] The `clean_and_get_unique_elements` function shall:\n        a. Clean an input string (str) and return a string of unique elements.\n\"\"\"\nimport logging\nimport re\nimport math\nfrom typing import Dict, List, Tuple\n\n\ndef clean_and_get_unique_elements(input_str: str) -> str:\n    \"\"\"\n    Clean an input string (str) and return a string of unique elements.\n    Args:\n        input_str (str): The input string to be cleaned.\n    Returns:\n        str: The cleaned string.\n    \"\"\"\n\n    def element_generator(input_str):\n        start, brace_level = 0, 0\n        for i, char in enumerate(input_str):\n            if char in \"{}\":\n                brace_level += 1 if char == \"{\" else -1\n            elif char == \",\" and brace_level == 0:\n                yield input_str[start:i]\n                start = i + 1\n        yield input_str[start:]\n\n    input_str = input_str.strip(\"[]'\\\"\").strip()\n    cleaned_elements = [\n        element.strip(\"'\\\" \").strip()\n        for element in element_generator(input_str)\n        if element.strip()\n    ]\n    # add a ' ' around each element\n    cleaned_elements = [f\"'{element}'\" for element in cleaned_elements]\n    returned_elements = \", \".join(cleaned_elements)\n    return returned_elements\n\n\nclass DatasetGenerator:\n    \"\"\"\n    Generate JSON formatted dictionary outputs for a Python file.\n    Attributes:\n        file_path (str): The path to the Python file.\n        file_details (Dict[str, Any]): Details of the Python file.\n        base_name (str): The base name of the Python file.\n        questions (List[Dict[str, str]]): Questions for generating responses.\n        instruct_list (List[Dict[str, str]]): Storage for generated instructions.\n        question_mapping (Dict[str, str]): Mapping of question types to keys in file details.\n        use_llm (bool): Flag indicating if a language model should be used.\n        llm (object): The language model for generating responses.\n        prompt (str): The prompt format for querying the language model.\n    Methods:\n        get_response_from_llm(query: str, context: str) -> str:\n            Get language model response to query for given context.\n        process_question(question_type: str, question_id: str, query: str, context: str, info: Dict) -> None:\n            Process question and add the generated response to the instruct_list.\n        get_info_string(info, item_type) -> str:\n            Get string from info dictionary.\n        process_question_type(question_type: str, question_id: str, question_text: str) -> None:\n            Process questions related to a file, function, class, or method.\n        generate() -> Tuple[List[Dict], List[Dict]]:\n            Generate responses for all the questions and returns the instruct_list.\n    \"\"\"\n\n    def __init__(\n        self,\n        file_path: str,\n        file_details: Dict,\n        base_name: str,\n        questions: List[Dict],\n        model_config: Dict,\n        detailed: bool,\n    ) -> None:\n        \"\"\"\n        Initialize the DatasetGenerator class.\n        Args:\n            file_path (str): The path to the Python file.\n            file_details (Dict[str, Any]): Details of the Python file.\n            base_name (str): The base name of the Python file.\n            questions (List[Dict[str, str]]): Questions for generating responses.\n            model_config (Dict): Configuration for the language model.\n        Returns:\n            None\n        \"\"\"\n        self.file_path = file_path\n        self.file_details = file_details\n        self.base_name = base_name\n        self.questions = questions\n        self.model_config = model_config\n        if model_config is not None:\n            self.llm = model_config[\"model\"]\n            self.use_llm = True\n            self.detailed = detailed\n        else:\n            self.llm = None\n            self.use_llm = False\n            self.detailed = False\n        self.instruct_list = []\n        self.question_mapping = {\n            \"file\": \"file\",\n            \"function\": \"functions\",\n            \"class\": \"classes\",\n            \"method\": \"classes\",\n        }\n        self.code_qa_list = []\n        self.code_qa_response = \"\"\n\n    def get_response_from_llm(self, query: str, context: str) -> str:\n        \"\"\"\n        Get language model response to query for given context.\n        Args:\n            query (str): The query to be used for generating the response.\n            context (str): The context to be used for generating the response.\n        Returns:\n            str: The generated response.\n        \"\"\"\n        context_strategies = [\n            lambda: \"{}\".format(str(context)),\n            lambda: \"```python\\n{}\\n```\".format(\n                str(self.file_details[\"file_info\"][\"file_code_simplified\"])\n            ),\n            lambda: \"```python\\n{}\\n```\".format(\n                self.get_info_string(self.file_details[\"file_info\"], \"file_summary\")\n            ),\n            lambda: \"\",\n        ]\n\n        max_context_length = self.model_config[\"inference_model\"][\"model_params\"][\n            \"context_length\"\n        ]\n        prompt_template = self.model_config[\"prompt_template\"].format(\n            system_prompt=self.model_config[\"system_prompt\"],\n            instruction_prompt=self.model_config[\"instruction_prompt\"],\n        )\n\n        for strategy in context_strategies:\n            context = strategy()\n            prompt = prompt_template.format(\n                context=context, query=query, code_objects=self.code_qa_response\n            )\n            context_size = len(self.llm.tokenize(prompt))\n            logging.info(\"***Context Size: \" + str(context_size))\n            if context_size <= 0.70 * max_context_length:\n                break\n        else:\n            err_msg = f\"Model response failed, increase py2dataset_model_config.yaml context_length > {math.ceil(context_size/0.70)}\"\n            logging.error(err_msg)\n            return \"\"\n        if context == \"\":\n            context = self.code_qa_list\n\n        response = \"\"\n        try:\n            response = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", self.llm(prompt))\n            response = response.replace(\"<|im_end|>\", \"\")\n            response = \"\\n\".join([line.lstrip() for line in response.split(\"\\n\")])\n            logging.info(f\"***Overall Response: {response}\")\n        except Exception as error:\n            logging.error(f\"Failed to generate model response: {error}\")\n\n        if self.detailed:\n            for item in self.code_qa_list:\n                try:\n                    instruct_key = list(item.keys())[0]\n                    instruct_value = list(item.values())[0]\n                    query = f\"Describe the Purpose and Significance of these {instruct_key}: [{instruct_value}] and Explain what each of these {instruct_key} does in the code.\"\n                    prompt = (\n                        self.model_config[\"prompt_template\"]\n                        .format(\n                            system_prompt=self.model_config[\"system_prompt\"],\n                            instruction_prompt=self.model_config[\"instruction_prompt\"],\n                        )\n                        .format(\n                            context=f\"{context}/nCode Summary:/n{response}\",\n                            query=f\"{query}\", code_objects=f\"{instruct_value}\"\n                        )\n                    )\n                    item_response = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", self.llm(prompt))\n                    item_response = item_response.replace(\"<|im_end|>\", \"\")\n                    logging.info(f\"\\n***Itemized Response: {query}\\n{item_response}\")\n                    for item in self.instruct_list:\n                        if item[\"instruction\"].startswith(instruct_key):\n                            output = f\"\\n\\nPurpose and Significance:\\n{item_response}\"\n                            item[\"output\"] += output\n                            break\n\n                except Exception as error:\n                    logging.error(f\"Failed to generate model response: {error}\")\n\n        return response\n\n    def get_code_qa(self):\n        \"\"\"\n        Get code responses from the instruct_list and update:\n            code_qa_list (List[Dict]): List of code question-answer pairs.\n            code_qa_response (str): structured text for code question-answer pairs.\n        \"\"\"\n        excluded_instructions = {\"Call code graph\", \"Docstring\"}\n        self.code_qa_list = []\n        code_objects_responses = {}\n\n        for item in self.instruct_list:\n            instruction = item[\"instruction\"].split(\" in Python file:\")[0]\n            output = item[\"output\"]\n            if any(instruction.startswith(prefix) for prefix in excluded_instructions):\n                continue\n            self.code_qa_list.append({instruction: output})\n            if \"`\" in instruction:\n                code_object = instruction.split(\"`\")[1]\n                code_type = instruction.split()[0]\n                code_objects_responses.setdefault(code_object, []).append(\n                    (code_type, output)\n                )\n\n        self.code_qa_response = \"\"\n        for idx, (code_object, type_responses) in enumerate(\n            code_objects_responses.items(), start=1\n        ):\n            self.code_qa_response += f\"{idx}) {code_object}:\\n\"\n            for subidx, (code_type, response) in enumerate(type_responses, start=1):\n                self.code_qa_response += f\"{idx}.{subidx}. {code_type}: {response}\\n\"\n\n    def process_question(\n        self, question_type: str, question_id: str, query: str, context: str, info: Dict\n    ) -> None:\n        \"\"\"\n        Process question and add the generated response to the instruct_list.\n        Args:\n            question_type (str): The type of question to be processed.\n            question_id (str): The ID of the question to be processed.\n            query (str): The query to be processed.\n            context (str): The context to be used for generating the response.\n            info (Dict): The information of the Python file.\n        Returns:\n            None\n        \"\"\"\n        response = \"\"\n        if question_id.endswith((\"code_graph\", \"docstring\")):\n            response = info.get(question_id, {})\n        elif question_id.endswith(\"file_purpose\"):  # file_purpose is last question\n            self.get_code_qa()\n            if self.use_llm:\n                response = self.get_response_from_llm(query, context)\n        else:\n            response = clean_and_get_unique_elements(str(info.get(question_id, \"\")))\n        response = str(response).strip()\n\n        if response and response != \"None\":\n            context = f\"```python\\n{context}\\n```\"\n            self.instruct_list.append(\n                {\"instruction\": query, \"input\": context, \"output\": response}\n            )\n\n    @staticmethod\n    def get_info_string(info: Dict, item_type: str) -> str:\n        \"\"\"\n        Get string from info dictionary.\n        Args:\n            info (Dict): The information of the Python file.\n            item_type (str): The type of item to get the string from.\n        Returns:\n            str: The string from the info.\n        \"\"\"\n        return \", \".join(\n            [item.strip() for item in str(info.get(item_type, \"\")).split(\",\") if item]\n        )\n\n    def process_question_type(\n        self, question_type: str, question_id: str, question_text: str\n    ) -> None:\n        \"\"\"\n        Process questions related to a file, function, class, or method.\n        Args:\n            question_type (str): The type of question to be processed.\n            question_id (str): The ID of the question to be processed.\n            question_text (str): The text of the question to be processed.\n        Returns:\n            None\n        \"\"\"\n        if question_type == \"file\":\n            query = question_text.format(filename=self.base_name)\n            info = self.file_details[\"file_info\"]\n            context = self.file_details[\"file_info\"][\"file_code\"]\n            self.process_question(question_type, question_id, query, context, info)\n        elif question_type == \"method\":\n            for class_name, class_info in self.file_details[\"classes\"].items():\n                for key, method_info in class_info.items():\n                    if key.startswith(\"class_method_\"):\n                        method_name = f\"{class_name}.{key[len('class_method_'):]}\"\n                        context = method_info[\"method_code\"]\n                        mapping = {\"class_name\": class_name, \"method_name\": method_name}\n                        query = question_text.format(filename=self.base_name, **mapping)\n                        self.process_question(\n                            question_type, question_id, query, context, method_info\n                        )\n        else:  # question_type is 'function' or 'class'\n            for name, info in self.file_details[\n                self.question_mapping[question_type]\n            ].items():\n                context = info[f\"{question_type}_code\"]\n                mapping = {f\"{question_type}_name\": name}\n                if question_id == f\"{question_type}_purpose\" and self.use_llm:\n                    variables = self.get_info_string(info, f\"{question_type}_variables\")\n                    inputs = self.get_info_string(info, f\"{question_type}_inputs\")\n                    combined = \", \".join([s for s in [variables, inputs] if s])\n                    mapping[\n                        f\"{question_type}_variables\"\n                    ] = clean_and_get_unique_elements(combined)\n\n                    if question_type == \"class\":\n                        methods_string = self.get_info_string(\n                            info, f\"{question_type}_methods\"\n                        )\n                        mapping[f\"{question_type}_methods\"] = methods_string\n\n                query = question_text.format(filename=self.base_name, **mapping)\n                self.process_question(question_type, question_id, query, context, info)\n\n    def generate(self) -> Tuple[List[Dict], List[Dict]]:\n        \"\"\"\n        Generate responses for all the questions and returns the instruct_list.\n        Args:\n            None\n        Returns:\n            Tuple[List[Dict], List[Dict]]:  .\n        \"\"\"\n        for question in self.questions:\n            self.process_question_type(\n                question[\"type\"], question[\"id\"], question[\"text\"]\n            )\n        self.instruct_list.sort(key=lambda x: len(x[\"input\"]), reverse=True)\n        return self.instruct_list\n\n\ndef get_python_datasets(\n    file_path: str,\n    file_details: Dict,\n    base_name: str,\n    questions: List[Dict],\n    model_config: Dict,\n    detailed: bool,\n) -> Tuple[List[Dict], List[Dict]]:\n    \"\"\"\n    Extract information from a Python file and return it in JSON format.\n    Args:\n        file_path (str): The path to the Python file.\n        file_details (Dict): The details of the file.\n        base_name (str): The base Python code filename.\n        questions (List[Dict]): The list of questions.\n        llm (object): The language model to be used for generating responses.\n        prompt (str): The prompt to be used for generating responses.\n        detailed (bool): Flag indicating if detailed information should be extracted.\n    Returns:\n        Tuple[List[Dict], List[Dict]]: Extracted information in JSON format.\n    \"\"\"\n    return DatasetGenerator(\n        file_path, file_details, base_name, questions, model_config, detailed\n    ).generate()\n\n```"
            }
        ],
        "nbytes": 24120,
        "source": "py2dataset/get_python_datasets.py"
    },
    {
        "conversation": [
            {
                "from": "system",
                "value": "code documentation: Overall Purpose and Processing Approach of Python File: get_python_file_details.py\nThis Python file offers a set of functions aimed at extracting detailed information about a given Python source code file. It involves several steps to achieve this goal, including removing docstrings from the code, traversing Abstract Syntax Tree (AST) for gathering function and class details, constructing a directed graph representing code call relationships using NetworkX library, and ultimately returning a comprehensive dictionary encompassing extracted file details.\n\n1. remove_docstring(code: str, tree: ast.AST) -> str:\nPurpose: Remove docstrings from the provided Python code string while maintaining syntax integrity by removing top-level module docstrings and docstrings in functions, classes, and async functions within the AST tree 'tree'.\nProcessing Approach:\n- Checks if root node has a docstring and removes it.\n- Traverses the tree recursively to identify docstrings inside function definitions (FunctionDef, ClassDef, AsyncFunctionDef) and removes them from respective nodes using ast.unparse().\nInputs: 'code' (original Python code), 'tree' (AST representation of code).\nCalled Functions/Methods: tree.body.pop(), ast.walk(), node.body.pop(), ast.unparse().\nVariables Used: 'tree', 'code', 'first_body_node'.\nReturn Value: Sanitized Python code string without docstrings ('ast.unparse(tree)').\n\n2. get_all_calls(node: ast.AST, calls=None) -> Dict[str, List[str]]:\nPurpose: Locate and aggregate all function calls found within the given 'node', which could include occurrences inside list comprehensions and lambda functions too.\nProcessing Approach:\n- Initializes accumulator 'calls' as empty dictionary if none provided ('calls is None').\n- Adds call details to 'calls' after encountering an Ast.Call node with function arguments ('calls.append()').\n- For class attribute definitions (ClassDef nodes), calls get_all_calls() recursively and builds up a calls dictionary using parentage relation of ast nodes.\nInputs: 'node', optional argument 'calls'.\nCalled Functions/Methods: isinstance(), ast.unparse(), ast.iter_child_nodes().\nVariables Used: 'calls', 'calls_dict', 'func', 'args', 'node'.\nReturn Value: Dictionary mapping function calls (as strings) to lists of their arguments ('calls_dict').\n\n3. get_python_file_details(file_path: str) -> Dict[str, Union[Dict, str]]:\nPurpose: Acquire information from a Python file specified by 'file_path', structure it into detailed dictionary format for easy access and analysis.\nProcessing Approach:\n- Opens the input file in read mode ('open()').\n- Parses the code using ast module to create an AST ('ast.parse()').\n- Instantiates CodeVisitor class with source code and AST ('CodeVisitor(code, tree)').\n- Analyzes the AST by invoking analyze() method on CodeVisitor instance ('visitor.analyze(tree)').\n- Constructs a directed graph using get_code_graph().\n- Serializes file summary JSON data for easier representation ('json.dumps(file_summary).replace()').\nInputs: 'file_path' (Python file path).\nCalled Functions/Methods: open(), logging.warning(), CodeVisitor('__init__'), get_code_graph(), json.dumps().\nVariables Used: 'tree', 'code', 'file_summary', 'visitor', 'file_path', 'file_details'.\nReturn Value: Dictionary containing extracted file details ('file_details'). If error occurs, returns None.\n\n4. CodeVisitor Class:\nThis class traverses an AST and extracts various code details using NodeVisitor inheritance from ast module for analyzing Python source code.\n4.1.__init__(self, code: str, tree: ast.AST) -> None:\nPurpose: Initialize a new CodeVisitor instance with 'code' and its respective 'tree'.\nProcessing Approach: Assigns attributes ('self', 'code', 'tree').\nInputs: 'self', Python code string ('code'), AST representation ('tree').\n4.2.visit_FunctionDef(node: ast.FunctionDef) -> None:\nPurpose: Extract details about a function within the class context ('self.current_class' is None).\nProcessing Approach: Calls extract_details() with 'function' as node type and stores results in 'functions'.\nInputs: 'self', 'node' (FunctionDef node).\n4.3.visit_ClassDef(self, node: ast.ClassDef) -> None:\nPurpose: Extract details about a class within the class context ('self.current_class' is not None).\nProcessing Approach: Calls extract_details() with 'class' as node type and stores results in 'classes'.\nInputs: 'self', 'node' (ClassDef node).\n4.4.generic_visit(self, node):\nPurpose: Handles nodes without explicit visitor functions by traversing the AST recursively ('ast.iter_child_nodes()').\nProcessing Approach: Invokes self.visit() for each child node.\nInputs: 'self', 'node'.\n4.5.visit_Assign(self, node: ast.Assign):\nPurpose: Collect constant assignments in the top level module scope ('self.constants').\nProcessing Approach: Identifies constants from Assign nodes and appends them to 'self.constants'.\nInputs: 'self', 'node'.\n4.6.extract_details(self, node: ast.AST, node_type: str) -> Dict[str, Union[str, List[str]]:\nPurpose: Extract details about a given node ('node') based on its type ('node_type').\nProcessing Approach: Walks through the AST using ast.walk(), gathers call data with get_all_calls().\nInputs: 'self', 'node', 'node_type'.\n- Builds a dictionary containing node details depending on node type ('details').\n4.7.analyze(self, node: ast.AST) -> None:\nPurpose: Traverse the AST to populate file information dictionary ('file_info') with various code aspects.\nProcessing Approach: Invokes self methods for function and class details extraction, constructs 'file_summary'.\nInputs: 'self', 'node'.\n- Creates lists of functions, classes, constants, dependencies, etc., simplifies code ('remove_docstring()'), and calls get_code_graph().\n\n5. Variables in CodeVisitor Class:\ntree: AST representation of Python source code during initialization.\ncurrent_class: Indicates if traversal is within a class definition context (True) or not (False).\nfile_info: Contains overall details about the analyzed file.\nfunctions: Dictionary containing function details extracted from the code.\nclasses: Dictionary storing class details from the source code."
            },
            {
                "from": "human",
                "value": "Output the Python code described by the code documentation."
            },
            {
                "from": "gpt",
                "value": "```python\n\"\"\"\nReturn file_details dictionary for python source code.\nRequirements:\n[req00] The `remove_docstring` function shall:\n        a. Accept a Python code string as an argument.\n        b. Remove docstrings and comments from the provided code.\n        c. Return the sanitized code string.\n[req01] The get_all_calls function shall:\n        a. Accept a node of type ast.AST as an argument.\n        b. Recursively find all function calls in the subtree rooted at the node.\n        c. Return a dictionary of all function calls in the subtree rooted at the node.\n[req02] The `CodeVisitor` class shall:\n        a. Accept the source code as input when instantiated.\n        b. Use the AST to extract details about the code.\n        c. Inherit from `ast.NodeVisitor`.\n        d. Implement `visit_FunctionDef` method to gather details about functions.\n        e. Implement `visit_ClassDef` method to gather details about classes.\n        f. Implement `extract_details` method to parse information about a given node.\n        g. Implement `analyze` method to traverse the AST and 'file_info'. \n        h. Maintain a current class context using the attribute 'current_class'.\n[req03] The `code_graph` function shall:\n        a. Accept the file summary as input.\n        b. Construct a directed graph with nodes and edges using networkx library.\n        c. Define elements such as function nodes, class nodes, and method nodes.\n        d. Specify edges to represent relationships. \n        e. Return a dictionary representation of the code call graph. \n[req04] The `get_python_file_details` function shall:\n        a. Accept a file path as an argument.\n        b. Extract info from Python file using the AST and the `CodeVisitor` class.\n        c. Include the entire code graph in the returned details.\n        d. Return a dictionary encompassing the extracted file details. \n\"\"\"\nimport ast\nimport logging\nimport json\nfrom typing import Dict, List, Union\nfrom get_code_graph import get_code_graph\n\n\ndef remove_docstring(code: str, tree: ast.AST) -> str:\n    \"\"\"\n    Remove docstrings from the provided Python code.\n    This includes top-level module docstrings and docstrings in\n    functions, classes, and async functions.\n    Args:\n        code (str): The source code from which to remove docstrings.\n    Returns:\n        str: The source code with docstrings removed.\n    \"\"\"\n    if (\n        tree.body\n        and isinstance(tree.body[0], ast.Expr)\n        and isinstance(tree.body[0].value, ast.Str)\n    ):\n        tree.body.pop(0)\n\n    for node in ast.walk(tree):\n        if (\n            isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef))\n            and node.body\n        ):\n            first_body_node = node.body[0]\n            if isinstance(first_body_node, ast.Expr) and isinstance(\n                first_body_node.value, ast.Str\n            ):\n                node.body.pop(0)\n\n    return ast.unparse(tree)\n\n\ndef get_all_calls(node: ast.AST, calls=None) -> Dict[str, List[str]]:\n    \"\"\"\n    Recursively find all function calls in the subtree rooted at `node`,\n    including those in class attributes, list comprehensions, and lambda functions.\n    Args:\n        node (ast.AST): The root node to start the search from.\n        calls (List[Tuple[str, List[str]]], optional): Accumulator for function calls found.\n    Returns:\n        Dict[str, List[str]]: dictionary mapping function calls (as strings) to lists of their arguments.\n    \"\"\"\n    if calls is None:\n        calls = []\n    if isinstance(node, ast.Call):\n        calls.append((ast.unparse(node.func), [ast.unparse(arg) for arg in node.args]))\n    elif isinstance(node, ast.ClassDef):\n        for body_item in node.body:\n            if isinstance(body_item, ast.Assign) and isinstance(\n                body_item.targets[0], ast.Name\n            ):\n                get_all_calls(body_item.value, calls)\n\n    for child in ast.iter_child_nodes(node):\n        get_all_calls(child, calls)\n\n    # Build the dictionary after traversal\n    calls_dict = {}\n    for func, args in calls:\n        if func not in calls_dict:\n            calls_dict[func] = []\n        calls_dict[func].extend(args)\n\n    return calls_dict\n\n\nclass CodeVisitor(ast.NodeVisitor):\n    \"\"\"\n    Visitor class for traversing an AST (Abstract Syntax Tree) and extracting details about the code.\n    Attributes:\n        code (str): The source code.\n        functions(Dict): details about functions in the code.\n        classes (Dict): details about classes in the code.\n        file_info (Dict): details about the file.\n    Methods:\n        visit_FunctionDef(node: ast.FunctionDef) -> None:\n            Extract details about a function.\n        visit_ClassDef(node: ast.ClassDef) -> None:\n            Extract details about a class.\n        extract_details(node: ast.AST, node_type: str) -> Dict[str, Union[str, List[str]]]:\n            Extract details about a node.\n        analyze(node: ast.AST) -> None:\n            Populate file_info with details about the file.\n    \"\"\"\n\n    def __init__(self, code: str, tree: ast.AST) -> None:\n        \"\"\"\n        Initialize a new instance of the class.\n        Args:\n            code: str: The source code.\n        \"\"\"\n        self.code: str = code\n        self.functions: Dict[str, Dict[str, Union[str, List[str]]]] = {}\n        self.classes: Dict[str, Dict[str, Union[str, List[str]]]] = {}\n        self.file_info: Dict[str, Union[str, List[str]]] = {}\n        self.current_class: str = None\n        self.constants: List[str] = []\n        self.tree = tree\n\n    def visit_FunctionDef(self, node: ast.FunctionDef) -> None:\n        \"\"\"\n        Extract details about a function.\n        Args:\n            node: ast.FunctionDef: The node to visit.\n        \"\"\"\n        details = self.extract_details(\n            node, \"method\" if self.current_class else \"function\"\n        )\n        if self.current_class:\n            self.classes[self.current_class][f\"class_method_{node.name}\"] = details\n        else:\n            self.functions[node.name] = details\n        self.generic_visit(node)\n\n    def visit_ClassDef(self, node: ast.ClassDef) -> None:\n        \"\"\"\n        Extract details about a class.\n        Args:\n            node: ast.ClassDef: The node to visit.\n        \"\"\"\n        self.classes[node.name] = self.extract_details(\n            node, \"class\"\n        )  # populate class dictionary when class definition found in AST\n        self.current_class = node.name  # set current_class to indicate inside a class\n        self.generic_visit(node)  # continue AST traversal to the next node\n        self.current_class = None  # reset current_class when finished with this class\n\n    def generic_visit(self, node):\n        \"\"\"\n        Called if no explicit visitor function exists for a node.\n        Args:\n            node: ast.AST: The node to visit.\n        \"\"\"\n        for child in ast.iter_child_nodes(node):\n            child.parent = node\n            self.visit(child)\n\n    def visit_Assign(self, node: ast.Assign):\n        \"\"\"\n        Get self.constants\n        Args:\n            node: ast.Assign: The node to visit.\n        \"\"\"\n        if isinstance(node.parent, ast.Module):\n            for target in node.targets:\n                if isinstance(target, ast.Name):\n                    value = node.value\n                    if isinstance(value, ast.Constant) and isinstance(value.value, str):\n                        value_repr = f\"'{value.value}'\"\n                    else:\n                        value_repr = ast.unparse(value)\n                    constant_assignment = f\"{target.id}={value_repr}\"\n                    self.constants.append(constant_assignment)\n        self.generic_visit(node)\n\n    def extract_details(\n        self, node: ast.AST, node_type: str\n    ) -> Dict[str, Union[str, List[str]]]:\n        \"\"\"\n        Extract details about a node.\n        Args:\n            node: ast.AST: The node to extract details from.\n            node_type: str: The type of node.\n        Returns:\n            Dict[str, Union[str, List[str]]]: The details extracted from the node.\n        \"\"\"\n        node_walk = list(ast.walk(node))\n        call_data = get_all_calls(node)\n        node_inputs = (\n            [arg.arg for arg in node.args.args]\n            if node_type in [\"function\", \"method\"]\n            else None\n        )\n        node_variables = list(\n            {\n                ast.unparse(target)\n                for subnode in node_walk\n                if isinstance(subnode, ast.Assign)\n                for target in subnode.targets\n                if isinstance(target, ast.Name)\n            }\n        )\n        if node_inputs:\n            node_variables = list(set(node_inputs + node_variables))\n\n        details = {\n            f\"{node_type}_name\": node.name,\n            f\"{node_type}_code\": ast.unparse(node),\n            f\"{node_type}_docstring\": next(\n                (\n                    n.value.s\n                    for n in node_walk\n                    if isinstance(n, ast.Expr) and isinstance(n.value, ast.Str)\n                ),\n                None,\n            ),\n            f\"{node_type}_inputs\": node_inputs,\n            f\"{node_type}_defaults\": [ast.unparse(d) for d in node.args.defaults]\n            if node_type in [\"function\", \"method\"]\n            else None,\n            f\"{node_type}_returns\": [\n                ast.unparse(subnode.value) if subnode.value is not None else \"None\"\n                for subnode in node_walk\n                if isinstance(subnode, ast.Return)\n            ],\n            f\"{node_type}_calls\": list(call_data.keys()),\n            f\"{node_type}_call_inputs\": call_data,\n            f\"{node_type}_variables\": node_variables,\n            f\"{node_type}_decorators\": list(\n                {ast.unparse(decorator) for decorator in node.decorator_list}\n                if node.decorator_list\n                else set()\n            ),\n            f\"{node_type}_annotations\": list(\n                {\n                    ast.unparse(subnode.annotation)\n                    for subnode in node_walk\n                    if isinstance(subnode, ast.AnnAssign)\n                    and subnode.annotation is not None\n                }\n            ),\n            f\"{node_type}_properties\": list(\n                {\n                    ast.unparse(subnode)\n                    for subnode in node_walk\n                    if isinstance(subnode, ast.Attribute)\n                    and isinstance(subnode.ctx, ast.Store)\n                }\n            ),\n        }\n        if node_type in [\"class\", \"method\"]:\n            if (\n                node_type == \"method\" and self.current_class\n            ):  # find attributes defined as self.attribute\n                attributes = [\n                    target.attr\n                    for subnode in node_walk\n                    if isinstance(subnode, ast.Assign)\n                    for target in subnode.targets\n                    if isinstance(target, ast.Attribute)\n                    and isinstance(target.value, ast.Name)\n                    and target.value.id == \"self\"\n                ]\n                class_attributes = self.classes[self.current_class].setdefault(\n                    \"class_attributes\", []\n                )\n                class_attributes += attributes\n            if node_type == \"class\":\n                details.update(\n                    {\n                        \"class_attributes\": [\n                            target.attr\n                            for subnode in node.body\n                            if isinstance(subnode, ast.Assign)\n                            for target in subnode.targets\n                            if isinstance(target, ast.Attribute)\n                        ],\n                        \"class_methods\": [\n                            subnode.name\n                            for subnode in node.body\n                            if isinstance(subnode, ast.FunctionDef)\n                        ],\n                        \"class_inheritance\": [ast.unparse(base) for base in node.bases]\n                        if node.bases\n                        else [],\n                        \"class_static_methods\": [\n                            subnode.name\n                            for subnode in node.body\n                            if isinstance(subnode, ast.FunctionDef)\n                            and any(\n                                isinstance(decorator, ast.Name)\n                                and decorator.id == \"staticmethod\"\n                                for decorator in subnode.decorator_list\n                            )\n                        ],\n                    }\n                )\n        return details\n\n    def analyze(self, node: ast.AST) -> None:\n        \"\"\"\n        Traverse the AST, list all nodes, and populate 'file_info'\n        Args:\n            node: ast.AST: The node to analyze.\n        \"\"\"\n        # Traverse the AST to capture various code details\n        self.visit(node)\n        node_walk = list(ast.walk(node))\n\n        file_dependencies = {\n            alias.name\n            for subnode in node_walk\n            if isinstance(subnode, ast.Import)\n            for alias in subnode.names\n        } | {\n            subnode.module\n            for subnode in node_walk\n            if isinstance(subnode, ast.ImportFrom)\n        }\n\n        function_defs = [\n            {\n                func_name: {\n                    \"inputs\": details[\"function_inputs\"],\n                    \"calls\": details[\"function_calls\"],\n                    \"call_inputs\": details[\"function_call_inputs\"],\n                    \"returns\": details[\"function_returns\"],\n                }\n            }\n            for func_name, details in self.functions.items()\n        ]\n\n        class_defs = [\n            {\n                class_name: {\n                    \"method_defs\": {\n                        method_name[len(\"class_method_\") :]: {\n                            \"inputs\": details[\"method_inputs\"],\n                            \"calls\": details[\"method_calls\"],\n                            \"call_inputs\": details[\"method_call_inputs\"],\n                            \"returns\": details[\"method_returns\"],\n                        }\n                        for method_name, details in class_details.items()\n                        if method_name.startswith(\"class_method_\")\n                    }\n                }\n            }\n            for class_name, class_details in self.classes.items()\n        ]\n\n        self.file_info = {\n            \"file_code\": self.code,\n            \"file_ast\": node,\n            \"file_dependencies\": list(file_dependencies),\n            \"file_functions\": list(self.functions.keys()),\n            \"file_classes\": list(self.classes.keys()),\n            \"file_constants\": self.constants,\n            \"file_summary\": {\n                \"dependencies\": list(file_dependencies),\n                \"function_defs\": function_defs,\n                \"class_defs\": class_defs,\n            },\n            \"file_code_simplified\": remove_docstring(ast.unparse(node), self.tree),\n        }\n\n\ndef get_python_file_details(file_path: str) -> Dict[str, Union[Dict, str]]:\n    \"\"\"\n    Extract details from a Python file.\n    Args:\n        file_path: str: The path to the Python file.\n    Returns:\n        Dict[str, Union[Dict, str]]: The details extracted from the file.\n    \"\"\"\n    try:\n        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n            code = f.read()\n            tree = ast.parse(code)\n    except (PermissionError, SyntaxError, IOError) as e:\n        logging.warning(f\"{e} error in file: {file_path}\")\n        return None\n\n    visitor = CodeVisitor(code, tree)\n    visitor.analyze(tree)\n    file_details = {\n        \"file_info\": visitor.file_info,\n        \"functions\": visitor.functions,\n        \"classes\": visitor.classes,\n    }\n\n    # get code graph items and add to file_details\n    file_summary = file_details[\"file_info\"][\"file_summary\"]\n    file_ast = file_details[\"file_info\"][\"file_ast\"]\n    entire_code_graph, control_flow_structure, plant_uml = get_code_graph(\n        file_summary, file_ast\n    )\n    file_details[\"file_info\"][\"entire_code_graph\"] = entire_code_graph\n    file_details[\"file_info\"][\"control_flow_structure\"] = control_flow_structure\n    file_details[\"file_info\"][\"plant_uml\"] = plant_uml\n    file_details[\"file_info\"][\"file_summary\"] = json.dumps(file_summary).replace(\n        '\"', \"\"\n    )\n    del file_details[\"file_info\"][\"file_ast\"]\n\n    return file_details\n\n```"
            }
        ],
        "nbytes": 22698,
        "source": "py2dataset/get_python_file_details.py"
    },
    {
        "conversation": [
            {
                "from": "system",
                "value": "code documentation: 1. Overview & Processing Approach of py2dataset.py Module:\nThe py2dataset.py module is designed for generating, saving, and returning datasets containing responses to questions about Python codes within a given directory using specified parameters. It consists of multiple functions that perform various tasks such as processing single Python files, handling GitHub repositories cloning, retrieving user inputs through command-line arguments, and managing logging levels. The primary function is py2dataset(), which generates datasets by iterating over Python files in a given directory or GitHub repository and calling process_single_python_file() for each file if required processing power constraints permit (default single multiprocessing is enabled upon utilizing language modeling capabilities). After all executions are done, the combined instruction data saved under '*.py.instruct.json' formats gets assembled via combine_json_files(). Processing of each individual Python code relies on functions process_single_python_file(), fetching configurations, analyzing code features and purpose responses by either relying upon input Model (py2dataset_model_config), Large Language Model from the supplied py2dataset_model_config file, or both.\n\n2. API Signatures & Functional Details:\na) process_single_python_file(python_pathname, relative_path, output_dir, model_config_pathname, questions, use_llm, model_config, detailed): This function processes an individual Python file to generate question-answer pairs and instructions. It accepts arguments representing paths to the Python file, output directory, model configuration file path, dictionary of questions, LLM usage flag, model configuration dictionary (optional), and detailed analysis preference (default False). If 'model_config' is unavailable but 'use_llm' is True, it loads an LLM config using get_model(). It collects Python file details via get_python_file_details() and generates datasets with get_python_datasets(), saving them in output_dir using save_python_data(). Returns None.\nb) py2dataset(start='', output_dir='', questions_pathname='', model_config_pathname='', use_llm=False, quiet=False, single_process=False, detailed=False, html=False, skip_regen=False): Generates datasets iterating over Python files within specified directories (start GitHub repo default is current working dir or cloned from input URL if valid). It manages logging levels based on 'quiet', retrieves questions dictionary from questions file pathname ('py2dataset_questions.json'), searches for Python files excluding hidden ones, and spawns child processes to process single_python_file() if needed configuration params permit multitasking; combination occurs under combine_json_files(), potentially transforming outcomes in HTML (flagged via html=True). Finally it renders 'combine_json_files()', parametric QuestionsDict and instruct data structure from JSON file collections.\nc) clone_github_repo(url): Retrieves the cloned repository path or updates the existing changes at current directory with specified GitHub URL, verifying connectivity initially before attempting actions using 'git' library calls; generates a unique repository folder under 'githubrepos'. Returns local repository path.\nd) get_bool_from_input(input_str, current_value): Interacts with users to alter boolean parameters by prompting inputs against existing values in py2dataset CLI arguments and lowers string entries. Input returns corresponding Boolean or holds present setting as 'current_value'.\ne) main(): Enacts command-line interaction; interprets given parameters into defaults for each flag if provided or seeks user confirmation, adheres to user responses while maintaining initial conditions, ensures repository validation and clones when GitHub URL input detected. It subsequently executes py2dataset() after updating inputs (when necessary), prints warnings/errors in invalid situations, or prompts usage guide via 'sys.exit'.\n\n3. Functional Requirements & Behavior Insights:\na) process_single_python_file(): Handles single Python file processing with LLM support if enabled and single multiprocessing mode; otherwise processes all files in one go.\nb) py2dataset(): Iterates through Python files, excluding hidden ones (_pycache__ or __init__.py), manages logging levels for quiet operations, retrieves questions dictionary from pathname, clones GitHub repositories if URL input detected and sets default paths. If multiple processes aren't enabled with LLM use, executes process_single_python_file() per file to prevent memory exhaustion; else it does them simultaneously through single processing. Combines all '*.py.instruct.json'.\nc) clone_github_repo(): Controls repository operations if input is URL and prepares the working directory if current directory suits starting condition checks; updates git settings or warns if unavailable with logs, and finally yields cloned repository path after performing fetches if any recent commits present.\nd) get_bool_from_input(): Acts as user query receiver in case 'Interactive Mode', retrieving alternative settings through interactive console interactions lowering the inputs if not satisfying Booleans or preserves initial state.\ne) main(): Receives command-line arguments, sets defaults, checks repository URL validity and initiates py2dataset() execution upon valid inputs or prompts help messages when required.\n\n4. Important Variables:\na) 'use_llm', 'output_dir', 'model_config_pathname', 'detailed', 'proc': Parameters involved in process_single_python_file(), controls LLM use, output directory path, model configuration file path, child process handling for Python files processing.\nb) 'start', 'questions_pathname', 'html', 'skip_regen': py2dataset() parameters; starting directory (default current working), questions JSON pathname, HTML output flag, skipping regeneration of existing instruct.json files flag.\nc) 'params': Aggregates all arguments for both process_single_python_file() and py2dataset(), dynamically altered during user inputs in main().\nd) 'instruct_pathname', 'base_pathname', 'repo_name', 'command', 'value_segment', 'githubrepos_dir': Temporary variables within py2dataset() and clone_github_repo(); 'instruct_pathname' stores Python file path with suffix, 'base_pathname' forms local repository location from relative paths, 'repo_name' derives GitHub repo name, 'command' is a git fetch command, 'value_segment' splits input strings into tokens.\ne) 'python_pathname', 'relative_path', 'file_details', 'model_config', 'instruct_data': Intermediate results or arguments for process_single_python_file()."
            },
            {
                "from": "human",
                "value": "Output the Python code described by the code documentation."
            },
            {
                "from": "gpt",
                "value": "```python\n\"\"\"\nFor each Python file within a given directory, this module is designed to generate, save, \nand return datasets that include responses to questions about the code. \nRequirements:\n[req00] The process_single_python_file function shall:\n    a. Accept parameters for Python file path, start directory, model config \n       pathname, questions dict, use of LLM, and output dir.\n    b. If 'use_llm' is True, use 'get_model' to instantiate LLM model config.\n    c. Use 'get_python_file_details' to retrieve Python file info.\n    d. Use 'get_python_datasets' to acquire instruct.json datasets.\n    e. Use 'save_python_data' to store file details and instruct.json data.\n[req01] The py2dataset function shall:\n    a. Accept parameters for start directory, output dir, questions path, model\n       config path, use of LLM, and quiet mode.\n    b. Adjust logging level based on 'quiet'.\n    c. Use current working dir if no valid start dir is provided.\n    d. Get output dir with 'get_output_dir'.\n    e. Retrieve questions dict with 'get_questions'.\n    f. Search for Python files using 'rglob', excluding those starting with \"_\".\n    g. For each Python file, spawn a child process with 'process_single_python_file'\n       to get file details and instruct.json data, if single_process is False.\n    h. Combine instruct.json files with 'combine_json_files'.\n    i. Return datasets.\n[req02] The clone_github_repo function shall:\n    a. Accept a url.\n    b. Clone repository or fetch the latest changes.\n    c. Return local repository path.\n[req03] The main function shall:\n    a. Accept and process command-line args.\n    b. Determine py2dataset parameters based on processed arguments.\n    c. Call py2dataset with derived parameters.\n\"\"\"\nimport sys\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, List\nfrom multiprocessing import Process\nimport subprocess\nimport os\nimport git\nimport shlex\n\nfrom get_python_file_details import get_python_file_details\nfrom get_python_datasets import get_python_datasets\nfrom get_params import (\n    get_questions,\n    get_model,\n    get_output_dir,\n    get_start_dir,\n)\nfrom save_output import combine_json_files, save_python_data\n\n\ndef process_single_python_file(\n    python_pathname: str,\n    relative_path: Path,\n    output_dir: str,\n    model_config_pathname: str,\n    questions: Dict,\n    use_llm: bool,\n    model_config: Dict,\n    detailed: bool,\n) -> None:\n    \"\"\"\n    Processes a single Python file to generate question-answer pairs and instructions.\n    Args:\n        python_pathname (str): Path to the Python file.\n        start (str): Starting directory for Python files.\n        output_dir (str): Directory to write the output files.\n        model_config_pathname (str): Path and filename of the model configuration file.\n        questions (Dict): Dictionary of questions to answer about the Python file.\n        use_llm (bool): Use llm to answer code purpose question.\n        model_config (Dict): Configuration dictionary for the LLM.\n        detailed (bool): Perform detailed analysis if True.\n    \"\"\"\n    logging.info(f\"Processing file: {python_pathname}\")\n    if model_config is None and use_llm:\n        model_config = get_model(model_config_pathname)\n\n    file_details = get_python_file_details(python_pathname)\n    if not file_details:\n        logging.error(f\"Failed to get file details for {python_pathname}\")\n        return\n\n    instruct_data = get_python_datasets(\n        python_pathname,\n        file_details,\n        relative_path,\n        questions,\n        model_config,\n        detailed,\n    )\n\n    if instruct_data:\n        save_python_data(file_details, instruct_data, relative_path, output_dir)\n    else:\n        logging.error(f\"Failed getting {python_pathname} dataset\")\n\n    del instruct_data, file_details, model_config\n\n\ndef py2dataset(\n    start: str = \"\",\n    output_dir: str = \"\",\n    questions_pathname: str = \"\",\n    model_config_pathname: str = \"\",\n    use_llm: bool = False,\n    quiet: bool = False,\n    single_process: bool = False,\n    detailed: bool = False,\n    html: bool = False,\n    skip_regen: bool = False,\n) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Generates datasets by processing Python files within a specified directory.\n    Args:\n        start (str, optional): Starting directory for Python files or GitHub repository Python files. Default: current working directory.\n        output_dir (str, optional): Directory to write the output files. Default: ./dataset/.\n        questions_pathname (str, optional): Path and filename of the questions file. Default: ./py2dataset_questions.json.\n        model_config_pathname (str, optional): Path and filename of the model configuration file. Default: ./py2dataset_model_config.yaml.\n        use_llm (bool, optional): Use llm to answer code purpose question. Default: False.\n        quiet (bool, optional): Limit logging output. Default: False.\n        single_process (bool, optional): Use a single process to process Python files if --use_llm. Default: False.\n        detailed (bool, optional): Include detailed analysis. Default: False.\n        html (bool, optional): Generate HTML output. Default: False.\n        skip_regen (bool, optional): Skip regeneration of existing instruct.json files. Default: False.\n    Returns:\n        Dict[str, List[Dict]]: Generated datasets.\n    \"\"\"\n    if quiet:\n        logging.getLogger().setLevel(logging.WARNING)\n    else:\n        logging.getLogger().setLevel(logging.INFO)\n\n    sys.setrecursionlimit(3000)  # Set recursion limit higher for AST parsing\n\n    model_config = None\n    if use_llm and single_process:\n        model_config = get_model(model_config_pathname)\n\n    params = {\n        \"python_pathname\": \"\",\n        \"relative_path\": \"\",\n        \"output_dir\": get_output_dir(output_dir),\n        \"model_config_pathname\": model_config_pathname,\n        \"questions\": get_questions(questions_pathname),\n        \"use_llm\": use_llm,\n        \"model_config\": model_config,\n        \"detailed\": detailed,\n    }\n\n    for python_pathname in Path(start).rglob(\"*.py\"):\n        \n        if \"__pycache__\" in python_pathname.parts:\n            continue\n        if python_pathname.name == \"__init__.py\":\n            continue\n        \n        params[\"python_pathname\"] = str(python_pathname)\n        params[\"relative_path\"] = Path(\n            os.path.relpath(python_pathname, os.path.dirname(get_start_dir(start)))\n        )\n\n        # determine if corresponding instruct.json exists and skip if skip_regen\n        base_pathname = Path(params[\"output_dir\"]) / params[\"relative_path\"]\n        instruct_pathname = base_pathname.with_suffix(\".py.instruct.json\")\n        if instruct_pathname.exists() and skip_regen:\n            continue\n        \n        # process each python file in a separate process to manage memory\n        if params[\"model_config\"] is None and params[\"use_llm\"]:\n            proc = Process(target=process_single_python_file, kwargs=params)\n            proc.start()\n            proc.join()\n        else:  # or process all files using use a single process\n            process_single_python_file(**params)\n\n    return combine_json_files(output_dir, html, params[\"questions\"])\n\n\ndef clone_github_repo(url: str) -> str:\n    \"\"\"\n    Clone repository or fetch the latest changes and return local repository path.\n    Args:\n        url (str): The url of the github repository.\n    Returns:\n        str: The path to the cloned repository.\n    \"\"\"\n    try:\n        command = f\"git ls-remote {shlex.quote(url)}\"\n        subprocess.run(\n            command,\n            shell=True,\n            check=True,\n            stdout=subprocess.DEVNULL,\n            stderr=subprocess.DEVNULL,\n        )\n        repo_name = url.split(\"/\")[-1]\n        githubrepos_dir = os.path.join(os.getcwd(), \"githubrepos\")\n        os.makedirs(githubrepos_dir, exist_ok=True)\n        path = os.path.join(githubrepos_dir, repo_name)\n        if not os.path.exists(path):\n            git.Repo.clone_from(url, path)\n        else:\n            repo = git.Repo(path)\n            with repo.git.custom_environment(\n                GIT_SSH_COMMAND=\"ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no\"\n            ):\n                repo.git.fetch()\n                default_branch = repo.head.reference.tracking_branch().remote_head\n                repo.git.reset(\"--hard\", default_branch)\n    except subprocess.CalledProcessError:\n        logging.info(f\"Invalid or inaccessible repository: {url}\")\n        path = \"\"\n\n    return path\n\n\ndef main():\n    \"\"\"\n    Command-line entry point for processing Python files and generating datasets.\n    Optional command-line arguments:\n    --start (str, optional): Starting directory for Python files or GitHub repository Python files. Default: cwd.\n    --output_dir (str, optional): Directory to write the output files. Default: ./dataset/.\n    --questions_pathname (str, optional): Path and filename of the questions file. Default: ./py2dataset_questions.json.\n    --model_config_pathname (str, optional): Path and filename of the model configuration file. Default: ./py2dataset_model_config.yaml.\n    --use_llm (bool, optional): Use llm to answer code purpose question. Default: False.\n    --quiet (bool, optional): Limit logging output. Default: False.\n    --single_process (bool, optional): Use a single process to process Python files if --use_llm. Default: False.\n    --detailed (bool, optional): Include detailed analysis. Default: False.\n    --html (bool, optional): Generate HTML output. Default: False.\n    --I (str, optional): Enable interactive mode. Default: False.\n    --skip_regen (str, optional): Skip regeneration of existing instruct.json files. Default: False.\n    --help: Display help message.\n    \"\"\"\n    if \"--help\" in sys.argv:\n        print(__doc__)\n        sys.exit()\n\n    # return boolean value based on the user input\n    def get_bool_from_input(input_str: str, current_value: bool) -> bool:\n        if input_str.lower() in [\"t\", \"true\", \"y\", \"yes\"]:\n            return True\n        elif input_str.lower() in [\"f\", \"false\", \"n\", \"no\"]:\n            return False\n        return current_value\n\n    # Defaults\n    params = {\n        \"start\": \".\",\n        \"output_dir\": \"./dataset/\",\n        \"questions_pathname\": \"./py2dataset_questions.json\",\n        \"model_config_pathname\": \"./py2dataset_model_config.yaml\",\n        \"use_llm\": False,\n        \"quiet\": False,\n        \"single_process\": False,\n        \"detailed\": False,\n        \"html\": False,\n        \"skip_regen\": False,\n        \"I\": False,\n    }\n\n    arg_string = \" \".join(sys.argv[1:])\n    for arg in params:  # parse command-line arguments\n        if \"--\" + arg in arg_string:\n            if isinstance(params[arg], bool):\n                params[arg] = True\n                arg_string = arg_string.replace(\"--\" + arg, \"\")\n            else:\n                value_segment = arg_string.split(\"--\" + arg + \" \")[1]\n                params[arg] = value_segment.split(\" --\")[0].strip('\"')\n                arg_string = arg_string.replace(\"--\" + arg + \" \" + params[arg], \"\")\n\n    if params[\"I\"]:  # query user for parameters to change\n        print(\"Interactive mode, enter new values or press enter to keep.\")\n        for arg in params:\n            if arg != \"I\":\n                user_input = input(f\"{arg} [{params[arg]}]: \").strip()\n                if isinstance(params[arg], bool):\n                    params[arg] = get_bool_from_input(user_input, params[arg])\n                elif user_input:\n                    params[arg] = user_input\n                print(f\"{arg}: {params[arg]}\")\n    params.pop(\"I\")\n\n    if params[\"start\"].startswith(\"https://github.com/\"):\n        params[\"start\"] = clone_github_repo(params[\"start\"])\n    elif not os.path.isdir(params[\"start\"]):\n        print(f\"'{params['start']}' Invalid. Using current working directory.\")\n        params[\"start\"] = os.getcwd()\n\n    py2dataset(**params)\n\n\nif __name__ == \"__main__\":\n    main()\n\n```"
            }
        ],
        "nbytes": 18637,
        "source": "py2dataset/py2dataset.py"
    },
    {
        "conversation": [
            {
                "from": "system",
                "value": "code documentation: Overview: The given `py2dataset\\save_output.py` script involves several utility functions focused on processing data files, transforming formats, and generating graphical outputs. These utilities primarily function as data readers, writers, and converters while interacting with Python file details. Main operations include handling JSON/YAML content management through `read_file()` and `write_file()`, translating JSON files to HTML format using `convert_json_to_html()`, consolidating multiple JSON files into 'instruct.json' with duplicate removal via `combine_json_files()`, creating code graphs as PNG images utilizing `create_code_graph()`, and saving Python file data in YAML/JSON formats along with generating graphs through `save_python_data()`.\n\n1) read_file(file_path: This function reads contents from a given JSON or YAML file path, identified by 'file_type' extracted from file suffix ('json', 'yaml'), accepting an instance of 'Path' class (for accessibility with standard libraries), returns respective format contents as Python dictionary ('json.load()' for JSON; 'yaml.load(f)' for YAML).\n2) write_file(data, file_path): Accepting a dictionary and a file path as arguments, it writes data into either JSON or YAML format depending on the suffix of 'file_path'. If JSON ('json.dump(data)', if YAML SafeDumper manages potential aliases disregard ('yaml.SafeDumper'), returning 'None'.\n3) convert_json_to_html(directory): Looping over '.json' files within directory specified ('Path(directory).rglob()'), reads JSON data into 'dataset', generates HTML content preserving spaces and tabs ('preserve_spacing()') in a table format, writes the result as '.html'. Handles exceptions during file saving ('logging.info').\n4) preserve_spacing(text, tab_width): Accepts text input and tab width to replace '\\n', '\\t' with '&nbsp;', maintaining spacing in text for HTML representation (returning transformed string).\n5) combine_json_files(directory, html=bool, questions): Processes all JSON files from the provided directory excluding a few skip-names ('instruct.json', 'chat_ml.json'), composites merged data as 'combined_data'. Afterwards, generates 'document_code.json' with filtered purposeful entries ('purpose_question') and updates 'nbytes' field in each entry by encoding conversation values ('len'). It further converts JSON files to HTML format if 'html=True'. Returns a dictionary containing 'instruct_list' datasets.\n6) create_code_graph(file_details, base_name, output_subdir): Constructs a graph from file details using NetworkX library ('nx.DiGraph'), adds nodes and edges according to input data structure ('G.add_nodes_from', 'G.add_edge'). Creates graphical layout with Matplotlib ('plt.figure', 'nx.spring_layout') then saves it as '.png' in specified output directory ('output_subdir', 'plt.savefig'), managing exceptions along the way ('logging.error').\n7) save_python_data(file_details, instruct_list, relative_path, output_dir): Takes details from Python files ('file_details') as input and extracted instruction list ('instruct_list'). Handles saving as YAML and JSON in a specific subdirectory created from 'output_dir' using write_file() functions. Lastly invokes code graph generation through `create_code_graph()` while logging errors if any occurrence.\n\nFunctional Requirements:\n1) read_file(): Reads JSON or YAML files into Python dictionary format based on file extension.\n2) write_file(): Writes a given dictionary data to JSON or YAML file formats according to suffix.\n3) convert_json_to_html(): Converts JSON files within a directory to HTML format, preserving spaces and tabs in text content.\n4) combine_json_files(): Combines all JSON files into 'instruct.json', eliminates duplicates, generates training data ('document_code'), updates nbytes fields in each conversation entry and optionally converts to HTML format on 'html' input specification returning an object of combined data set (('instruct_list') extracted JSON entries.\n5) create_code_graph(): Visualizes the file graph representation with given details creating .png outputs for graph visualizations with user-specified base name and subdirectory.\n6) save_python_data(): Handles saving Python file data as YAML, generates instruction datasets as JSON files alongside graphical outputs (utilizing `create_code_graph()`) considering exceptions during execution.\n\nVariables:\nread_file(): file_path (Path), file_type, f - temporary file handler, dataset (dict), json_file (Path)\nwrite_file(): data (Dict), file_path (Path), file_type\nconvert_json_to_html(): html_content (str), tab_width (int=4), column_count (float), row_parts (list), column_width (float), html_rows (list), json_file (Path), cleaned_name (str), purpose_question (str), edge_labels (dict)\npreserve_spacing(): text (str), tab_width (int)\ncombine_json_files(): directory (str), combined_data (List[Dict]), code_filename (List[str]), skip_files (Set[str]), purpose_question (str), file_data (List), graph_type (str), chat_ml (List[dict])\ncreate_code_graph(): output_subdir (Path), edge_labels (dict), pos (dict), label (list), G (networkx.DiGraph)\nsave_python_data(): file_details (dict), instruct_list (List[Dict]), relative_path (Path), output_dir (str), output_file (Path), base_name (str), graph_type (str), edge (tuple), plt._current_fig_manager()"
            },
            {
                "from": "human",
                "value": "Output the Python code described by the code documentation."
            },
            {
                "from": "gpt",
                "value": "```python\n\"\"\"\nUtility functions for reading the input and saving the output of the py2dataset script.\nRequirements:\n[req01] The `read_file` function shall:\n        a. Accept a file path as an argument.\n        b. Read and return the contents of a JSON or YAML file as a dictionary.\n[req02] The `write_file` function shall:\n        a. Accept a dictionary and a file path as arguments.\n        b. Write the dictionary to a file in either JSON or YAML format.\n[req03] The `convert_json_to_html` function shall:\n        a. Convert JSON files within a given directory to HTML format.\n        b. Save each converted file with a .html extension.\n        c. Preserve spacing and tabs for the 'input' field.\n[req04] The `combine_json_files` function shall:\n        a. Accept a directory path as an argument.\n        b. Merge all JSON files in the directory.\n        c. Remove duplicates from the combined JSON files.\n        d. Write the combined data to 'instruct.json' files.\n        e. Convert the merged JSON files to HTML format.\n        f. Return the 'instruct_list' datasets.\n[req05] The `create_code_graph` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Generate code graphs based on the provided file details.\n        c. Save the graphs as PNG images in the specified output directory.\n[req06] The `save_python_data` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Save the details of the Python file as a YAML file.\n        c. Save the instruction data as JSON files.\n        d. Generate and save code graphs.\n\"\"\"\nimport json\nimport logging\nfrom html import escape\nfrom pathlib import Path\nfrom typing import Dict, List\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport yaml\n\n\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == \"json\":\n            return json.load(f)\n        if file_type == \"yaml\":\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}\n\n\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file.\n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open(\"w\") as f:\n        if file_type == \"json\":\n            json.dump(data, f, indent=4)\n        elif file_type == \"yaml\":\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n\n\ndef convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format and save .html file.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int = 4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(\" \", \"&nbsp;\").replace(\"\\t\", \"&nbsp;\" * tab_width)\n\n    for json_file in Path(directory).rglob(\"*.json\"):\n        try:\n            dataset = read_file(json_file)\n            if not dataset:\n                continue\n        except Exception:\n            continue\n\n        html_content = \"\"\"\n        <html>\n        <head>\n            <style>\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\n                th, td {\n                    border: 1px solid black;\n                    padding: 8px;\n                    text-align: left;\n                    white-space: pre-line;\n                    vertical-align: top;\n                    word-wrap: break-word;\n                }\n            </style>\n        </head>\n        <body>\n            <table>\n                <thead>\n                    <tr>\n        \"\"\"\n        column_count = len(dataset[0].keys())\n        column_width = round(100 / column_count, 2)\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += \"\"\"\n                    </tr>\n                </thead>\n                <tbody>\n        \"\"\"\n        html_rows = []\n        for entry in dataset:\n            row_parts = [\"<tr>\"]\n            for key in entry:\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace(\"\\n\", \"<br/>\")\n                row_parts.append(f\"<td>{value}</td>\")\n            row_parts.append(\"</tr>\")\n            html_rows.append(\"\".join(row_parts))\n        html_content += \"\".join(html_rows)\n\n        html_content += \"\"\"\n                </tbody>\n            </table>\n        </body>\n        </html>\n        \"\"\"\n        html_file_path = json_file.with_suffix(\".html\")\n        try:\n            with open(html_file_path, \"w\", encoding=\"utf-8\") as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f\"Failed saving: {html_file_path}\")\n\n\ndef combine_json_files(\n    directory: str, html: bool, questions: Dict\n) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and\n    then remove duplicates. Also generate a 'training.json' file.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n    logging.info(f\"Combining JSON files in {directory}\")\n\n    # Save instruct.json file\n    combined_data, code_filename = [], []\n    skip_files = {\"instruct.json\", \"chat_ml.json\", \"document_code.json\"}\n    for json_file in Path(directory).rglob(\"*.json\"):\n        if json_file.name in skip_files:\n            continue\n        try:\n            file_data = read_file(json_file)\n            if file_data:\n                combined_data.extend(file_data)\n                cleaned_name = (\n                    json_file.relative_to(directory)\n                    .with_suffix(\"\")\n                    .as_posix()\n                    .replace(\".instruct\", \"\")\n                )\n                code_filename.append(cleaned_name)\n        except Exception as e:\n            logging.info(f\"Failed reading: {json_file}. Error: {e}\")\n    write_file(combined_data, Path(directory) / \"instruct.json\")\n\n    # Generate document_code.json file\n    purpose_question = [\n        item[\"text\"] for item in questions if item[\"id\"] == \"file_purpose\"\n    ][0]\n    purpose_question = purpose_question.split(\"{filename}\")[0]\n    purpose_data = [\n        item\n        for item in combined_data\n        if item[\"instruction\"].startswith(purpose_question)\n    ]\n\n    document_code = [\n        {\n            \"document\": item[\"output\"],\n            \"code\": item[\"input\"],\n        }\n        for item in purpose_data\n    ]\n    for i, item in enumerate(document_code):\n        item[\"code filename\"] = code_filename[i]\n\n    write_file(document_code, Path(directory) / \"document_code.json\")\n\n    chat_ml = [\n        {\n            \"conversation\": [\n                {\"from\": \"system\", \"value\": f\"code documentation: {item['document']}\"},\n                {\n                    \"from\": \"human\",\n                    \"value\": \"Output the Python code described by the code documentation.\",\n                },\n                {\"from\": \"gpt\", \"value\": item[\"code\"]},\n            ],\n            \"nbytes\": \"0\",\n            \"source\": item[\"code filename\"],\n        }\n        for item in document_code\n    ]\n\n    # Compute the number of bytes for each conversation and update the nbytes field\n    for item in chat_ml:\n        nbytes = 0\n        for conv in item[\"conversation\"]:\n            nbytes += len(conv[\"value\"].encode(\"utf-8\"))\n        item[\"nbytes\"] = nbytes\n\n    write_file(chat_ml, Path(directory) / \"chat_ml.json\")\n\n    if html:\n        logging.info(\"Converting JSON files to HTML\")\n        convert_json_to_html(directory)\n\n    return {\"instruct_list\": combined_data}\n\n\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    # create graph\n    graph_type = \"entire_code_graph\"\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details[\"file_info\"][graph_type][\"nodes\"])\n    for edge in file_details[\"file_info\"][graph_type][\"edges\"]:\n        source, target = edge[\"source\"], edge[\"target\"]\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(\n                source,\n                target,\n                **{\n                    k: v\n                    for k, v in edge.items()\n                    if k in [\"target_inputs\", \"target_returns\"]\n                },\n            )\n\n    # draw graph\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(\n        G,\n        pos,\n        with_labels=True,\n        font_weight=\"bold\",\n        font_size=8,\n        node_shape=\"s\",\n        node_size=500,\n        width=1,\n        arrowsize=12,\n    )\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if \"target_inputs\" in edge[2] and edge[2][\"target_inputs\"]:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if \"target_returns\" in edge[2] and edge[2][\"target_returns\"]:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[(edge[0], edge[1])] = \"\\n\".join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n\n    # save graph\n    output_file = output_subdir / f\"{base_name}.{graph_type}.png\"\n    plt.savefig(output_file)\n    plt.close()\n\n\ndef save_python_data(\n    file_details: dict, instruct_list: list, relative_path: Path, output_dir: str\n) -> None:\n    \"\"\"\n    Save the details of the Python file as a YAML file, the instruction data as JSON files,\n    and generate and save code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path of the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parent\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    base_name = relative_path.name\n    write_file(instruct_list, output_subdir / f\"{base_name}.instruct.json\")\n    write_file(file_details, output_subdir / f\"{base_name}.details.yaml\")\n\n    # Generating and saving the code graph\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except Exception as e:\n        logging.error(f\"Error creating graph for {base_name}: {e}\", exc_info=True)\n\n```"
            }
        ],
        "nbytes": 16730,
        "source": "py2dataset/save_output.py"
    },
    {
        "conversation": [
            {
                "from": "system",
                "value": "code documentation: 1. Purpose and Processing Approach of Python File `setup.py`:\nThe given `setup.py` file serves as a configuration script primarily utilized by the Setuptools package during software distribution creation for Python projects. It provides essential metadata about the project, including its name, version, author details, description, license type, dependencies, and entry points while defining packages and modules required for installation. This facilitates seamless packaging and deployment of `py2dataset` into an executable format for end-users or other systems.\n\n2. API Signatures (Functional Components):\na) `get_code_graph`: Details unavailable as this is a module name within the package but not explicitly mentioned as a function or class method in the given code snippet. However, it might be part of an internal implementation for generating graph representations of Python source code structures.\n\nb) `get_params`: Similar to 'a', this module name lacks explicit API signature information but could involve retrieving parameters related to Python code analysis or dataset generation processes within the `py2dataset` toolkit.\n\nc) `get_python_datasets`: Not explicitly defined as a function or class method either, but likely responsible for generating structured datasets from Python source codes by extracting relevant data points.\n\nd) `get_python_file_details`: Again, a module name without an API signature provided in the code snippet; possibly handling detailed information extraction about individual Python files during dataset generation.\n\ne) `py2dataset` (Main Module):\ni) `main()` - Entry point function for console script execution when running `py2dataset` as a command-line tool via `python -m py2dataset`.\n\n3. Functional Requirements:\na) The core objective of py2dataset is transforming Python source codes into structured datasets that users can further analyze, interpret, or use in their analyses/workflows. It combines features like generating graph representations ('get_code_graph'), collecting parameters ('get_params'), extracting file details ('get_python_file_details'), and producing output files ('save_output').\n\nb) The software operates under Python 3.8 or higher versions, ensuring compatibility with modern programming standards and libraries' functionalities.\n\nc) Dependencies include `matplotlib` for advanced plotting support if visualizing the extracted graphs; `networkx`, an excellent choice for representing dataset relations through graph objects; `ctransformers`, a useful Natural Language Processing toolkit in contemporary machine learning practices; `PyYaml`, responsible for loading or manipulating data structured with YAML files, potentially supporting customization; lastly `GitPython` enables interacting with Git repositories, enabling source code version tracking or project analysis from repository history.\n\nd) Users can execute `py2dataset` as a console script using the command `python -m py2dataset`, launching the main function ('main()') to trigger the tool's functionality from any location after installation.\n\n4. Variables Description:\na) `long_description` - Holds contents of 'README.md', providing detailed documentation about the project in Markdown format for users to understand its purpose, usage instructions, and other relevant information.\n\nb) `name` - Identifies the package as 'py2dataset' during installation or distribution processes.\n\nc) `version` - Denotes current software version (0.4).\n\nd) `author` - Specifies Jeff Meloy as the creator of this tool with contact email 'jeffmeloy@gmail.com'.\n\ne) `description` - Briefly explains py2dataset's functionality ('A tool to generate structured datasets from Python source code').\n\nf) `long_description_content_type` - Marks file contents as text in Markdown format for easier documentation consumption and understanding.\n\ng) `url` - Directs users to the project GitHub repository (https://github.com/jeffmeloy/py2dataset), serving as additional learning material and supporting contribution discussions or issues reporting.\n\nh) `python_requires` - Lists required Python versions of '3' and higher, enabling seamless usage across newer development platforms with optimized functionality.\n\ni) `install_requires` - Specifies dependencies needed for py2dataset to function correctly: `matplotlib`, `networkx`, `ctransformers`, `PyYaml`, and `GitPython`.\n\nj) `entry_points` - Defines 'console_scripts' section mapping `py2dataset` command execution to the main() function within the package.\n\nk) `packages` - Informs Setuptools about the root directory ('py2dataset') containing Python modules/packages for installation purposes.\n\nl) `package_dir` - Relates module paths to their respective locations on disk ('.\\\\' denotes relative pathing from `setup.py`)."
            },
            {
                "from": "human",
                "value": "Output the Python code described by the code documentation."
            },
            {
                "from": "gpt",
                "value": "```python\nfrom setuptools import setup\n\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\n\nsetup(\n    name=\"py2dataset\",\n    version=\"0.4\",\n    author=\"Jeff Meloy\",\n    author_email=\"jeffmeloy@gmail.com\",\n    description=\"A tool to generate structured datasets from Python source code\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/jeffmeloy/py2dataset\",\n    py_modules=[\"get_code_graph\", \"get_params\", \"get_python_datasets\", \"get_python_file_details\", \"py2dataset\", \"save_output\"],\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires=\">=3.8\",\n    install_requires=[\"matplotlib\", \"networkx\", \"ctransformers\", \"PyYaml\", \"GitPython\", ],\n    entry_points={\"console_scripts\": [\"py2dataset = py2dataset:main\"]},\n    packages=[\"py2dataset\"],\n    package_dir={\"py2dataset\": \".\\\\\"},\n)\n\n```"
            }
        ],
        "nbytes": 5873,
        "source": "py2dataset/setup.py"
    }
]