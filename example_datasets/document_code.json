[
    {
        "document": "1) Overview and Approach of get_code_graph.py:\nThe 'get_code_graph.py' file primarily focuses on generating a graph representation of software code details extracted from given file summaries with NodeXL's networkx library alongside producing plantUML representations of its corresponding control flow tree visualization for enhanced readability. Its purpose consists of creating dictionary-structured files based upon functions and classes in terms of dependencies (NodeXL Directed Graph), extracting control flow trees from Abstract Syntax Tree (AST) nodes, reorganizing the control flow structure to match code graph starting points, and generating plantUML activity diagrams for each element.\n\n2) API Signatures with Inputs/Outputs:\na. `code_graph(file_summary: Dict[str, Union[Dict, str]] -> Dict[str, Union[List[str], Dict[str, List[str]]]:`\n- Processes file details stored in 'file_summary' dictionary containing function and class method definitions. It constructs a networkx Directed Graph (nx.DiGraph) with nodes representing functions, classes, and methods as well as edges signifying their relationships. Edge data is derived from target method details. Output returns nodes and edges representation of the code in formatted dictionary format.\nb. `get_edge_data_from_details(target_details: dict, source_details: dict, target: str) -> dict:`\n- Retrieves edge data by extracting information from target method details given source method details. Returns a dictionary containing 'source inputs' and 'target returns'.\nc. `add_edge_with_data(source: str, target: str, init_method: Optional[str] ) -> None:`\n- Adds an edge with associated data to the graph by accessing function or class method details from lookup dictionaries.\nd. `add_edges_for_calls(source_name: str, calls: List[ast.AST]) -> None:`\n- Processes calls from given nodes/elements (function names for functions, methods), builds the edges and updates nodes according to dependency tree while connecting with proper function lookup sources in a directed graph format.\ne. `extract_control_flow_tree(nodes: List[ast.AST]) -> List[Union[str, dict]]:`\n- Extracts control flow trees from Abstract Syntax Tree (AST) nodes into list form containing nested dictionaries and strings representing Python code elements.\nf. `reorganize_control_flow(code_graph: Dict[str, Union[List[str], Dict[str, List[str]], control_flow_structure: List[Union[str, dict]]) -> List[dict]:`\n- Reorders the extracted control flow structure to match code graph starting points by traversing through each element recursively. Output is a reorganized list of elements.\ng. `get_plantUML_element(element: Union[dict, str, List[str]], indentation: str=\"\") -> str:`\n- Generates plantUML code for each element in the control flow structure with given indentation level. Returns plantUML representation of elements.\nh. `get_plantUML(control_flow_structure: List[Union[str, dict]]) -> str:`\n- Produces overall plantUML activity diagram code from the entire control flow structure list obtained earlier.\ni. `get_code_graph(file_summary: Dict[str, Union[Dict, str], file_ast: ast.AST) -> Tuple[Dict[str, List[Union[str, dict]], List[Union[str, dict]], str:`\n- Combines code graph generation with control flow extraction and plantUML creation for given 'file_summary' dictionary and Abstract Syntax Tree ('file_ast'). Outputs entire code graph, reorganized control flow structure, and plantUML representation as a tuple.\n\n3) Functional Requirements Breakdown:\n- Function `code_graph()` focuses on parsing file summary for class methods & functions (with node creations), determining call relations in graphs. Edge details come from lookup dictionaries (class/function).\n- `get_edge_data_from_details()` derives edge data from target method info while handling inputs and returns related to sources.\n- `add_edge_with_data()` establishes directed edges using the gathered data within function details lookup sources/targets.\n- `add_edges_for_calls()` navigates call nodes recursively based on functions and class dependencies, constructing edge relations accordingly.\n- `extract_control_flow_tree()` breaks down AST into control flow structure by parsing nodes & generating dictionary tree.\n- `reorganize_control_flow()` sorts control flow structure elements in a way that matches the code graph's starting points for readability purposes.\n- `get_plantUML_element()` creates plantUML visualization of each element considering indentation levels.\n- `get_plantUML()` assembles comprehensive plantUML activity diagrams from the reorganized control flow structure.\n- `get_code_graph()` consolidates all these processes, merging code graph creation with control flow extraction and plantUML generation for a given file summary & AST input.\n\n4) Variables Description:\na. G (networkx Directed Graph): Represents the code relationships with nodes for classes, methods and functions\nb. function_details_lookup / class_method_details_lookup (dictionary): Stored data associated to specific elements/keys as detailed summary nodes ('class_defs' & 'function_defs')\nc. nodes / edges: Used within code graph construction, holding node list and edge data for representation output\nd. source / target / init_method / fully_qualified_name / keyword / node_type / element / remaining / organized / starting_points / targets / control_flow_tree / indentation / value / try_block / except_block / else_block / finally_block: Intermediate variables during function execution\ne. nodes_to_remove: Removes unnecessary nodes without graph connections\nf. node_keywords_map (dict): Maps Python keywords to their respective types ('def', 'class', etc.)\ng. control_flow_structure: Control flow tree list from AST traversal\nh. plantuml_str / indentation: String buffers for plantUML code generation",
        "code": "```python\n\"\"\"\nRequirements\n[req01] The code_graph function performs the following:\n        a. Extracts the function and class method details from the file summary.\n        b. Creates a lookup dictionary for function and class method details.\n        c. Creates a directed graph with nodes and edges representing the relationships in the code.\n        d. Adds edges for function and class method calls.\n        e. Adds edge data to edges.\n        f. Returns a dictionary with nodes and edges representing the relationships in the code.\n[req02] The extract_control_flow_tree function performs the following:\n        a. Extracts control flow tree from AST.\n        b. Returns control flow tree.\n[req03] The reorganize_control_flow function performs the following:\n        a. Gets starting points from the code graph.\n        b. Reorganizes the control flow structure recursively.\n        c. Returns reorganized control flow structure.\n[req04] The get_plantUML_element function performs the following:\n        a. Gets plantUML code for each element.\n        b. Returns plantUML code for each element.\n[req05] The get_plantUML function performs the following: \n        a. Gets plantUML code for entire file.\n        b. Returns plantUML code for entire file.\n[req06] The get_code_graph function performs the following:\n        a. Gets the entire code graph\n        b. Gets the control flow structure\n        c. Gets the plantUML code\n        d. Returns entire code graph, control flow structure and plantUML code\n\"\"\"\nimport ast\nimport json\nfrom typing import Dict, List, Optional, Union\nimport networkx as nx\n\n\ndef code_graph(\n    file_summary: Dict[str, Union[Dict, str]],\n) -> Dict[str, Union[List[str], Dict[str, List[str]]]]:\n    \"\"\"\n    Create a dictionary representation of file details.\n    Args:\n        file_summary: Dict[str, Union[Dict, str]]: The details extracted from the file.\n    Returns:\n        dict: A dictionary with nodes and edges representing the relationships in the code.\n    \"\"\"\n    G = nx.DiGraph()\n\n    # Create lookup dictionaries for function and class method details\n    function_details_lookup = {}\n    for function_def in file_summary[\"function_defs\"]:\n        function_details_lookup.update(function_def)\n    class_method_details_lookup = {}\n    for class_def in file_summary[\"class_defs\"]:\n        for (\n            class_name,\n            class_details,\n        ) in class_def.items():  # Extract class name and details\n            G.add_node(class_name)  # Add class as a graph node\n            for method_name, method_details in class_details[\"method_defs\"].items():\n                qualified_method_name = (\n                    f\"{class_name}.{method_name}\"  # Create method fully qualified name\n                )\n                G.add_node(qualified_method_name)  # Add method as a graph node\n                class_method_details_lookup[\n                    qualified_method_name\n                ] = method_details  # Store method details\n                G.add_edge(\n                    class_name, qualified_method_name\n                )  # Add edge from class to method\n\n    # Helper function to extract edge data from target details\n    def get_edge_data_from_details(\n        target_details: dict, source_details: dict, target: str\n    ) -> dict:\n        edge_data = {}\n        if target_details:\n            edge_data[\"target_inputs\"] = target_details.get(\"inputs\")\n            edge_data[\"target_returns\"] = list(set(target_details.get(\"returns\", [])))\n        if (\n            source_details\n            and \"call_inputs\" in source_details\n            and target in source_details[\"call_inputs\"]\n        ):\n            edge_data[\"target_inputs\"] = source_details[\"call_inputs\"][target]\n        return edge_data\n\n    # Helper function to add edge with data\n    def add_edge_with_data(\n        source: str, target: str, init_method: Optional[str] = None\n    ) -> None:\n        target_details = class_method_details_lookup.get(\n            init_method or target\n        ) or function_details_lookup.get(target)\n        source_details = function_details_lookup.get(\n            source\n        ) or class_method_details_lookup.get(source)\n        G.add_edge(\n            source,\n            target,\n            **get_edge_data_from_details(target_details, source_details, target),\n        )\n\n    # Helper function to add edges for function or class method calls\n    def add_edges_for_calls(source_name, calls):\n        class_names = [\n            list(class_def.keys())[0] for class_def in file_summary[\"class_defs\"]\n        ]\n        for called in calls:\n            called_class_name = called.split(\".\")[0]\n            if called.startswith(\"self.\"):\n                method_name = called.replace(\"self.\", \"\")\n                fully_qualified_name = f\"{source_name.split('.')[0]}.{method_name}\"\n                if fully_qualified_name in class_method_details_lookup:\n                    add_edge_with_data(source_name, fully_qualified_name)\n                    continue\n            if (\n                called in function_details_lookup\n                or called in class_method_details_lookup\n                or f\"{source_name.split('.')[0]}.{called}\"\n                in class_method_details_lookup\n            ):\n                add_edge_with_data(source_name, called)\n            elif called_class_name in class_names:\n                init_method = None\n                init_method_name = f\"{called}.__init__\"\n                if init_method_name in class_method_details_lookup:\n                    init_method = init_method_name\n                add_edge_with_data(source_name, called, init_method)\n            else:\n                G.add_node(called)\n                add_edge_with_data(source_name, called)\n\n    # Add function nodes to graph and edges for function calls\n    for function_name in function_details_lookup:\n        G.add_node(function_name)\n    for func_name, details in function_details_lookup.items():\n        add_edges_for_calls(func_name, details[\"calls\"])\n\n    # Add edges for method calls\n    for qualified_method_name, details in class_method_details_lookup.items():\n        add_edges_for_calls(qualified_method_name, details[\"calls\"])\n\n    # Add edge data to edges and create node and edges to return\n    for edge in G.edges:\n        source, target = edge\n        target_details = function_details_lookup.get(\n            target\n        ) or class_method_details_lookup.get(target)\n        source_details = function_details_lookup.get(\n            source\n        ) or class_method_details_lookup.get(source)\n        edge_data = get_edge_data_from_details(target_details, source_details, target)\n        G[source][target].update(edge_data)\n    nodes = list(G.nodes)\n    edges = [\n        {\"source\": edge[0], \"target\": edge[1], **edge[2]} for edge in G.edges.data()\n    ]\n\n    # remove any nodes that are not either a source of an edge or a target of an edge\n    nodes_to_remove = []\n    for node in nodes:\n        if node not in [edge[\"source\"] for edge in edges] and node not in [\n            edge[\"target\"] for edge in edges\n        ]:\n            nodes_to_remove.append(node)\n\n    return {\"nodes\": nodes, \"edges\": edges}\n\n\ndef extract_control_flow_tree(nodes: List[ast.AST]) -> List[Union[str, dict]]:\n    \"\"\"\n    Extract control flow tree from AST.\n    Args:\n        nodes: AST nodes\n    Returns:\n        control_flow_tree: control flow tree\n    \"\"\"\n    control_flow_tree = []\n    node_keywords_map = {\n        ast.FunctionDef: \"def\",\n        ast.AsyncFunctionDef: \"def\",\n        ast.ClassDef: \"class\",\n        ast.While: \"while\",\n        ast.For: \"for\",\n        ast.AsyncFor: \"for\",\n        ast.With: \"with\",\n        ast.AsyncWith: \"with\",\n        ast.Return: \"return\",\n        ast.If: \"if\",\n        ast.Try: \"try\",\n    }\n\n    for node in nodes:\n        node_type = type(node)\n        if node_type not in node_keywords_map:\n            control_flow_tree.append(ast.unparse(node))\n            continue\n        keyword = node_keywords_map[node_type]\n\n        if keyword == \"def\":\n            args_str = \", \".join([ast.unparse(arg) for arg in node.args.args])\n            key = f\"def {node.name}({args_str})\"\n            value = extract_control_flow_tree(node.body)\n            control_flow_tree.append({key: value})\n        elif keyword == \"class\":\n            key = f\"class {node.name}\"\n            value = extract_control_flow_tree(node.body)\n            control_flow_tree.append({key: value})\n        elif keyword == \"while\":\n            key = f\"while {ast.unparse(node.test)}\"\n            value = extract_control_flow_tree(node.body)\n            control_flow_tree.append({key: value})\n        elif keyword == \"for\":\n            key = f\"for {ast.unparse(node.target)} in {ast.unparse(node.iter)}\"\n            value = extract_control_flow_tree(node.body)\n            control_flow_tree.append({key: value})\n        elif keyword == \"with\":\n            key = f\"with {', '.join([ast.unparse(item) for item in node.items])}\"\n            value = extract_control_flow_tree(node.body)\n            control_flow_tree.append({key: value})\n        elif keyword == \"return\":\n            key = \"return\"\n            value = [ast.unparse(node.value)] if node.value is not None else []\n            control_flow_tree.append({key: value})\n        elif keyword == \"if\":\n            key = f\"if {ast.unparse(node.test)}\"\n            value = extract_control_flow_tree(node.body)\n            if_block = {key: value}\n            orelse = node.orelse\n            while orelse and isinstance(orelse[0], ast.If):\n                key = f\"elif {ast.unparse(orelse[0].test)}\"\n                value = extract_control_flow_tree(orelse[0].body)\n                if_block[key] = value\n                orelse = orelse[0].orelse\n            if orelse:\n                key = \"else\"\n                value = extract_control_flow_tree(orelse)\n                if_block[key] = value\n            control_flow_tree.append(if_block)\n        elif keyword == \"try\":\n            try_block = extract_control_flow_tree(node.body)\n            except_block = []\n            for handler in node.handlers:\n                h_type = ast.unparse(handler.type) if handler.type is not None else \"\"\n                h_name = (\n                    ast.unparse(handler.name)\n                    if isinstance(handler.name, ast.Name)\n                    else \"\"\n                )\n                key = f\"except {h_type} as {h_name}:\"\n                value = extract_control_flow_tree(handler.body)\n                except_block.append({key: value})\n            control_flow_dict = {\"try\": try_block, \"except\": except_block}\n            if node.orelse:\n                else_block = extract_control_flow_tree(node.orelse)\n                control_flow_dict[\"else\"] = else_block\n            if node.finalbody:\n                finally_block = extract_control_flow_tree(node.finalbody)\n                control_flow_dict[\"finally\"] = finally_block\n            control_flow_tree.append(control_flow_dict)\n        else:\n            control_flow_tree.append(ast.unparse(node))\n\n    return control_flow_tree\n\n\ndef reorganize_control_flow(code_graph, control_flow_structure):\n    \"\"\"\n    Reorganize control flow structure to match the code graph.\n    Args:\n        file_details: file details\n        control_flow_structure: control flow structure\n    Returns:\n        reorganized_control_flow_structure: reorganized control flow structure\n    \"\"\"\n    # Get starting points from the code graph as those\n    targets = [edge[\"target\"] for edge in code_graph[\"edges\"]]\n    starting_points = [node for node in code_graph[\"nodes\"] if node not in targets]\n\n    # Define a function to reorganize the structure recursively\n    def reorganize_structure(structure, start_points):\n        organized, seen = [], set()\n\n        # Iterate through each start point and find matching elements in structure\n        for start in start_points:\n            for element in structure:\n                if isinstance(element, dict):\n                    key = next(iter(element))  # Get the first key of the dictionary\n                    if (\n                        start in key\n                    ):  # Add element if it matches the start point and hasn't been seen\n                        element_id = json.dumps(element)\n                        if element_id not in seen:\n                            organized.append(element)\n                            seen.add(element_id)\n                elif (\n                    isinstance(element, str) and start in element\n                ):  # Handle string elements\n                    organized.append(element)\n\n        # Append elements not included in the organized list\n        remaining = [elem for elem in structure if json.dumps(elem) not in seen]\n        organized.extend(remaining)\n        return organized\n\n    # Reorganize the control flow structure recursively\n    return reorganize_structure(control_flow_structure, starting_points)\n\n\ndef get_plantUML_element(element, indentation=\"\"):\n    \"\"\"\n    Get plantUML code for each element.\n    Args:\n        element: element\n        indentation: current indentation level\n    Returns:\n        plantuml_str: plantUML code for each element\n    \"\"\"\n    plantuml_str = \"\"\n    if isinstance(element, dict):\n        key = next(iter(element))\n        value = element[key]\n        if key.startswith(\"def \") or key.startswith(\"async def \"):\n            plantuml_str += f\"{indentation}def {value} {{\\n\"\n            inner_indentation = indentation + \"  \"\n        elif key.startswith(\"class \"):\n            plantuml_str += f\"{indentation}class {value} {{\\n\"\n            inner_indentation = indentation + \"  \"\n        elif key.startswith(\"if \"):\n            plantuml_str += f\"{indentation}if ({value}) {{\\n\"\n            inner_indentation = indentation + \"  \"\n        elif (\n            key.startswith(\"for \")\n            or key.startswith(\"while \")\n            or key.startswith(\"asyncfor \")\n        ):\n            plantuml_str += f\"{indentation}while ({value}) {{\\n\"\n            inner_indentation = indentation + \"  \"\n        elif key.startswith(\"try \"):\n            plantuml_str += f\"{indentation}try {{\\n\"\n            inner_indentation = indentation + \"  \"\n        elif key.startswith(\"except \"):\n            plantuml_str += f\"{indentation}catch ({value}) {{\\n\"\n            inner_indentation = indentation + \"  \"\n        else:\n            plantuml_str += f\"{indentation}:{key};\\n\"\n            inner_indentation = indentation\n\n        if isinstance(value, list):\n            for child in value:\n                plantuml_str += get_plantUML_element(child, inner_indentation)\n\n        if key.startswith(\"def \") or key.startswith(\"class \"):\n            plantuml_str += f\"{indentation}}}\\n\"\n        elif (\n            key.startswith(\"if \")\n            or key.startswith(\"for \")\n            or key.startswith(\"while \")\n            or key.startswith(\"asyncfor \")\n            or key.startswith(\"try \")\n            or key.startswith(\"except \")\n        ):\n            plantuml_str += f\"{indentation}}}\\n\"\n\n    elif isinstance(element, str):\n        plantuml_str += f\"{indentation}:{element};\\n\"\n\n    return plantuml_str\n\n\ndef get_plantUML(control_flow_structure):\n    \"\"\"\n    Get plantUML activity diagram code for entire file.\n    Args:\n        file_details: file details\n    Returns:\n        plantuml_str: plantUML code for entire file\n    \"\"\"\n    plantuml_str = \"@startuml\\n\"\n    for element in control_flow_structure:\n        plantuml_str += get_plantUML_element(element, \"  \")\n    plantuml_str += \"end\\n@enduml\"\n    return plantuml_str\n\n\ndef get_code_graph(file_summary, file_ast):\n    \"\"\"\n    Add code graph and control flow to file details.\n    Args:\n        file_details: file details\n    Returns:\n        file_details: file details\n    \"\"\"\n    try:\n        entire_code_graph = code_graph(file_summary)\n        control_flow_tree = extract_control_flow_tree(file_ast.body)\n        control_flow_structure = reorganize_control_flow(\n            entire_code_graph, control_flow_tree\n        )\n        plantUML = get_plantUML(control_flow_structure)\n    except Exception as e:\n        control_flow_structure = [str(e)]\n        plantUML = str(e)\n\n    #print('file_summary:', file_summary)\n    #print('entire_code_graph:', entire_code_graph)\n    #print('control_flow_structure:', control_flow_structure)\n    #print('plantUML:', plantUML)\n\n    return entire_code_graph, control_flow_structure, plantUML\n\n```",
        "code filename": "py2dataset/get_code_graph.py"
    },
    {
        "document": "nFunctionality Summary:\nThis module contains several helper functions used to manage configuration data retrieval for py2dataset software. These include obtaining default questions list, model configurations, setting up directories for output files and loading or writing JSON/YAML configs, as well as instantiating the large language model using the specified parameters. Functions like `get_start_dir` set the directory from where the search begins; `get_output_dir` ensures the given path is valid and exists. `get_questions` reads question list either from file or defaults to a hardcoded JSON object, while `instantiate_model` creates an instance of the specified model class using provided parameters. `get_model` loads model configuration from file or uses default settings. Finally, `write_questions_file` and `write_model_config_file` write JSON/YAML files respectively in the given directory or current working directory if none is supplied.\n\nParameters:\n- `get_start_dir` accepts an optional string argument `start_dir`.\n- `get_output_dir` takes an optional string argument `output_dir`.\n- `get_questions` requires a string `questions_pathname`.\n- `instantiate_model` demands dictionary `model_config`.\n- `get_model` optionally takes `model_config_pathname`.\n- `write_questions_file` can take an optional string argument `output_dir`.\n- `write_model_config_file` also accepts `output_dir`.\n- `get_default_questions` returns a list of default questions as dictionaries.\n- `get_default_model_config` gives the dictionary with model settings defaults.\n'''",
        "code": "```python\n\"\"\"\nObtain data parameter and model from the py2dataset functions.\nRequirements:\n[req01] The `get_default_questions` function shall:\n        a. Return a list of default questions.\n        b. Ensure each question in the list is a dictionary.\n        c. Ensure each dictionary has the keys: id, text, and type.\n[req02] The `get_default_model_config` function shall:\n        a. Return a dictionary representing the default model configuration.\n[req03] The `get_output_dir` function shall:\n        a. Accept an optional output_dir argument.\n        b. Return the absolute path of the provided output_dir if it exists or can be created.\n        c. Return the default OUTPUT_DIR if the provided output_dir argument is not provided or invalid.\n[req04] The `get_questions` function shall:\n        a. Accept an optional questions_pathname argument.\n        b. Validate the question file provided by the questions_pathname.\n        c. Return the questions from the provided questions_pathname if valid.\n        d. Return default questions if the questions_pathname is not provided or invalid.\n[req05] The `instantiate_model` function shall:\n        a. Accept a model_config dictionary as an argument.\n        b. Import the specified module and class from the model_config.\n        c. Instantiate and return the model using the provided configuration.\n[req06] The `get_model` function shall:\n        a. Accept an optional model_config_pathname argument.\n        b. Validate the model config file provided by the model_config_pathname.\n        c. Return an instantiated model and a prompt template based on the provided configuration.\n        d. Return an instantiated model and a prompt template based on the default model configuration if the model_config_pathname is not provided or invalid.\n[req07] The `write_questions_file` function shall:\n        a. Accept an optional output_dir argument.\n        b. Write the default questions to the QUESTIONS_FILE in the specified directory.\n        c. Write the default questions to the QUESTIONS_FILE in the current working directory if the output_dir argument is not provided or invalid.\n[req08] The `write_model_config_file` function shall:\n        a. Accept an optional output_dir argument.\n        b. Write the default model configuration to the MODEL_CONFIG_FILE in the specified directory.\n        c. Write the default model configuration to the MODEL_CONFIG_FILE in the current working directory if the output_dir argument is not provided or invalid.\n\"\"\"\nimport os\nimport json\nimport logging\nimport importlib\nfrom typing import Dict, List\nfrom pathlib import Path\nimport yaml\n\n# Setting up a basic logger\nlogging.basicConfig(level=logging.INFO)\n\n# defaults if provided inputs fail\nQUESTIONS_FILE = \"py2dataset_questions.json\"\nMODEL_CONFIG_FILE = \"py2dataset_model_config.yaml\"\nOUTPUT_DIR = \"datasets\"\n\n\ndef get_default_questions() -> List[Dict]:\n    \"\"\"Return default question list\n    Args:\n        None\n    Returns:\n        List[Dict]: The default question list\n    \"\"\"\n    questions = [\n        {\n            \"id\": \"file_dependencies\",\n            \"text\": \"Dependencies in Python file: `{filename}`?\",\n            \"type\": \"file\",\n        },\n        {\n            \"id\": \"entire_code_graph\",\n            \"text\": \"Call code graph in Python file: `{filename}`?\",\n            \"type\": \"file\",\n        },\n        {\n            \"id\": \"file_functions\",\n            \"text\": \"Functions in Python file: `{filename}`?\",\n            \"type\": \"file\",\n        },\n        {\n            \"id\": \"file_classes\",\n            \"text\": \"Classes in Python file: `{filename}`?\",\n            \"type\": \"file\",\n        },\n        {\n            \"id\": \"function_inputs\",\n            \"text\": \"Inputs to `{function_name}` in Python file: `{filename}`?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"function_docstring\",\n            \"text\": \"Docstring of `{function_name}` in Python file: `{filename}`?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"function_calls\",\n            \"text\": \"Calls made in `{function_name}` in Python file: `{filename}`?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"function_variables\",\n            \"text\": \"Variables in `{function_name}` in Python file: `{filename}`?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"function_returns\",\n            \"text\": \"Returns from `{function_name}` in Python file: `{filename}`?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"class_methods\",\n            \"text\": \"Methods in `{class_name}` in Python file: `{filename}`?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"class_docstring\",\n            \"text\": \"Docstring of `{class_name}` in Python file: `{filename}`?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"class_attributes\",\n            \"text\": \"Attributes of `{class_name}` in Python file: `{filename}`?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"class_inheritance\",\n            \"text\": \"Inheritance of `{class_name}` in Python file: `{filename}`?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"method_inputs\",\n            \"text\": \"Inputs to `{method_name}` in Python file: `{filename}`?\",\n            \"type\": \"method\",\n        },\n        {\n            \"id\": \"method_docstring\",\n            \"text\": \"Docstring of `{method_name}` in Python file: `{filename}`?\",\n            \"type\": \"method\",\n        },\n        {\n            \"id\": \"method_calls\",\n            \"text\": \"Calls made in `{method_name}` in Python file: `{filename}`?\",\n            \"type\": \"method\",\n        },\n        {\n            \"id\": \"method_variables\",\n            \"text\": \"Variables in `{method_name}` in Python file: `{filename}`?\",\n            \"type\": \"method\",\n        },\n        {\n            \"id\": \"method_returns\",\n            \"text\": \"Returns from `{method_name}` in Python file: `{filename}`?\",\n            \"type\": \"method\",\n        },\n        {\n            \"id\": \"file_purpose\",\n            \"text\": \"1) Describe the overall Purpose and Processing Approach of Python file: `{filename}`; 2) Define API Signatures for each applicable Function and Class Method; 3) Derive Functional Requirements to specify the code's behavior and purpose; 4) Explain what each variable does in the code.\",\n            \"type\": \"file\",\n        },\n    ]\n    return questions\n\n\ndef get_default_model_config() -> Dict:\n    \"\"\"Return default model config dict\n    Args:\n        None\n    Returns:\n        Dict: The default model config dictionary\n    \"\"\"\n    model_config = {\n        \"system_prompt\": \"Lang: English. Format: unformatted, outline. Task: Create structured software documentation using this code Context:\\n'{context}'\\n\",\n        \"instruction_prompt\": \"Create outline structured code documentation for these objects:\\n'{code_objects}'\\n to comply with this instruction:\\n'{query}'\\n\",\n        \"prompt_template\": \"system:\\n{system_prompt}\\n\\ninstruction:n{instruction_prompt}\\n\\ndocumentation:\\n\", \n        \"inference_model\": {\n            \"model_import_path\": \"ctransformers.AutoModelForCausalLM\",\n            \"model_inference_function\": \"from_pretrained\",\n            \"model_params\": {\n                \"model_path\": \"jeffmeloy/WestLake-7B-v2.Q8_0.gguf\",\n                \"model_type\": \"mistral\",\n                \"local_files_only\": False,\n                ## MODEL CONFIGURATION PARAMETERS (set for current model with: GPU 4090-24GB VRAM, CPU 5950x-32 threads, 64GB RAM)\n                # avx2 and gpu_layers are not compatible\n                # \"lib\": \"avx2\",\n                \"threads\": 16,\n                \"batch_size\": 128,\n                \"context_length\": 20000,\n                \"max_new_tokens\": 20000,\n                \"gpu_layers\": 100,\n                \"reset\": True,\n            },\n        },\n    }\n    return model_config\n\n\ndef get_start_dir(start_dir: str = \"\") -> str:\n    \"\"\"\n    Returns the appropriate start directory.\n    Args:\n        start_dir (str): The directory to start the search from.\n    Returns:\n        str: The absolute path of the provided start_dir if it exists or can be created.\n    \"\"\"\n    if start_dir and not Path(start_dir).is_dir():\n        logging.info(f\"Setting Start Dir : {start_dir}\")\n        start_dir = os.getcwd()\n    else:\n        start_dir = os.path.abspath(start_dir)\n    return start_dir\n\n\ndef get_output_dir(output_dir: str = \"\") -> str:\n    \"\"\"Returns the appropriate output directory.\n    Args:\n        output_dir (str): The directory to write the output to.\n    Returns:\n        str: The absolute path of the provided output_dir if it exists or can be created.\n    \"\"\"\n    output_dir = os.path.abspath(output_dir or OUTPUT_DIR)\n    os.makedirs(output_dir, exist_ok=True)\n    logging.info(f\"Using output directory: {output_dir}\")\n    return output_dir\n\n\ndef get_questions(questions_pathname: str) -> List[Dict]:\n    \"\"\"\n    Get questions from file or default\n    Args:\n        questions_pathname (str): The pathname of the questions file\n    Returns:\n        List[Dict]: The list of questions\n    \"\"\"\n    try:  # get questions from provided or default configuration file\n        if not questions_pathname:\n            questions_pathname = os.path.join(os.getcwd(), QUESTIONS_FILE)\n        with open(questions_pathname, \"r\") as f:\n            questions = json.load(f)\n        logging.info(f\"Using questions from file: {questions_pathname}\")\n    except (FileNotFoundError, json.decoder.JSONDecodeError):\n        logging.info(\n            f\"Questions file not valid: {questions_pathname} Using default questions\"\n        )\n        questions = get_default_questions()\n    return questions\n\n\ndef instantiate_model(model_config: Dict) -> object:\n    \"\"\"\n    Imports and instantiates a model based on the provided configuration.\n    Args:\n        model_config (dict): model configuration dictionary.\n    Returns:\n        object: An instance of the specified model class, or None if error.\n    \"\"\"\n    try:\n        module_name, class_name = model_config[\"model_import_path\"].rsplit(\".\", 1)\n        ModelClass = getattr(importlib.import_module(module_name), class_name)\n        model_params = model_config[\"model_params\"]\n        inference_function_name = model_config[\"model_inference_function\"]\n        if inference_function_name != \"\":\n            inference_function = getattr(ModelClass, inference_function_name)\n            llm = inference_function(model_params.pop(\"model_path\"), **model_params)\n        else:\n            llm = ModelClass(model_params.pop(\"model_path\"), **model_params)\n        return llm\n    except (ImportError, AttributeError, Exception) as e:\n        logging.info(f\"Failed to instantiate the model. Error: {e}\")\n        return None\n\n\ndef get_model(model_config_pathname: str) -> object:\n    \"\"\"\n    Returns an instantiated model and prompt template based on the model configuration.\n    Agrs:\n        model_config_pathname (str): The pathname of the model config file\n    Returns:\n        Tuple[object, str]: The instantiated model\n    \"\"\"\n    try:\n        if not model_config_pathname:\n            model_config_pathname = os.path.join(os.getcwd(), MODEL_CONFIG_FILE)\n        with open(model_config_pathname, \"r\") as config_file:\n            model_config = yaml.safe_load(config_file)\n        logging.info(f\"Using model config from file: {model_config_pathname}\")\n    except (FileNotFoundError, yaml.YAMLError):\n        logging.info(\n            f\"Model config file not valid: {model_config_pathname} Using default model config\"\n        )\n        model_config = get_default_model_config()\n    model_config[\"model\"] = instantiate_model(model_config[\"inference_model\"])\n\n    return model_config\n\n\ndef write_questions_file(output_dir: str = \"\") -> None:\n    \"\"\"\n    Writes the default questions to a file in JSON format.\n    Args:\n        output_dir (str): The directory to write the questions file to.\n    Returns:\n        None\n    \"\"\"\n    questions = get_default_questions()\n    output_dir = output_dir if output_dir and Path(output_dir).is_dir() else os.getcwd()\n    with open(os.path.join(output_dir, QUESTIONS_FILE), \"w\") as file:\n        json.dump(questions, file, indent=4)\n\n\ndef write_model_config_file(output_dir: str = \"\") -> None:\n    \"\"\"\n    Writes the default model config to a file in YAML format.\n    Args:\n        output_dir (str): The directory to write the model config file to.\n    Returns:\n        None\n    \"\"\"\n    model_config = get_default_model_config()\n    output_dir = output_dir if output_dir and Path(output_dir).is_dir() else os.getcwd()\n    with open(os.path.join(output_dir, MODEL_CONFIG_FILE), \"w\") as file:\n        yaml.dump(model_config, file)\n\n```",
        "code filename": "py2dataset/get_params.py"
    },
    {
        "document": "1. Overview of py2dataset\\get_python_datasets.py:\nThis Python script primarily focuses on generating JSON formatted question-answer pairs extracted from a given Python file along with instructions for understanding its contents better. It utilizes a DatasetGenerator class and several supporting functions to accomplish this task. The main functionality is encapsulated within the `get_python_datasets` function that instantiates the DatasetGenerator object using provided inputs, calls its generate method, and returns the instruct_list which is generated output as tuple comprising structured code data followed by the extracted instructions for specific question answers regarding files, classes, functions, and methods contained in a Python source file.\n\n2. Key Function API Signatures:\na. clean_and_unique_elements(input_str): Cleans input string to return a new string consisting of unique elements with whitespace trimmed from the start and end. It enlists tokens generated through loops using 'enumerate', 'strip' operations, and iterates over 'element_generator'.\nb. DatasetGenerator(__init__): Initializes the DatasetGenerator class instance accepting parameters such as Python file path, details about the Python file, base name of the file, question list, model configuration (language model setup), and detailed flag. If a language model is specified in the configuration, llm usage and detailed processing are enabled else defaults to off. It sets various attributes including question_mapping dictionary mapping question types to relevant keys within the Python file information dictionary and two instance lists namely instruct_list and code_qa_response which stores instruction data with their corresponding responses during the extraction process.\nc. get_response_from_llm(query:str, context:str): Retrieves language model response for a given query in context using predefined strategies to manage context length constraints. It tries different prompt templates based on context size and attempts to generate output from the language model. If successful, it returns the cleaned LLM generated string with inline commentary indicating successful operations along the process flow. Otherwise raises errors on failures.\nd. get_code_qa(): Acquires code responses by appending question answer pairs from instruct_list to the instance attributes: `code_qa_list` holding QnA structure list while concatenating necessary content within a singular 'self.code_qa_response'. It sorts codes graphically depicting how extraction operates distinctively than language model based responses.\ne. process_question(question_type:str, question_id:str, query:str, context:Dict[Any], info:Dict): Processes various types of questions such as file related, function-specific, class/method related with suitable input processing, generating context using specific info dictionary values from Python file details and retrieving response via 'get_response_from_llm', or applying clean_and_unique_elements where needed. This method sorts instruct_list based on input length in reverse order for better readability.\nf. get_info_string(info:Dict, item_type:str): Returns a string representation of values from info dictionary corresponding to the given item type by joining unique elements extracted through list comprehension and splitting comma separated strings.\ng. process_question_type(question_type:str, question_id:str, question_text:str): Handles processing for file types like `method` and `function` besides inherited behavior of generic question-based calls via query preparation within distinct conditions along with manipulations relevant to type selected by passing instance data between iterators through closure to final query crafting stage followed by calling process_question.\nh. generate(): Main method of DatasetGenerator class that processes all questions and returns instruct_list as tuple containing extracted information in JSON format.\ni. get_python_datasets(file_path:str, file_details:Dict, base_name:str, questions:List[Dict], model_config:Dict, detailed:bool): This top-level function instantiates DatasetGenerator using inputs provided and triggers generate method execution returning tuple with extracted information in JSON format.\n\n3. Functional Requirements Breakdown:\na. file_path represents the Python file path for extraction purpose.\nb. file_details contains details about the Python file structure required to answer questions.\nc. base_name refers to the base name of the Python file.\nd. questions is a list of question dictionary inputs for generating responses.\ne. model_config specifies language model configuration (if used) along with prompt template and inference parameters.\nf. detailed flag determines whether detailed information should be extracted from the Python code or not.\n\n4. Key Variables Explanation:\na. file_path: Path to the Python file for extraction process.\nb. file_details: Dictionary holding Python file details required for question answering.\nc. base_name: Base name of the Python file.\nd. questions: List containing question inputs for generating responses.\ne. model_config: Configuration dict for language model usage (if any).\nf. detailed: Flag indicating whether to extract detailed information or not.\ng. llm: Language Model object instance used for response generation (when configured).\nh. use_llm: Flag representing LLM presence in configuration.\ni. instruct_list: List containing instruction-response pairs generated during extraction.\nj. question_mapping: Maps question types to relevant keys within file details dictionary.\nk. code_qa_list: List of code question-answer pairs extracted from instruct_list and updated during `get_code_qa` call.\nl. code_qa_response: Structured text representation of code QnA pairs obtained from instruct_list sorting.\nm. Prominent methods within DatasetGenerator class like get_response_from_llm, get_info_string are involved in question processing based on query type (file, function or method) while interacting with file details and manipulating instance variables accordingly. Each handles input validations with conditions & relevant method chaining accordingly during query answer formatting operations within defined formats leading to updated output values saved under self variables ultimately generating a response if necessary within sorted instruct_list after function termination. These variables help construct final JSON output.",
        "code": "```python\n\"\"\"\nGenerates JSON format question-answer pairs and instructions for a Python file\nRequirements:\n[req01] The `DatasetGenerator` class shall:\n        a. Accept Python file path, file details, base name, list of questions,\n        and model configuration as input during instantiation.\n        b. Initialize and store Python file path, file details, base name,\n        question list, llm, and use_llm flag as class attributes.\n        d. Provide `get_response_from_llm` method to retrieve llm response.\n        e. Provide `process_question` method to process each question, generate\n        corresponding responses, and add to the instruct_list.\n        f. Provide `process_question_type` method to process questions\n        related to the file, functions, classes, and methods.\n        g. Provide `generate` method to generate responses for all questions\n        and return the instruct_list.\n        h. Internally manage question mapping to file details.\n[req02] The `get_python_datasets` function shall:\n        a. Accept a Python file path, file details, base name, questions list,\n        and model config as input.\n        b. Instantiate `DatasetGenerator` class using the provided input.\n        c. Generate instruct_list using `DatasetGenerator` `generate` method.\n        d. Return the generated `instruct_list`.\n[req03] The `clean_and_get_unique_elements` function shall:\n        a. Clean an input string (str) and return a string of unique elements.\n\"\"\"\nimport logging\nimport re\nimport math\nfrom typing import Dict, List, Tuple\n\n\ndef clean_and_get_unique_elements(input_str: str) -> str:\n    \"\"\"\n    Clean an input string (str) and return a string of unique elements.\n    Args:\n        input_str (str): The input string to be cleaned.\n    Returns:\n        str: The cleaned string.\n    \"\"\"\n\n    def element_generator(input_str):\n        start, brace_level = 0, 0\n        for i, char in enumerate(input_str):\n            if char in \"{}\":\n                brace_level += 1 if char == \"{\" else -1\n            elif char == \",\" and brace_level == 0:\n                yield input_str[start:i]\n                start = i + 1\n        yield input_str[start:]\n\n    input_str = input_str.strip(\"[]'\\\"\").strip()\n    cleaned_elements = [\n        element.strip(\"'\\\" \").strip()\n        for element in element_generator(input_str)\n        if element.strip()\n    ]\n    # add a ' ' around each element\n    cleaned_elements = [f\"'{element}'\" for element in cleaned_elements]\n    returned_elements = \", \".join(cleaned_elements)\n    return returned_elements\n\n\nclass DatasetGenerator:\n    \"\"\"\n    Generate JSON formatted dictionary outputs for a Python file.\n    Attributes:\n        file_path (str): The path to the Python file.\n        file_details (Dict[str, Any]): Details of the Python file.\n        base_name (str): The base name of the Python file.\n        questions (List[Dict[str, str]]): Questions for generating responses.\n        instruct_list (List[Dict[str, str]]): Storage for generated instructions.\n        question_mapping (Dict[str, str]): Mapping of question types to keys in file details.\n        use_llm (bool): Flag indicating if a language model should be used.\n        llm (object): The language model for generating responses.\n        prompt (str): The prompt format for querying the language model.\n    Methods:\n        get_response_from_llm(query: str, context: str) -> str:\n            Get language model response to query for given context.\n        process_question(question_type: str, question_id: str, query: str, context: str, info: Dict) -> None:\n            Process question and add the generated response to the instruct_list.\n        get_info_string(info, item_type) -> str:\n            Get string from info dictionary.\n        process_question_type(question_type: str, question_id: str, question_text: str) -> None:\n            Process questions related to a file, function, class, or method.\n        generate() -> Tuple[List[Dict], List[Dict]]:\n            Generate responses for all the questions and returns the instruct_list.\n    \"\"\"\n\n    def __init__(\n        self,\n        file_path: str,\n        file_details: Dict,\n        base_name: str,\n        questions: List[Dict],\n        model_config: Dict,\n        detailed: bool,\n    ) -> None:\n        \"\"\"\n        Initialize the DatasetGenerator class.\n        Args:\n            file_path (str): The path to the Python file.\n            file_details (Dict[str, Any]): Details of the Python file.\n            base_name (str): The base name of the Python file.\n            questions (List[Dict[str, str]]): Questions for generating responses.\n            model_config (Dict): Configuration for the language model.\n        Returns:\n            None\n        \"\"\"\n        self.file_path = file_path\n        self.file_details = file_details\n        self.base_name = base_name\n        self.questions = questions\n        self.model_config = model_config\n        if model_config is not None:\n            self.llm = model_config[\"model\"]\n            self.use_llm = True\n            self.detailed = detailed\n        else:\n            self.llm = None\n            self.use_llm = False\n            self.detailed = False\n        self.instruct_list = []\n        self.question_mapping = {\n            \"file\": \"file\",\n            \"function\": \"functions\",\n            \"class\": \"classes\",\n            \"method\": \"classes\",\n        }\n        self.code_qa_list = []\n        self.code_qa_response = \"\"\n\n    def get_response_from_llm(self, query: str, context: str) -> str:\n        \"\"\"\n        Get language model response to query for given context.\n        Args:\n            query (str): The query to be used for generating the response.\n            context (str): The context to be used for generating the response.\n        Returns:\n            str: The generated response.\n        \"\"\"\n        context_strategies = [\n            lambda: \"{}\".format(str(context)),\n            lambda: \"```python\\n{}\\n```\".format(\n                str(self.file_details[\"file_info\"][\"file_code_simplified\"])\n            ),\n            lambda: \"```python\\n{}\\n```\".format(\n                self.get_info_string(self.file_details[\"file_info\"], \"file_summary\")\n            ),\n            lambda: \"\",\n        ]\n\n        max_context_length = self.model_config[\"inference_model\"][\"model_params\"][\n            \"context_length\"\n        ]\n        prompt_template = self.model_config[\"prompt_template\"].format(\n            system_prompt=self.model_config[\"system_prompt\"],\n            instruction_prompt=self.model_config[\"instruction_prompt\"],\n        )\n\n        for strategy in context_strategies:\n            context = strategy()\n            prompt = prompt_template.format(\n                context=context, query=query, code_objects=self.code_qa_response\n            )\n            context_size = len(self.llm.tokenize(prompt))\n            logging.info(\"***Context Size: \" + str(context_size))\n            if context_size <= 0.70 * max_context_length:\n                break\n        else:\n            err_msg = f\"Model response failed, increase py2dataset_model_config.yaml context_length > {math.ceil(context_size/0.70)}\"\n            logging.error(err_msg)\n            return \"\"\n        if context == \"\":\n            context = self.code_qa_list\n\n        response = \"\"\n        try:\n            response = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", self.llm(prompt))\n            response = response.replace(\"<|im_end|>\", \"\")\n            response = \"\\n\".join([line.lstrip() for line in response.split(\"\\n\")])\n            logging.info(f\"***Overall Response: {response}\")\n        except Exception as error:\n            logging.error(f\"Failed to generate model response: {error}\")\n\n        if self.detailed:\n            for item in self.code_qa_list:\n                try:\n                    instruct_key = list(item.keys())[0]\n                    instruct_value = list(item.values())[0]\n                    query = f\"Describe the Purpose and Significance of these {instruct_key}: [{instruct_value}] and Explain what each of these {instruct_key} does in the code.\"\n                    prompt = (\n                        self.model_config[\"prompt_template\"]\n                        .format(\n                            system_prompt=self.model_config[\"system_prompt\"],\n                            instruction_prompt=self.model_config[\"instruction_prompt\"],\n                        )\n                        .format(\n                            context=f\"{context}/nCode Summary:/n{response}\",\n                            query=f\"{query}\", code_objects=f\"{instruct_value}\"\n                        )\n                    )\n                    item_response = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", self.llm(prompt))\n                    item_response = item_response.replace(\"<|im_end|>\", \"\")\n                    logging.info(f\"\\n***Itemized Response: {query}\\n{item_response}\")\n                    for item in self.instruct_list:\n                        if item[\"instruction\"].startswith(instruct_key):\n                            output = f\"\\n\\nPurpose and Significance:\\n{item_response}\"\n                            item[\"output\"] += output\n                            break\n\n                except Exception as error:\n                    logging.error(f\"Failed to generate model response: {error}\")\n\n        return response\n\n    def get_code_qa(self):\n        \"\"\"\n        Get code responses from the instruct_list and update:\n            code_qa_list (List[Dict]): List of code question-answer pairs.\n            code_qa_response (str): structured text for code question-answer pairs.\n        \"\"\"\n        excluded_instructions = {\"Call code graph\", \"Docstring\"}\n        self.code_qa_list = []\n        code_objects_responses = {}\n\n        for item in self.instruct_list:\n            instruction = item[\"instruction\"].split(\" in Python file:\")[0]\n            output = item[\"output\"]\n            if any(instruction.startswith(prefix) for prefix in excluded_instructions):\n                continue\n            self.code_qa_list.append({instruction: output})\n            if \"`\" in instruction:\n                code_object = instruction.split(\"`\")[1]\n                code_type = instruction.split()[0]\n                code_objects_responses.setdefault(code_object, []).append(\n                    (code_type, output)\n                )\n\n        self.code_qa_response = \"\"\n        for idx, (code_object, type_responses) in enumerate(\n            code_objects_responses.items(), start=1\n        ):\n            self.code_qa_response += f\"{idx}) {code_object}:\\n\"\n            for subidx, (code_type, response) in enumerate(type_responses, start=1):\n                self.code_qa_response += f\"{idx}.{subidx}. {code_type}: {response}\\n\"\n\n    def process_question(\n        self, question_type: str, question_id: str, query: str, context: str, info: Dict\n    ) -> None:\n        \"\"\"\n        Process question and add the generated response to the instruct_list.\n        Args:\n            question_type (str): The type of question to be processed.\n            question_id (str): The ID of the question to be processed.\n            query (str): The query to be processed.\n            context (str): The context to be used for generating the response.\n            info (Dict): The information of the Python file.\n        Returns:\n            None\n        \"\"\"\n        response = \"\"\n        if question_id.endswith((\"code_graph\", \"docstring\")):\n            response = info.get(question_id, {})\n        elif question_id.endswith(\"file_purpose\"):  # file_purpose is last question\n            self.get_code_qa()\n            if self.use_llm:\n                response = self.get_response_from_llm(query, context)\n        else:\n            response = clean_and_get_unique_elements(str(info.get(question_id, \"\")))\n        response = str(response).strip()\n\n        if response and response != \"None\":\n            context = f\"```python\\n{context}\\n```\"\n            self.instruct_list.append(\n                {\"instruction\": query, \"input\": context, \"output\": response}\n            )\n\n    @staticmethod\n    def get_info_string(info: Dict, item_type: str) -> str:\n        \"\"\"\n        Get string from info dictionary.\n        Args:\n            info (Dict): The information of the Python file.\n            item_type (str): The type of item to get the string from.\n        Returns:\n            str: The string from the info.\n        \"\"\"\n        return \", \".join(\n            [item.strip() for item in str(info.get(item_type, \"\")).split(\",\") if item]\n        )\n\n    def process_question_type(\n        self, question_type: str, question_id: str, question_text: str\n    ) -> None:\n        \"\"\"\n        Process questions related to a file, function, class, or method.\n        Args:\n            question_type (str): The type of question to be processed.\n            question_id (str): The ID of the question to be processed.\n            question_text (str): The text of the question to be processed.\n        Returns:\n            None\n        \"\"\"\n        if question_type == \"file\":\n            query = question_text.format(filename=self.base_name)\n            info = self.file_details[\"file_info\"]\n            context = self.file_details[\"file_info\"][\"file_code\"]\n            self.process_question(question_type, question_id, query, context, info)\n        elif question_type == \"method\":\n            for class_name, class_info in self.file_details[\"classes\"].items():\n                for key, method_info in class_info.items():\n                    if key.startswith(\"class_method_\"):\n                        method_name = f\"{class_name}.{key[len('class_method_'):]}\"\n                        context = method_info[\"method_code\"]\n                        mapping = {\"class_name\": class_name, \"method_name\": method_name}\n                        query = question_text.format(filename=self.base_name, **mapping)\n                        self.process_question(\n                            question_type, question_id, query, context, method_info\n                        )\n        else:  # question_type is 'function' or 'class'\n            for name, info in self.file_details[\n                self.question_mapping[question_type]\n            ].items():\n                context = info[f\"{question_type}_code\"]\n                mapping = {f\"{question_type}_name\": name}\n                if question_id == f\"{question_type}_purpose\" and self.use_llm:\n                    variables = self.get_info_string(info, f\"{question_type}_variables\")\n                    inputs = self.get_info_string(info, f\"{question_type}_inputs\")\n                    combined = \", \".join([s for s in [variables, inputs] if s])\n                    mapping[\n                        f\"{question_type}_variables\"\n                    ] = clean_and_get_unique_elements(combined)\n\n                    if question_type == \"class\":\n                        methods_string = self.get_info_string(\n                            info, f\"{question_type}_methods\"\n                        )\n                        mapping[f\"{question_type}_methods\"] = methods_string\n\n                query = question_text.format(filename=self.base_name, **mapping)\n                self.process_question(question_type, question_id, query, context, info)\n\n    def generate(self) -> Tuple[List[Dict], List[Dict]]:\n        \"\"\"\n        Generate responses for all the questions and returns the instruct_list.\n        Args:\n            None\n        Returns:\n            Tuple[List[Dict], List[Dict]]:  .\n        \"\"\"\n        for question in self.questions:\n            self.process_question_type(\n                question[\"type\"], question[\"id\"], question[\"text\"]\n            )\n        self.instruct_list.sort(key=lambda x: len(x[\"input\"]), reverse=True)\n        return self.instruct_list\n\n\ndef get_python_datasets(\n    file_path: str,\n    file_details: Dict,\n    base_name: str,\n    questions: List[Dict],\n    model_config: Dict,\n    detailed: bool,\n) -> Tuple[List[Dict], List[Dict]]:\n    \"\"\"\n    Extract information from a Python file and return it in JSON format.\n    Args:\n        file_path (str): The path to the Python file.\n        file_details (Dict): The details of the file.\n        base_name (str): The base Python code filename.\n        questions (List[Dict]): The list of questions.\n        llm (object): The language model to be used for generating responses.\n        prompt (str): The prompt to be used for generating responses.\n        detailed (bool): Flag indicating if detailed information should be extracted.\n    Returns:\n        Tuple[List[Dict], List[Dict]]: Extracted information in JSON format.\n    \"\"\"\n    return DatasetGenerator(\n        file_path, file_details, base_name, questions, model_config, detailed\n    ).generate()\n\n```",
        "code filename": "py2dataset/get_python_datasets.py"
    },
    {
        "document": "Overall Purpose and Processing Approach of Python File 'get_python_file_details.py': This Python script aims to extract comprehensive details about a given Python file by analyzing its Abstract Syntax Tree (AST), removing docstrings, identifying functions, classes, dependencies, constants, calls between functions/methods, and constructing a code graph representation using the NetworkX library. It utilizes various helper methods such as remove_docstring(), get_all_calls(), CodeVisitor class for traversing AST nodes, extracting details about functions and classes, and generating a code graph via get_code_graph().\n\n1) Function: remove_docstring\n- Purpose: Remove docstrings from provided Python code including top-level module docstrings and docstrings in functions/classes/async functions.\n- Inputs: 'code' (source code string), 'tree' (AST representation of the source code).\n- Calls Made: tree manipulation methods like 'tree.body.pop', 'ast.walk', 'node.body.pop', 'ast.unparse'.\n- Variables Used: 'tree', 'first_body_node', 'code'.\n- Returns: Sanitized code string after removing docstrings ('ast.unparse(tree)').\n\n2) Function: get_all_calls\n- Purpose: Recursively find all function calls in the subtree rooted at a given node including those in class attributes, list comprehensions, and lambda functions.\n- Inputs: 'node' (root node), optional argument 'calls'. If none provided initializes calls as an empty list of tuples [function name, arguments].\n- Calls Made: 'isinstance', 'calls.append', 'ast.unparse', 'get_all_calls', 'ast.iter_child_nodes', 'calls_dict[func].extend'.\n- Variables Involved: 'calls', 'calls_dict', 'calls', 'node', 'func'.\n- Returns: Dictionary mapping function calls as strings to their argument lists ('calls_dict').\n\n3) Function Call: get_python_file_details\n- Purpose: Extract file details from a Python file path and generate structured documentation.\n- Inputs: 'file_path' (str, file location).\n- Calls Made: Opening file ('open'), parsing ('ast.parse'), warning logging via 'logging.warning', Instantiating 'CodeVisitor', traversing with visitor ('visitor.analyze'), 'json.dumps', string modification to remove quotations.\n- Variables Handled: 'file_path', 'tree', 'file_summary', 'file_details', 'code', 'visitor', 'file_ast'.\n- Returns: Dictionary containing extracted file details ('file_details') or None on error.\n\n4) Class: CodeVisitor\n- Type/Role: Class extending ast.NodeVisitor that traverses an AST extracting code information and storing in instance variables such as 'code', 'functions', 'classes', 'file_info', and maintaining 'current_class'.\n- Methods Implemented: __init__, visit_FunctionDef, visit_ClassDef, generic_visit, visit_Assign, extract_details, analyze.\n- Inheritance: ast.NodeVisitor.\n- Variables Stored: 'tree', 'current_class', 'file_info'.\n\n5) Method: CodeVisitor.__init__\n- Purpose: Initialize a new instance of the class with source code and AST tree as inputs.\n- Inputs: 'self' (instance), 'code' (source code string), 'tree' (AST representation).\n- Variables Assigned: 'self', 'tree', 'code'.\n\n6) Method: CodeVisitor.visit_FunctionDef\n- Purpose: Extract details about functions found during AST traversal, adds to the function dictionary in instances.\n- Inputs: 'self', 'node' (function node).\n- Called Functions/Methods: Invoking 'self.extract_details()', propagates processing down through generic visit method.\n\n7) Method: CodeVisitor.visit_ClassDef\n- Purpose: Extract class details and stores them in instances during AST traversal.\n- Inputs: 'self', 'node' (class node).\n- Called Functions/Methods: Invoking 'self.extract_details()', propagates processing down through generic visit method.\n\n8) Method: CodeVisitor.generic_visit\n- Purpose: Perform child nodes traversal while executing a visiting method of a subclass node when not defined specifically in CodeVisitor class.\n- Inputs: 'self', 'node' (the actual visiting Node).\n- Called Function: AstTree iterator, visits inheritable attributes/values and maintains stack based process in a 'parent to child nodes.' Relay propagates call of 'self.visit()'.\n\n9) Method: CodeVisitor.visit_Assign\n- Purpose: Capture constant variable assignments within module scope.\n- Inputs: 'self', 'node' (Assignment node).\n- Called Functions/Methods: Checking instance types ('isinstance'), updating 'self.constants'.\n- Variables Created: 'value_repr', 'constant_assignment', 'node', 'self', 'value'.\n\n10) Method: CodeVisitor.extract_details\n- Purpose: Extract details about an ast node depending on type and maintain data as dict {'key': Union[str, List[str]}.\n- Inputs: 'self', 'node', 'node_type' (type of the AST Node).\n- Called Functions/Methods: Walking tree ('ast.walk'), constructing dictionaries of various attributes ('get_all_calls', 'next', 'set', 'call_data.keys').\n- Variables Manipulated: 'node_inputs', 'details', 'node', 'self', 'node_variables', 'node_walk', 'node_type', 'call_data'.\n- Returns: Dict of node details extracted from the ast Node ('details').\n\n11) Method: CodeVisitor.analyze\n- Purpose: Traverse AST, list all nodes and populate file information dictionary.\n- Inputs: 'self', 'node' (root node).\n- Called Functions/Methods: Invoking self visit methods ('self.visit'), constructing dependencies lists, function defs dict, class defs dict, removing docstrings ('remove_docstring').\n- Variables Manipulated: 'file_dependencies', 'function_defs', 'class_defs', 'node', 'node_walk'.",
        "code": "```python\n\"\"\"\nReturn file_details dictionary for python source code.\nRequirements:\n[req00] The `remove_docstring` function shall:\n        a. Accept a Python code string as an argument.\n        b. Remove docstrings and comments from the provided code.\n        c. Return the sanitized code string.\n[req01] The get_all_calls function shall:\n        a. Accept a node of type ast.AST as an argument.\n        b. Recursively find all function calls in the subtree rooted at the node.\n        c. Return a dictionary of all function calls in the subtree rooted at the node.\n[req02] The `CodeVisitor` class shall:\n        a. Accept the source code as input when instantiated.\n        b. Use the AST to extract details about the code.\n        c. Inherit from `ast.NodeVisitor`.\n        d. Implement `visit_FunctionDef` method to gather details about functions.\n        e. Implement `visit_ClassDef` method to gather details about classes.\n        f. Implement `extract_details` method to parse information about a given node.\n        g. Implement `analyze` method to traverse the AST and 'file_info'. \n        h. Maintain a current class context using the attribute 'current_class'.\n[req03] The `code_graph` function shall:\n        a. Accept the file summary as input.\n        b. Construct a directed graph with nodes and edges using networkx library.\n        c. Define elements such as function nodes, class nodes, and method nodes.\n        d. Specify edges to represent relationships. \n        e. Return a dictionary representation of the code call graph. \n[req04] The `get_python_file_details` function shall:\n        a. Accept a file path as an argument.\n        b. Extract info from Python file using the AST and the `CodeVisitor` class.\n        c. Include the entire code graph in the returned details.\n        d. Return a dictionary encompassing the extracted file details. \n\"\"\"\nimport ast\nimport logging\nimport json\nfrom typing import Dict, List, Union\nfrom get_code_graph import get_code_graph\n\n\ndef remove_docstring(code: str, tree: ast.AST) -> str:\n    \"\"\"\n    Remove docstrings from the provided Python code.\n    This includes top-level module docstrings and docstrings in\n    functions, classes, and async functions.\n    Args:\n        code (str): The source code from which to remove docstrings.\n    Returns:\n        str: The source code with docstrings removed.\n    \"\"\"\n    if (\n        tree.body\n        and isinstance(tree.body[0], ast.Expr)\n        and isinstance(tree.body[0].value, ast.Str)\n    ):\n        tree.body.pop(0)\n\n    for node in ast.walk(tree):\n        if (\n            isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef))\n            and node.body\n        ):\n            first_body_node = node.body[0]\n            if isinstance(first_body_node, ast.Expr) and isinstance(\n                first_body_node.value, ast.Str\n            ):\n                node.body.pop(0)\n\n    return ast.unparse(tree)\n\n\ndef get_all_calls(node: ast.AST, calls=None) -> Dict[str, List[str]]:\n    \"\"\"\n    Recursively find all function calls in the subtree rooted at `node`,\n    including those in class attributes, list comprehensions, and lambda functions.\n    Args:\n        node (ast.AST): The root node to start the search from.\n        calls (List[Tuple[str, List[str]]], optional): Accumulator for function calls found.\n    Returns:\n        Dict[str, List[str]]: dictionary mapping function calls (as strings) to lists of their arguments.\n    \"\"\"\n    if calls is None:\n        calls = []\n    if isinstance(node, ast.Call):\n        calls.append((ast.unparse(node.func), [ast.unparse(arg) for arg in node.args]))\n    elif isinstance(node, ast.ClassDef):\n        for body_item in node.body:\n            if isinstance(body_item, ast.Assign) and isinstance(\n                body_item.targets[0], ast.Name\n            ):\n                get_all_calls(body_item.value, calls)\n\n    for child in ast.iter_child_nodes(node):\n        get_all_calls(child, calls)\n\n    # Build the dictionary after traversal\n    calls_dict = {}\n    for func, args in calls:\n        if func not in calls_dict:\n            calls_dict[func] = []\n        calls_dict[func].extend(args)\n\n    return calls_dict\n\n\nclass CodeVisitor(ast.NodeVisitor):\n    \"\"\"\n    Visitor class for traversing an AST (Abstract Syntax Tree) and extracting details about the code.\n    Attributes:\n        code (str): The source code.\n        functions(Dict): details about functions in the code.\n        classes (Dict): details about classes in the code.\n        file_info (Dict): details about the file.\n    Methods:\n        visit_FunctionDef(node: ast.FunctionDef) -> None:\n            Extract details about a function.\n        visit_ClassDef(node: ast.ClassDef) -> None:\n            Extract details about a class.\n        extract_details(node: ast.AST, node_type: str) -> Dict[str, Union[str, List[str]]]:\n            Extract details about a node.\n        analyze(node: ast.AST) -> None:\n            Populate file_info with details about the file.\n    \"\"\"\n\n    def __init__(self, code: str, tree: ast.AST) -> None:\n        \"\"\"\n        Initialize a new instance of the class.\n        Args:\n            code: str: The source code.\n        \"\"\"\n        self.code: str = code\n        self.functions: Dict[str, Dict[str, Union[str, List[str]]]] = {}\n        self.classes: Dict[str, Dict[str, Union[str, List[str]]]] = {}\n        self.file_info: Dict[str, Union[str, List[str]]] = {}\n        self.current_class: str = None\n        self.constants: List[str] = []\n        self.tree = tree\n\n    def visit_FunctionDef(self, node: ast.FunctionDef) -> None:\n        \"\"\"\n        Extract details about a function.\n        Args:\n            node: ast.FunctionDef: The node to visit.\n        \"\"\"\n        details = self.extract_details(\n            node, \"method\" if self.current_class else \"function\"\n        )\n        if self.current_class:\n            self.classes[self.current_class][f\"class_method_{node.name}\"] = details\n        else:\n            self.functions[node.name] = details\n        self.generic_visit(node)\n\n    def visit_ClassDef(self, node: ast.ClassDef) -> None:\n        \"\"\"\n        Extract details about a class.\n        Args:\n            node: ast.ClassDef: The node to visit.\n        \"\"\"\n        self.classes[node.name] = self.extract_details(\n            node, \"class\"\n        )  # populate class dictionary when class definition found in AST\n        self.current_class = node.name  # set current_class to indicate inside a class\n        self.generic_visit(node)  # continue AST traversal to the next node\n        self.current_class = None  # reset current_class when finished with this class\n\n    def generic_visit(self, node):\n        \"\"\"\n        Called if no explicit visitor function exists for a node.\n        Args:\n            node: ast.AST: The node to visit.\n        \"\"\"\n        for child in ast.iter_child_nodes(node):\n            child.parent = node\n            self.visit(child)\n\n    def visit_Assign(self, node: ast.Assign):\n        \"\"\"\n        Get self.constants\n        Args:\n            node: ast.Assign: The node to visit.\n        \"\"\"\n        if isinstance(node.parent, ast.Module):\n            for target in node.targets:\n                if isinstance(target, ast.Name):\n                    value = node.value\n                    if isinstance(value, ast.Constant) and isinstance(value.value, str):\n                        value_repr = f\"'{value.value}'\"\n                    else:\n                        value_repr = ast.unparse(value)\n                    constant_assignment = f\"{target.id}={value_repr}\"\n                    self.constants.append(constant_assignment)\n        self.generic_visit(node)\n\n    def extract_details(\n        self, node: ast.AST, node_type: str\n    ) -> Dict[str, Union[str, List[str]]]:\n        \"\"\"\n        Extract details about a node.\n        Args:\n            node: ast.AST: The node to extract details from.\n            node_type: str: The type of node.\n        Returns:\n            Dict[str, Union[str, List[str]]]: The details extracted from the node.\n        \"\"\"\n        node_walk = list(ast.walk(node))\n        call_data = get_all_calls(node)\n        node_inputs = (\n            [arg.arg for arg in node.args.args]\n            if node_type in [\"function\", \"method\"]\n            else None\n        )\n        node_variables = list(\n            {\n                ast.unparse(target)\n                for subnode in node_walk\n                if isinstance(subnode, ast.Assign)\n                for target in subnode.targets\n                if isinstance(target, ast.Name)\n            }\n        )\n        if node_inputs:\n            node_variables = list(set(node_inputs + node_variables))\n\n        details = {\n            f\"{node_type}_name\": node.name,\n            f\"{node_type}_code\": ast.unparse(node),\n            f\"{node_type}_docstring\": next(\n                (\n                    n.value.s\n                    for n in node_walk\n                    if isinstance(n, ast.Expr) and isinstance(n.value, ast.Str)\n                ),\n                None,\n            ),\n            f\"{node_type}_inputs\": node_inputs,\n            f\"{node_type}_defaults\": [ast.unparse(d) for d in node.args.defaults]\n            if node_type in [\"function\", \"method\"]\n            else None,\n            f\"{node_type}_returns\": [\n                ast.unparse(subnode.value) if subnode.value is not None else \"None\"\n                for subnode in node_walk\n                if isinstance(subnode, ast.Return)\n            ],\n            f\"{node_type}_calls\": list(call_data.keys()),\n            f\"{node_type}_call_inputs\": call_data,\n            f\"{node_type}_variables\": node_variables,\n            f\"{node_type}_decorators\": list(\n                {ast.unparse(decorator) for decorator in node.decorator_list}\n                if node.decorator_list\n                else set()\n            ),\n            f\"{node_type}_annotations\": list(\n                {\n                    ast.unparse(subnode.annotation)\n                    for subnode in node_walk\n                    if isinstance(subnode, ast.AnnAssign)\n                    and subnode.annotation is not None\n                }\n            ),\n            f\"{node_type}_properties\": list(\n                {\n                    ast.unparse(subnode)\n                    for subnode in node_walk\n                    if isinstance(subnode, ast.Attribute)\n                    and isinstance(subnode.ctx, ast.Store)\n                }\n            ),\n        }\n        if node_type in [\"class\", \"method\"]:\n            if (\n                node_type == \"method\" and self.current_class\n            ):  # find attributes defined as self.attribute\n                attributes = [\n                    target.attr\n                    for subnode in node_walk\n                    if isinstance(subnode, ast.Assign)\n                    for target in subnode.targets\n                    if isinstance(target, ast.Attribute)\n                    and isinstance(target.value, ast.Name)\n                    and target.value.id == \"self\"\n                ]\n                class_attributes = self.classes[self.current_class].setdefault(\n                    \"class_attributes\", []\n                )\n                class_attributes += attributes\n            if node_type == \"class\":\n                details.update(\n                    {\n                        \"class_attributes\": [\n                            target.attr\n                            for subnode in node.body\n                            if isinstance(subnode, ast.Assign)\n                            for target in subnode.targets\n                            if isinstance(target, ast.Attribute)\n                        ],\n                        \"class_methods\": [\n                            subnode.name\n                            for subnode in node.body\n                            if isinstance(subnode, ast.FunctionDef)\n                        ],\n                        \"class_inheritance\": [ast.unparse(base) for base in node.bases]\n                        if node.bases\n                        else [],\n                        \"class_static_methods\": [\n                            subnode.name\n                            for subnode in node.body\n                            if isinstance(subnode, ast.FunctionDef)\n                            and any(\n                                isinstance(decorator, ast.Name)\n                                and decorator.id == \"staticmethod\"\n                                for decorator in subnode.decorator_list\n                            )\n                        ],\n                    }\n                )\n        return details\n\n    def analyze(self, node: ast.AST) -> None:\n        \"\"\"\n        Traverse the AST, list all nodes, and populate 'file_info'\n        Args:\n            node: ast.AST: The node to analyze.\n        \"\"\"\n        # Traverse the AST to capture various code details\n        self.visit(node)\n        node_walk = list(ast.walk(node))\n\n        file_dependencies = {\n            alias.name\n            for subnode in node_walk\n            if isinstance(subnode, ast.Import)\n            for alias in subnode.names\n        } | {\n            subnode.module\n            for subnode in node_walk\n            if isinstance(subnode, ast.ImportFrom)\n        }\n\n        function_defs = [\n            {\n                func_name: {\n                    \"inputs\": details[\"function_inputs\"],\n                    \"calls\": details[\"function_calls\"],\n                    \"call_inputs\": details[\"function_call_inputs\"],\n                    \"returns\": details[\"function_returns\"],\n                }\n            }\n            for func_name, details in self.functions.items()\n        ]\n\n        class_defs = [\n            {\n                class_name: {\n                    \"method_defs\": {\n                        method_name[len(\"class_method_\") :]: {\n                            \"inputs\": details[\"method_inputs\"],\n                            \"calls\": details[\"method_calls\"],\n                            \"call_inputs\": details[\"method_call_inputs\"],\n                            \"returns\": details[\"method_returns\"],\n                        }\n                        for method_name, details in class_details.items()\n                        if method_name.startswith(\"class_method_\")\n                    }\n                }\n            }\n            for class_name, class_details in self.classes.items()\n        ]\n\n        self.file_info = {\n            \"file_code\": self.code,\n            \"file_ast\": node,\n            \"file_dependencies\": list(file_dependencies),\n            \"file_functions\": list(self.functions.keys()),\n            \"file_classes\": list(self.classes.keys()),\n            \"file_constants\": self.constants,\n            \"file_summary\": {\n                \"dependencies\": list(file_dependencies),\n                \"function_defs\": function_defs,\n                \"class_defs\": class_defs,\n            },\n            \"file_code_simplified\": remove_docstring(ast.unparse(node), self.tree),\n        }\n\n\ndef get_python_file_details(file_path: str) -> Dict[str, Union[Dict, str]]:\n    \"\"\"\n    Extract details from a Python file.\n    Args:\n        file_path: str: The path to the Python file.\n    Returns:\n        Dict[str, Union[Dict, str]]: The details extracted from the file.\n    \"\"\"\n    try:\n        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n            code = f.read()\n            tree = ast.parse(code)\n    except (PermissionError, SyntaxError, IOError) as e:\n        logging.warning(f\"{e} error in file: {file_path}\")\n        return None\n\n    visitor = CodeVisitor(code, tree)\n    visitor.analyze(tree)\n    file_details = {\n        \"file_info\": visitor.file_info,\n        \"functions\": visitor.functions,\n        \"classes\": visitor.classes,\n    }\n\n    # get code graph items and add to file_details\n    file_summary = file_details[\"file_info\"][\"file_summary\"]\n    file_ast = file_details[\"file_info\"][\"file_ast\"]\n    entire_code_graph, control_flow_structure, plant_uml = get_code_graph(\n        file_summary, file_ast\n    )\n    file_details[\"file_info\"][\"entire_code_graph\"] = entire_code_graph\n    file_details[\"file_info\"][\"control_flow_structure\"] = control_flow_structure\n    file_details[\"file_info\"][\"plant_uml\"] = plant_uml\n    file_details[\"file_info\"][\"file_summary\"] = json.dumps(file_summary).replace(\n        '\"', \"\"\n    )\n    del file_details[\"file_info\"][\"file_ast\"]\n\n    return file_details\n\n```",
        "code filename": "py2dataset/get_python_file_details.py"
    },
    {
        "document": "1. Overview & Processing Approach of py2dataset.py Module:\nThe py2dataset.py module is designed for generating datasets from Python files within a given directory by analyzing their code structure and responding to user-defined questions about them. It incorporates various functionalities such as processing single Python files, handling GitHub repository cloning, managing logging levels, and generating output in JSON or HTML format. The main functionality involves extracting file details, retrieving question-answer pairs ('instruct.json' datasets), saving data into specified directories, and combining multiple 'instruct.json' files to create comprehensive datasets.\n\n2. API Signatures for Key Functions and Classes:\n\na. `process_single_python_file(...)`: A utility function working with one Python file; input includes relative file pathnames along with multiple settings options concerning outputs directory paths, model configurations, language processing using Large Language Model (LLM), detailed analysis flag, etc., and returns None after executing its tasks.\n- Parameters: python_pathname (str), relative_path (Path), output_dir (str), model_config_pathname (str), questions (Dict), use_llm (bool), model_config (Dict), detailed (bool)\n- Calls: logging functions, get_model(), get_python_file_details(), get_python_datasets(), save_python_data()\n- Variables used: python_pathname, file_details, relative_path, instruct_data, model_config_pathname, questions, output_dir, use_llm, model_config, detailed\n- Return value: None\n\nb. `py2dataset(...)`: A core function processing Python files in a given directory creating datasets from them; supports user customizations such as logging levels, parallel execution options, generating HTML outputs, skipping existing instruct.json regeneration, etc., and returns the consolidated dataset JSONs.\n- Parameters: start (str), output_dir (str), questions_pathname (str), model_config_pathname (str), use_llm (bool), quiet (bool), single_process (bool), detailed (bool), html (bool), skip_regen (bool)\n- Calls: logging adjustments, get_output_dir(), get_questions(), Path(), str(), os.path methods, rglob(), Process creation with process_single_python_file(), combine_json_files()\n- Variables created: params (dict), python_pathname, relative_path, start, quiet, output_dir, questions_pathname, model_config_pathname, single_process, html, skip_regen\n- Return value: combine_json_files(output_dir), HTML generation flag, parameters['questions'] dictionary\n\nc. `clone_github_repo(...)`: A utility function cloning GitHub repositories or fetching latest changes; returns the local repository path after operation completion.\n- Parameter: url (str)\n- Calls: git commands via subprocess, os methods like join(), makedirs(), repo creation with Git library functions\n- Variables used: path, command, githubrepos_dir, repo_name, default_branch, repo instance\n- Return value: path string\n\nd. `get_bool_from_input(...)`: A helper function converting user input strings into boolean values for various arguments; checks lowercase words for true/false interpretation and returns appropriate bool type accordingly.\n- Parameters: input_str (str), current_value (initial bool)\n- Calls: None\n- Variables used: input_str, user input segment\n- Return value: modified bool state\n\n3) `main(...)` function operates as a command line interface by managing program parameters including startup path choices through either CWD or cloning a repository with git and ultimately running the py2dataset functionality. It also allows interactive mode for modifying parameters interactively.\n- Calls: print(), sys methods, input(), os methods like join(), pop(), clone_github_repo(), os.path checks\n- Variables used: params (dict), arg_string, user_input segment\n- Return value: None but executes py2dataset() with processed arguments\n\n3. Functional Requirements Description:\na. `process_single_python_file`: Handles one Python file analysis, initiates LLM model usage if enabled, fetches Python file details and question datasets accordingly; saves results to the designated directory. If LLM configuration is missing, it loads it from the given path. It operates either as a single process or spawns a separate process based on requirements.\nb. `py2dataset`: Starts with default values if not overridden via command-line arguments and determines appropriate output directory ('githubrepos' creation included); finds Python files within given directories while skipping cache regeneration for existing instruct.json files; processes each file using multiple processes or single process depending on LLM usage.\nc. `clone_github_repo`: Clones GitHub repositories by fetching remote branches if accessible else returns an error message.\nd. `get_bool_from_input`: Converts user input strings into boolean values for parameters adjustments during interactive mode; checks lowercase words like 'true', 'false', 'yes', 'no' to determine true/false states.\ne. `main`: Parses command-line arguments, sets defaults if not provided, handles GitHub repository cloning if URL given, invokes py2dataset() with modified parameters based on user inputs or interactive prompts; otherwise, it operates using default settings. Interactive mode lets users update arguments as desired.\n\n4. Explanation of variables involved in the code:\na. params (dictionary): Holds customizable command line argument values in py2dataset function for flexible execution.\nb. python_pathname: Path to the Python file processed by `process_single_python_file`.\nc. relative_path: Relative path of Python files within start directory used in `py2dataset` function.\nd. output_dir: Output directory for saving generated datasets in `py2dataset`.\ne. questions_pathname: Path to the JSON file containing user-defined questions in `py2dataset`.\nf. model_config_pathname: Path to the configuration file for LLM model in `py2dataset`.\ng. use_llm: Boolean flag indicating LLM usage in `process_single_python_file` and `py2dataset` function(s).\nh. quiet: Flag deciding log verbosity levels ('loglevel') within py2dataset to reduce excessive outputs when True.\ni. single_process: Utilizes one process while performing 'process_single_python_file' if LLM is enabled in `py2dataset`.\nj. detailed: Enables detailed analysis for Python files processing in `py2dataset`.\nk. html: Generates HTML output format datasets flag in `py2dataset`.\nl. skip_regen: Skips regeneration of existing instruct.json files if True in `py2dataset`.\nm. start: Starting directory path for Python files or GitHub repository URL in `py2dataset`.\nn. repo_name: Name extracted from given GitHub repository URL in `clone_github_repo`.\no. command: Command string for git operations in `clone_github_repo`.\np. githubrepos_dir: Local directory path to store cloned repositories in `clone_github_repo`.\nq. repo instance: Git repository object created during cloning process in `clone_github_repo`.\nr. user_input segment: User input string for modifying parameters interactively in `main`.\ns. arg_string: Combined command line arguments string in `main`.\nt. value_segment: Split user input string into separate values in `get_bool_from_input`.\nu. path: Local path where the cloned GitHub repository exists after execution of `clone_github_repo`.",
        "code": "```python\n\"\"\"\nFor each Python file within a given directory, this module is designed to generate, save, \nand return datasets that include responses to questions about the code. \nRequirements:\n[req00] The process_single_python_file function shall:\n    a. Accept parameters for Python file path, start directory, model config \n       pathname, questions dict, use of LLM, and output dir.\n    b. If 'use_llm' is True, use 'get_model' to instantiate LLM model config.\n    c. Use 'get_python_file_details' to retrieve Python file info.\n    d. Use 'get_python_datasets' to acquire instruct.json datasets.\n    e. Use 'save_python_data' to store file details and instruct.json data.\n[req01] The py2dataset function shall:\n    a. Accept parameters for start directory, output dir, questions path, model\n       config path, use of LLM, and quiet mode.\n    b. Adjust logging level based on 'quiet'.\n    c. Use current working dir if no valid start dir is provided.\n    d. Get output dir with 'get_output_dir'.\n    e. Retrieve questions dict with 'get_questions'.\n    f. Search for Python files using 'rglob', excluding those starting with \"_\".\n    g. For each Python file, spawn a child process with 'process_single_python_file'\n       to get file details and instruct.json data, if single_process is False.\n    h. Combine instruct.json files with 'combine_json_files'.\n    i. Return datasets.\n[req02] The clone_github_repo function shall:\n    a. Accept a url.\n    b. Clone repository or fetch the latest changes.\n    c. Return local repository path.\n[req03] The main function shall:\n    a. Accept and process command-line args.\n    b. Determine py2dataset parameters based on processed arguments.\n    c. Call py2dataset with derived parameters.\n\"\"\"\nimport sys\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, List\nfrom multiprocessing import Process\nimport subprocess\nimport os\nimport git\nimport shlex\n\nfrom get_python_file_details import get_python_file_details\nfrom get_python_datasets import get_python_datasets\nfrom get_params import (\n    get_questions,\n    get_model,\n    get_output_dir,\n    get_start_dir,\n)\nfrom save_output import combine_json_files, save_python_data\n\n\ndef process_single_python_file(\n    python_pathname: str,\n    relative_path: Path,\n    output_dir: str,\n    model_config_pathname: str,\n    questions: Dict,\n    use_llm: bool,\n    model_config: Dict,\n    detailed: bool,\n) -> None:\n    \"\"\"\n    Processes a single Python file to generate question-answer pairs and instructions.\n    Args:\n        python_pathname (str): Path to the Python file.\n        start (str): Starting directory for Python files.\n        output_dir (str): Directory to write the output files.\n        model_config_pathname (str): Path and filename of the model configuration file.\n        questions (Dict): Dictionary of questions to answer about the Python file.\n        use_llm (bool): Use llm to answer code purpose question.\n        model_config (Dict): Configuration dictionary for the LLM.\n        detailed (bool): Perform detailed analysis if True.\n    \"\"\"\n    logging.info(f\"Processing file: {python_pathname}\")\n    if model_config is None and use_llm:\n        model_config = get_model(model_config_pathname)\n\n    file_details = get_python_file_details(python_pathname)\n    if not file_details:\n        logging.error(f\"Failed to get file details for {python_pathname}\")\n        return\n\n    instruct_data = get_python_datasets(\n        python_pathname,\n        file_details,\n        relative_path,\n        questions,\n        model_config,\n        detailed,\n    )\n\n    if instruct_data:\n        save_python_data(file_details, instruct_data, relative_path, output_dir)\n    else:\n        logging.error(f\"Failed getting {python_pathname} dataset\")\n\n    del instruct_data, file_details, model_config\n\n\ndef py2dataset(\n    start: str = \"\",\n    output_dir: str = \"\",\n    questions_pathname: str = \"\",\n    model_config_pathname: str = \"\",\n    use_llm: bool = False,\n    quiet: bool = False,\n    single_process: bool = False,\n    detailed: bool = False,\n    html: bool = False,\n    skip_regen: bool = False,\n) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Generates datasets by processing Python files within a specified directory.\n    Args:\n        start (str, optional): Starting directory for Python files or GitHub repository Python files. Default: current working directory.\n        output_dir (str, optional): Directory to write the output files. Default: ./dataset/.\n        questions_pathname (str, optional): Path and filename of the questions file. Default: ./py2dataset_questions.json.\n        model_config_pathname (str, optional): Path and filename of the model configuration file. Default: ./py2dataset_model_config.yaml.\n        use_llm (bool, optional): Use llm to answer code purpose question. Default: False.\n        quiet (bool, optional): Limit logging output. Default: False.\n        single_process (bool, optional): Use a single process to process Python files if --use_llm. Default: False.\n        detailed (bool, optional): Include detailed analysis. Default: False.\n        html (bool, optional): Generate HTML output. Default: False.\n        skip_regen (bool, optional): Skip regeneration of existing instruct.json files. Default: False.\n    Returns:\n        Dict[str, List[Dict]]: Generated datasets.\n    \"\"\"\n    if quiet:\n        logging.getLogger().setLevel(logging.WARNING)\n    else:\n        logging.getLogger().setLevel(logging.INFO)\n\n    sys.setrecursionlimit(3000)  # Set recursion limit higher for AST parsing\n\n    model_config = None\n    if use_llm and single_process:\n        model_config = get_model(model_config_pathname)\n\n    params = {\n        \"python_pathname\": \"\",\n        \"relative_path\": \"\",\n        \"output_dir\": get_output_dir(output_dir),\n        \"model_config_pathname\": model_config_pathname,\n        \"questions\": get_questions(questions_pathname),\n        \"use_llm\": use_llm,\n        \"model_config\": model_config,\n        \"detailed\": detailed,\n    }\n\n    for python_pathname in Path(start).rglob(\"*.py\"):\n        \n        if \"__pycache__\" in python_pathname.parts:\n            continue\n        if python_pathname.name == \"__init__.py\":\n            continue\n        \n        params[\"python_pathname\"] = str(python_pathname)\n        params[\"relative_path\"] = Path(\n            os.path.relpath(python_pathname, os.path.dirname(get_start_dir(start)))\n        )\n\n        # determine if corresponding instruct.json exists and skip if skip_regen\n        base_pathname = Path(params[\"output_dir\"]) / params[\"relative_path\"]\n        instruct_pathname = base_pathname.with_suffix(\".py.instruct.json\")\n        if instruct_pathname.exists() and skip_regen:\n            continue\n        \n        # process each python file in a separate process to manage memory\n        if params[\"model_config\"] is None and params[\"use_llm\"]:\n            proc = Process(target=process_single_python_file, kwargs=params)\n            proc.start()\n            proc.join()\n        else:  # or process all files using use a single process\n            process_single_python_file(**params)\n\n    return combine_json_files(output_dir, html, params[\"questions\"])\n\n\ndef clone_github_repo(url: str) -> str:\n    \"\"\"\n    Clone repository or fetch the latest changes and return local repository path.\n    Args:\n        url (str): The url of the github repository.\n    Returns:\n        str: The path to the cloned repository.\n    \"\"\"\n    try:\n        command = f\"git ls-remote {shlex.quote(url)}\"\n        subprocess.run(\n            command,\n            shell=True,\n            check=True,\n            stdout=subprocess.DEVNULL,\n            stderr=subprocess.DEVNULL,\n        )\n        repo_name = url.split(\"/\")[-1]\n        githubrepos_dir = os.path.join(os.getcwd(), \"githubrepos\")\n        os.makedirs(githubrepos_dir, exist_ok=True)\n        path = os.path.join(githubrepos_dir, repo_name)\n        if not os.path.exists(path):\n            git.Repo.clone_from(url, path)\n        else:\n            repo = git.Repo(path)\n            with repo.git.custom_environment(\n                GIT_SSH_COMMAND=\"ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no\"\n            ):\n                repo.git.fetch()\n                default_branch = repo.head.reference.tracking_branch().remote_head\n                repo.git.reset(\"--hard\", default_branch)\n    except subprocess.CalledProcessError:\n        logging.info(f\"Invalid or inaccessible repository: {url}\")\n        path = \"\"\n\n    return path\n\n\ndef main():\n    \"\"\"\n    Command-line entry point for processing Python files and generating datasets.\n    Optional command-line arguments:\n    --start (str, optional): Starting directory for Python files or GitHub repository Python files. Default: cwd.\n    --output_dir (str, optional): Directory to write the output files. Default: ./dataset/.\n    --questions_pathname (str, optional): Path and filename of the questions file. Default: ./py2dataset_questions.json.\n    --model_config_pathname (str, optional): Path and filename of the model configuration file. Default: ./py2dataset_model_config.yaml.\n    --use_llm (bool, optional): Use llm to answer code purpose question. Default: False.\n    --quiet (bool, optional): Limit logging output. Default: False.\n    --single_process (bool, optional): Use a single process to process Python files if --use_llm. Default: False.\n    --detailed (bool, optional): Include detailed analysis. Default: False.\n    --html (bool, optional): Generate HTML output. Default: False.\n    --I (str, optional): Enable interactive mode. Default: False.\n    --skip_regen (str, optional): Skip regeneration of existing instruct.json files. Default: False.\n    --help: Display help message.\n    \"\"\"\n    if \"--help\" in sys.argv:\n        print(__doc__)\n        sys.exit()\n\n    # return boolean value based on the user input\n    def get_bool_from_input(input_str: str, current_value: bool) -> bool:\n        if input_str.lower() in [\"t\", \"true\", \"y\", \"yes\"]:\n            return True\n        elif input_str.lower() in [\"f\", \"false\", \"n\", \"no\"]:\n            return False\n        return current_value\n\n    # Defaults\n    params = {\n        \"start\": \".\",\n        \"output_dir\": \"./dataset/\",\n        \"questions_pathname\": \"./py2dataset_questions.json\",\n        \"model_config_pathname\": \"./py2dataset_model_config.yaml\",\n        \"use_llm\": False,\n        \"quiet\": False,\n        \"single_process\": False,\n        \"detailed\": False,\n        \"html\": False,\n        \"skip_regen\": False,\n        \"I\": False,\n    }\n\n    arg_string = \" \".join(sys.argv[1:])\n    for arg in params:  # parse command-line arguments\n        if \"--\" + arg in arg_string:\n            if isinstance(params[arg], bool):\n                params[arg] = True\n                arg_string = arg_string.replace(\"--\" + arg, \"\")\n            else:\n                value_segment = arg_string.split(\"--\" + arg + \" \")[1]\n                params[arg] = value_segment.split(\" --\")[0].strip('\"')\n                arg_string = arg_string.replace(\"--\" + arg + \" \" + params[arg], \"\")\n\n    if params[\"I\"]:  # query user for parameters to change\n        print(\"Interactive mode, enter new values or press enter to keep.\")\n        for arg in params:\n            if arg != \"I\":\n                user_input = input(f\"{arg} [{params[arg]}]: \").strip()\n                if isinstance(params[arg], bool):\n                    params[arg] = get_bool_from_input(user_input, params[arg])\n                elif user_input:\n                    params[arg] = user_input\n                print(f\"{arg}: {params[arg]}\")\n    params.pop(\"I\")\n\n    if params[\"start\"].startswith(\"https://github.com/\"):\n        params[\"start\"] = clone_github_repo(params[\"start\"])\n    elif not os.path.isdir(params[\"start\"]):\n        print(f\"'{params['start']}' Invalid. Using current working directory.\")\n        params[\"start\"] = os.getcwd()\n\n    py2dataset(**params)\n\n\nif __name__ == \"__main__\":\n    main()\n\n```",
        "code filename": "py2dataset/py2dataset.py"
    },
    {
        "document": "Overview: The 'py2dataset\\save_output.py' script offers utility functions for handling input/output operations within the py2dataset framework. It includes reading JSON or YAML files, writing data to these formats back, converting JSON files into HTML format, combining multiple JSON files into instruct.json and document_code.json while removing duplicates, generating code graphs based on Python file details as images, and saving Python file information in YAML format along with instruction data as JSON files.\n\n1) Purpose and Processing Approach of py2dataset\\save_output.py: This script serves as a supporting module for managing various input/output operations within the py2dataset toolchain by offering different functionalities related to handling data storage and presentation formats. These operations enable comprehensive analysis, merging datasets from multiple sources into cohesive representations suitable for diverse downstream tasks like training models or documentation generation.\n\n2) API Signatures with Function Descriptions:\na) read_file(file_path: This function reads JSON or YAML files given their paths and returns the contents as dictionaries. It identifies file types using suffixes ('json'/'yaml') and utilizes appropriate Python libraries (json/yaml) to load data accordingly.\nb) write_file(data, file_path): Accepts a dictionary 'data' and a path 'file_path', writes it into JSON or YAML format based on the suffix of 'file_path'. It uses json/'yaml' libraries for serialization with indentation settings (json/'SafeDumper').\nc) convert_json_to_html(directory): Iterates through all JSON files in a given directory ('directory'), converts them into HTML format by preserving spaces and tabs within input fields. It creates an HTML table representation of each file's content using string manipulation techniques.\nd) preserve_spacing(text, tab_width=4): A utility function that maintains whitespaces in text while replacing spaces with '&nbsp;' entities and tabs with specified width ('tab_width') sequences.\ne) combine_json_files(directory, html=False, questions): Combines JSON files from a directory into instruct.json excluding 'shargpt.json' and 'document_code.json', identifies documents describing the purpose from instructions based on query key (\"file_purpose\"). Also produces 'training.json', and when called with html set True performs conversion of generated JSON data to HTML format. Returns a dictionary containing 'instruct_list'.\nf) create_code_graph(file_details, base_name, output_subdir): Creates directed graphs representing Python file code structure using NetworkX library, saves them as PNG images in an output subdirectory ('output_subdir'). It requires file details ('file_details') extracted from a Python script and generates graphs for specific graph types.\ng) save_python_data(file_details, instruct_list, relative_path, output_dir): Takes extracted information of a Python file, its instructions dataset ('instruct_list'), a path within the output directory ('relative_path') and saves these data in YAML/'details.yaml' format while generating code graphs using 'create_code_graph'.\n\n3) Functional Requirements:\na) read_file(file_path): Reads JSON or YAML files, ensuring proper handling of file types through suffix identification ('file_type').\nb) write_file(data, file_path): Writes dictionary data into JSON/YAML format maintaining indentation for readability.\nc) convert_json_to_html(directory): Converts JSON files within a directory to HTML preserving whitespaces in input fields.\nd) combine_json_files(directory, html=False, questions): Merges JSON files into instruct.json and document_code.json after removing duplicates, generates training.json if needed, and converts merged data to HTML format when 'html' is True.\ne) create_code_graph(file_details, base_name, output_subdir): Generates code graphs from Python file details ('file_details'), saving them as PNG images in a subdirectory of 'output_dir'. It leverages networkX to create the graph representation.\nf) save_python_data(file_details, instruct_list, relative_path, output_dir): Processes a Python script's information ('file_details') along with instruction data ('instruct_list'), saves it in YAML/JSON format within 'output_dir', generates code graphs when called.\ng) preserve_spacing(text, tab_width=4): Preserves whitespaces by replacing spaces with HTML entities and tabs with specified width sequences ('tab_width').\n\n4) Variable Descriptions:\na) file_path (read_file): Path object representing the input JSON/YAML file location.\nb) file_type (read_file): File suffix indicating its format ('json'/'yaml').\nc) f with read_file: Opened file stream used for reading data into memory.\nd) row_parts (convert_json_to_html): List of strings to construct HTML table rows incrementally.\ne) html_content (convert_json_to_html): String containing the growing HTML representation with headers and rows data.\nf) column_count (convert_json_to_html): Number of columns in a JSON file table.\ng) dataset (convert_json_to_html): Contents of a JSON file read by 'read_file'.\nh) json_file (convert_json_to_html): Path object representing a JSON file within the directory.\ni) html_rows (convert_json_to_html): List holding rows constructed from table data for writing to HTML files.\nj) cleaned_name (combine_json_files): File name excluding extensions needed in 'instruct'/'document_code'.json files creation.\nk) code_filename (combine_json_files): Relative file names from merged JSON files required for 'document_code.json'.\nl) skip_files (combine_json_files): Set of file names to be skipped during processing ('instruct.json', 'shargpt.json').\nm) purpose_question (combine_json_files): String extracted from instruction dataset describing file purpose.\nn) purpose_data (combine_json_files): List containing entries matching the purpose query ('file_purpose') in combined data.\no) document_code (combine_json_files): List of dictionaries representing Python code documentation with 'code filename'.\np) nbytes (combine_json_files): Summation of encoded string lengths for each conversation edge weight in 'sharegpt' JSON file.\nq) output_subdir (create_code_graph): Path object referring to subdirectory storing output files/images generated by this script.\nr) pos (create_code_graph): Coordinates node positioning using spring layout for drawing graph edges ('nx.spring_layout').\ns) G (create_code_graph): NetworkX DiGraph object representing Python code structure.\nt) edge_labels (create_code_graph): Dictionary mapping edge tuples to multi-line labels in HTML format.\nu) base_name (save_python_data): Base name for output files generated from a Python script ('relative_path').\nv) output_file (create_code_graph): Path object representing the graph image file path with '.png' extension.''''",
        "code": "```python\n\"\"\"\nUtility functions for reading the input and saving the output of the py2dataset script.\nRequirements:\n[req01] The `read_file` function shall:\n        a. Accept a file path as an argument.\n        b. Read and return the contents of a JSON or YAML file as a dictionary.\n[req02] The `write_file` function shall:\n        a. Accept a dictionary and a file path as arguments.\n        b. Write the dictionary to a file in either JSON or YAML format.\n[req03] The `convert_json_to_html` function shall:\n        a. Convert JSON files within a given directory to HTML format.\n        b. Save each converted file with a .html extension.\n        c. Preserve spacing and tabs for the 'input' field.\n[req04] The `combine_json_files` function shall:\n        a. Accept a directory path as an argument.\n        b. Merge all JSON files in the directory.\n        c. Remove duplicates from the combined JSON files.\n        d. Write the combined data to 'instruct.json' files.\n        e. Convert the merged JSON files to HTML format.\n        f. Return the 'instruct_list' datasets.\n[req05] The `create_code_graph` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Generate code graphs based on the provided file details.\n        c. Save the graphs as PNG images in the specified output directory.\n[req06] The `save_python_data` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Save the details of the Python file as a YAML file.\n        c. Save the instruction data as JSON files.\n        d. Generate and save code graphs.\n\"\"\"\nimport json\nimport logging\nfrom html import escape\nfrom pathlib import Path\nfrom typing import Dict, List\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport yaml\n\n\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == \"json\":\n            return json.load(f)\n        if file_type == \"yaml\":\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}\n\n\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file.\n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open(\"w\") as f:\n        if file_type == \"json\":\n            json.dump(data, f, indent=4)\n        elif file_type == \"yaml\":\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n\n\ndef convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format and save .html file.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int = 4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(\" \", \"&nbsp;\").replace(\"\\t\", \"&nbsp;\" * tab_width)\n\n    for json_file in Path(directory).rglob(\"*.json\"):\n        try:\n            dataset = read_file(json_file)\n            if not dataset:\n                continue\n        except Exception:\n            continue\n\n        html_content = \"\"\"\n        <html>\n        <head>\n            <style>\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\n                th, td {\n                    border: 1px solid black;\n                    padding: 8px;\n                    text-align: left;\n                    white-space: pre-line;\n                    vertical-align: top;\n                    word-wrap: break-word;\n                }\n            </style>\n        </head>\n        <body>\n            <table>\n                <thead>\n                    <tr>\n        \"\"\"\n        column_count = len(dataset[0].keys())\n        column_width = round(100 / column_count, 2)\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += \"\"\"\n                    </tr>\n                </thead>\n                <tbody>\n        \"\"\"\n        html_rows = []\n        for entry in dataset:\n            row_parts = [\"<tr>\"]\n            for key in entry:\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace(\"\\n\", \"<br/>\")\n                row_parts.append(f\"<td>{value}</td>\")\n            row_parts.append(\"</tr>\")\n            html_rows.append(\"\".join(row_parts))\n        html_content += \"\".join(html_rows)\n\n        html_content += \"\"\"\n                </tbody>\n            </table>\n        </body>\n        </html>\n        \"\"\"\n        html_file_path = json_file.with_suffix(\".html\")\n        try:\n            with open(html_file_path, \"w\", encoding=\"utf-8\") as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f\"Failed saving: {html_file_path}\")\n\n\ndef combine_json_files(\n    directory: str, html: bool, questions: Dict\n) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and\n    then remove duplicates. Also generate a 'training.json' file.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n    logging.info(f\"Combining JSON files in {directory}\")\n\n    # Save instruct.json file\n    combined_data, code_filename = [], []\n    skip_files = {\"instruct.json\", \"shargpt.json\", \"document_code.json\"}\n    for json_file in Path(directory).rglob(\"*.json\"):\n        if json_file.name in skip_files:\n            continue\n        try:\n            file_data = read_file(json_file)\n            if file_data:\n                combined_data.extend(file_data)\n                cleaned_name = (\n                    json_file.relative_to(directory)\n                    .with_suffix(\"\")\n                    .as_posix()\n                    .replace(\".instruct\", \"\")\n                )\n                code_filename.append(cleaned_name)\n        except Exception as e:\n            logging.info(f\"Failed reading: {json_file}. Error: {e}\")\n    write_file(combined_data, Path(directory) / \"instruct.json\")\n\n    # Generate document_code.json file\n    purpose_question = [\n        item[\"text\"] for item in questions if item[\"id\"] == \"file_purpose\"\n    ][0]\n    purpose_question = purpose_question.split(\"{filename}\")[0]\n    purpose_data = [\n        item\n        for item in combined_data\n        if item[\"instruction\"].startswith(purpose_question)\n    ]\n    document_code = [\n        {\n            \"document\": item[\"output\"],\n            \"code\": item[\"input\"],\n        }\n        for item in purpose_data\n    ]\n    for i, item in enumerate(document_code):\n        item[\"code filename\"] = code_filename[i]\n    write_file(document_code, Path(directory) / \"document_code.json\")\n\n    # Generate sharegpt.json file\n    sharegpt = [\n        {\n            \"conversation\": [\n                {\"from\": \"system\", \"value\": f\"code documentation: {item['document']}\"},\n                {\n                    \"from\": \"human\",\n                    \"value\": \"Output the Python code described by the code documentation.\",\n                },\n                {\"from\": \"gpt\", \"value\": item[\"code\"]},\n            ],\n            \"nbytes\": \"0\",\n            \"source\": item[\"code filename\"],\n        }\n        for item in document_code\n    ]\n\n    # Compute the number of bytes for each conversation and update the nbytes field\n    for item in sharegpt:\n        nbytes = 0\n        for conv in item[\"conversation\"]:\n            nbytes += len(conv[\"value\"].encode(\"utf-8\"))\n        item[\"nbytes\"] = nbytes\n\n    write_file(sharegpt, Path(directory) / \"shargpt.json\")\n\n    if html:\n        logging.info(\"Converting JSON files to HTML\")\n        convert_json_to_html(directory)\n\n    return {\"instruct_list\": combined_data}\n\n\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    # create graph\n    graph_type = \"entire_code_graph\"\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details[\"file_info\"][graph_type][\"nodes\"])\n    for edge in file_details[\"file_info\"][graph_type][\"edges\"]:\n        source, target = edge[\"source\"], edge[\"target\"]\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(\n                source,\n                target,\n                **{\n                    k: v\n                    for k, v in edge.items()\n                    if k in [\"target_inputs\", \"target_returns\"]\n                },\n            )\n\n    # draw graph\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(\n        G,\n        pos,\n        with_labels=True,\n        font_weight=\"bold\",\n        font_size=8,\n        node_shape=\"s\",\n        node_size=500,\n        width=1,\n        arrowsize=12,\n    )\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if \"target_inputs\" in edge[2] and edge[2][\"target_inputs\"]:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if \"target_returns\" in edge[2] and edge[2][\"target_returns\"]:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[(edge[0], edge[1])] = \"\\n\".join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n\n    # save graph\n    output_file = output_subdir / f\"{base_name}.{graph_type}.png\"\n    plt.savefig(output_file)\n    plt.close()\n\n\ndef save_python_data(\n    file_details: dict, instruct_list: list, relative_path: Path, output_dir: str\n) -> None:\n    \"\"\"\n    Save the details of the Python file as a YAML file, the instruction data as JSON files,\n    and generate and save code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path of the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parent\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    base_name = relative_path.name\n    write_file(instruct_list, output_subdir / f\"{base_name}.instruct.json\")\n    write_file(file_details, output_subdir / f\"{base_name}.details.yaml\")\n\n    # Generating and saving the code graph\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except Exception as e:\n        logging.error(f\"Error creating graph for {base_name}: {e}\", exc_info=True)\n\n```",
        "code filename": "py2dataset/save_output.py"
    },
    {
        "document": "1. Purpose and Processing Approach of Python File `setup.py`:\nThe given `setup.py` file serves as a configuration script primarily utilized by the Setuptools package during software distribution processes such as packaging and installation of Python projects. It specifies crucial metadata about the project 'py2dataset', including its name, version, author details, description, dependencies, classifiers, requirements, entry points, etc., enabling seamless deployment and management on various platforms.\n\n2. API Signatures (Functional Detail):\na) `get_code_graph`: Information unavailable as this function signature is not explicitly provided in the given code snippet. It seems to be a module within 'py2dataset' package associated with creating a code graph for analyzing Python source code.\nb) `get_params`: Similarly, lacks direct signature detail but could represent another module handling specific parameters relevant to generating datasets from Python projects.\nc) `get_python_datasets`: Possibly involved in acquiring dataset objects derived from Python scripts as its name suggests; again no direct signature information.\nd) `get_python_file_details`: This may hold the function responsibility for fetching attributes concerning a single Python file or overall source code's structure information - uncertain about precise usage due to absent signature definition.\ne) `py2dataset` (likely module's entry point): Potential entry module performing comprehensive data processing, aggregating functionality of others, although explicit details omitted from setup file.\nf) `save_output`: Responsible for saving output generated during dataset creation process; signature not provided.\n- `main` function within `py2dataset` package (console script): Serves as the executable command-line interface to invoke 'py2dataset' tool via `python -m py2dataset`.\n\n3. Functional Requirements:\na) Generate structured datasets from Python source code: The primary purpose of 'py2dataset' is to analyze Python projects and create organized datasets out of them, potentially extracting relevant data for further analysis or processing.\nb) Compatibility with Python 3.8+ versions ensures the tool remains up-to-date with modern programming standards.\nc) Utilizes external libraries such as `matplotlib`, `networkx`, `ctransformers`, and `PyYaml` for enhanced functionality.\nd) Git integration through `GitPython` allows version control system interaction, potentially enabling dataset generation from source code repositories like GitHub or local ones.\ne) The MIT License provides open-source license information with an 'OS Independent' operational scope suggests platform compatibility across different operating systems.\n\n4. Variables Explanation in Code:\na) `long_description`: Contains contents from README.md file, providing extended documentation about the project in Markdown format.\nb) `name`, `version`, `author`, `author_email`, `description`, `url`, `py_modules`, `classifiers`, `python_requires`, `install_requires`, `entry_points`, `packages`, and `package_dir`: Metadata attributes defining project identity, dependencies, entry points, and packaging structure.\nc) `open(\"README.md\", \"r\")`: Opens README file in read mode for extracting long description content.\nd) `setup()` function call initiates setup process with provided arguments to configure 'py2dataset' package for distribution and installation.",
        "code": "```python\nfrom setuptools import setup\n\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\n\nsetup(\n    name=\"py2dataset\",\n    version=\"0.4\",\n    author=\"Jeff Meloy\",\n    author_email=\"jeffmeloy@gmail.com\",\n    description=\"A tool to generate structured datasets from Python source code\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/jeffmeloy/py2dataset\",\n    py_modules=[\"get_code_graph\", \"get_params\", \"get_python_datasets\", \"get_python_file_details\", \"py2dataset\", \"save_output\"],\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires=\">=3.8\",\n    install_requires=[\"matplotlib\", \"networkx\", \"ctransformers\", \"PyYaml\", \"GitPython\", ],\n    entry_points={\"console_scripts\": [\"py2dataset = py2dataset:main\"]},\n    packages=[\"py2dataset\"],\n    package_dir={\"py2dataset\": \".\\\\\"},\n)\n\n```",
        "code filename": "py2dataset/setup.py"
    }
]