[
    {
        "document": "I) The purpose of Python file 'get_code_graph.py' is to generate a graph representation of a given Python file along with its control flow structure while converting it into PlantUML format for visualization. This script aims to break down different aspects such as functions, classes, methods, and their interdependencies using networkx library for graph creation and parse Abstract Syntax Tree (AST) from Python code using ast module for extracting control flow logic. It then organizes the extracted information into a visually understandable format using PlantUML notation.\n\nII) Function Descriptions:\n1. code_graph(file_summary): Creates a dictionary representation of file details by building a directed graph using networkx DiGraph to depict function and class method relationships within the Python file based on provided 'file_summary'. It populates function_details_lookup and class_method_details_lookup dictionaries for function/class nodes addition in the graph along with edges. Edges hold metadata derived from get_edge_data_from_details function using node details retrieved later through those lookups, added edges, edges' metadata augmentation. Returned dict has 'nodes' containing graph nodes and 'edges' representing connectivity details.\n2. get_edge_data_from_details(target_details, source_details, target): Extracts edge data from target and source node details for adding directed links in code graph; appends edge inputs, return types of both entities depending upon available detail existence within given params. Returns edge attributes dictionary.\n3. add_edge_with_data(source, target, init_method=None): Helper function to add edges with metadata from class method lookup dictionaries into the networkx DiGraph using get_edge_data_from_details and necessary argument updates if applicable.\n4. add_edges_for_calls(source_name, calls): Loops through Python AST nodes finding functions/methods calls to build edges between them in code graph with appropriate metadata from lookup dictionaries.\n5. extract_control_flow_tree(nodes: List[ast.AST]): Parses given Abstract Syntax Tree nodes into a control flow structure representing Python program logic using keywords like 'def', 'class', 'if', 'while', 'for', 'try', etc., and returns it as a list of strings or nested dictionaries.\n6. reorganize_control_flow(code_graph, control_flow_structure): Orders execution in control flow graph with graph starting nodes according to interconnection structure segregates imported variables & global scope details and rest elements, organizes them into logical order using depth-first search (dfs_order) followed by mapping it back into a single list. Returns organized structure along with separated parts if any.\n7. segregate_control_flow(control_flow_structure): Separates imports/globals from the rest of control flow elements in given structure returning two lists; one containing import/global variables and another having program logic parts.\n8. dfs_order(graph, start, visited=None, order=None): Implementation of depth-first search traversal for determining execution order in a graph starting from specified nodes with optional visited set tracking and resultant order list creation.\n9. map_to_control_flow(execution_order, rest_structure): Maps execution order back to control flow elements by filtering relevant parts from given structure into a new list based on matching node names in execution sequence.\n10. get_plantUML_element(element: dict | str, indentation=\"\"): Creates PlantUML formatted strings representing elements of the control flow structure for indentation levels, managing 'def', 'class', 'if', 'try' partitioning blocks recursively while converting elements into visual graph notation.\n11. get_plantUML(control_flow_structure: List[Union[str, dict]]): Generates PlantUML representation of the entire control flow structure using get_plantUML_element function for each element in the given list and appends 'startuml', 'stopuml' tags at start/end respectively.\n12. get_code_graph(file_summary: Dict[str, Union[Dict, str], file_ast: ast.AST): Combines code graph creation using code_graph function with control flow extraction through extract_control_flow_tree followed by reorganization and PlantUML generation using get_plantUML; handles exceptions for any error occurrence during process steps returning the entire graph data, structured flow data & UML visualization string respectively.\n\nIII) Inputs/Variables/Returns details across functions:\n- code_graph(): Input 'file_summary' is a dictionary representing function defs and class defs information while file_ast argument denotes Python abstract syntax tree data from code parsed with ast library for control flow extraction purposes; returns code graph nodes & edges as dict.\n- get_edge_data_from_details(): Takes target details, source details, and target node names as inputs to derive edge attributes data based on availability; outputs a dictionary of edge metadata.\n- add_edge_with_data(): Called with source name, target node reference & optional initializer method for adding edges in code graph using lookup dictionaries along with metadata from get_edge_data_from_details().\n- add_edges_for_calls(): Passes function names having calls within file_summary to find corresponding class definitions and append related edges to the graph based on their hierarchies.\n- extract_control_flow_tree(): Takes Python AST nodes list as input, parses them into control flow structure containing strings for keywords like 'def', 'class' alongside nested partitions signifying different clauses as per code syntactic construction resulting into 'control_flow_tree' return list/dict collection.\n- reorganize_control_flow(): Requires code graph object & control flow structure derived from file execution order; manages separated imports, globals lists extraction, rest structured parts creation using segregate_control_flow() & dfs_order(). Outputs organized structure (lists) along with partitions if present.\n- segregate_control_flow(): Accepts control flow structure and segregates it into two separate lists containing imports/globals elements versus other program logic parts returning respective tuples.\n- dfs_order(): Receives graph object, start node reference along with optional visited set & order list; updates visited set tracking nodes traversal and populates order list accordingly.\n- map_to_control_flow(): Maps execution order to control flow elements by appending relevant parts into a new list based on node name matches from order list extracted using extract_control_flow_tree(). Outputs mapped structure.\n- get_plantUML_element(): Expects an element (dict or string) with indentation level as input; generates PlantUML representation for each control flow part recursively while handling different constructs like 'def', 'class', 'if', 'try' blocks etc.; returns formatted string.\n- get_plantUML(): Takes control flow structure list as input, applies get_plantUML_element() on every element creating PlantUML notation including '@startuml', '@enduml' tags around generated content; returns single string output.\n- get_code_graph(): Consumes file summary dictionary with Python code details and AST object for code graph creation followed by control flow extraction & visualization steps, catches exceptions if any occurring during processing returning entire graph data, structured flow data & UML representation respectively.",
        "code": "```python\nimport ast\nfrom typing import Dict, List, Optional, Union\nimport networkx as nx\n\n\ndef code_graph(\n    file_summary: Dict[str, Union[Dict, str]],\n) -> Dict[str, Union[List[str], Dict[str, List[str]]]]:\n    \"\"\"Create a dictionary representation of file details.\"\"\"\n    G = nx.DiGraph()\n\n    # Create lookup dictionaries for function and class method details\n    function_details_lookup = {}\n    for function_def in file_summary[\"function_defs\"]:\n        function_details_lookup.update(function_def)\n    class_method_details_lookup = {}\n    for class_def in file_summary[\"class_defs\"]:\n        for (\n            class_name,\n            class_details,\n        ) in class_def.items():  # Extract class name and details\n            G.add_node(class_name)  # Add class as a graph node\n            for method_name, method_details in class_details[\"method_defs\"].items():\n                qualified_method_name = (\n                    f\"{class_name}.{method_name}\"  # Create method fully qualified name\n                )\n                G.add_node(qualified_method_name)  # Add method as a graph node\n                class_method_details_lookup[qualified_method_name] = (\n                    method_details  # Store method details\n                )\n                G.add_edge(\n                    class_name, qualified_method_name\n                )  # Add edge from class to method\n\n    def get_edge_data_from_details(\n        target_details: dict, source_details: dict, target: str\n    ) -> dict:\n        \"\"\"Extract edge data from target details.\"\"\"\n        edge_data = {}\n        if target_details:\n            edge_data[\"target_inputs\"] = target_details.get(\"inputs\")\n            edge_data[\"target_returns\"] = list(set(target_details.get(\"returns\", [])))\n        if (\n            source_details\n            and \"call_inputs\" in source_details\n            and target in source_details[\"call_inputs\"]\n        ):\n            edge_data[\"target_inputs\"] = source_details[\"call_inputs\"][target]\n        return edge_data\n\n\n    def add_edge_with_data(\n        source: str, target: str, init_method: Optional[str] = None\n    ) -> None:\n        \"\"\"Helper function to add edge with data\"\"\"\n        target_details = class_method_details_lookup.get(\n            init_method or target\n        ) or function_details_lookup.get(target)\n        source_details = function_details_lookup.get(\n            source\n        ) or class_method_details_lookup.get(source)\n        G.add_edge(\n            source,\n            target,\n            **get_edge_data_from_details(target_details, source_details, target),\n        )\n\n    def add_edges_for_calls(source_name, calls):\n        \"\"\"Helper function to add edges for function or class method calls\"\"\"\n        class_names = [\n            list(class_def.keys())[0] for class_def in file_summary[\"class_defs\"]\n        ]\n        for called in calls:\n            called_class_name = called.split(\".\")[0]\n            if called.startswith(\"self.\"):\n                method_name = called.replace(\"self.\", \"\")\n                fully_qualified_name = f\"{source_name.split('.')[0]}.{method_name}\"\n                if fully_qualified_name in class_method_details_lookup:\n                    add_edge_with_data(source_name, fully_qualified_name)\n                    continue\n            if (\n                called in function_details_lookup\n                or called in class_method_details_lookup\n                or f\"{source_name.split('.')[0]}.{called}\"\n                in class_method_details_lookup\n            ):\n                add_edge_with_data(source_name, called)\n            elif called_class_name in class_names:\n                init_method = None\n                init_method_name = f\"{called}.__init__\"\n                if init_method_name in class_method_details_lookup:\n                    init_method = init_method_name\n                add_edge_with_data(source_name, called, init_method)\n            else:\n                G.add_node(called)\n                add_edge_with_data(source_name, called)\n\n    # Add function nodes to graph and edges for function calls\n    for function_name in function_details_lookup:\n        G.add_node(function_name)\n    for func_name, details in function_details_lookup.items():\n        add_edges_for_calls(func_name, details[\"calls\"])\n\n    # Add edges for method calls\n    for qualified_method_name, details in class_method_details_lookup.items():\n        add_edges_for_calls(qualified_method_name, details[\"calls\"])\n\n    # Add edge data to edges and create node and edges to return\n    for edge in G.edges:\n        source, target = edge\n        target_details = function_details_lookup.get(\n            target\n        ) or class_method_details_lookup.get(target)\n        source_details = function_details_lookup.get(\n            source\n        ) or class_method_details_lookup.get(source)\n        edge_data = get_edge_data_from_details(target_details, source_details, target)\n        G[source][target].update(edge_data)\n    nodes = list(G.nodes)\n    edges = [\n        {\"source\": edge[0], \"target\": edge[1], **edge[2]} for edge in G.edges.data()\n    ]\n\n    return {\"nodes\": nodes, \"edges\": edges}\n\n\ndef extract_control_flow_tree(nodes: List[ast.AST]) -> List[Union[str, dict]]:\n    \"\"\"Extract control flow tree from AST.\"\"\"\n    # todo1: make each function, class, and method a partition\n    # todo2: add visual indicators or styles to distinguish between functions, classes, and methods\n    # todo3: when calling an internal function, class, or method, add the line and arrow to the partition\n    # todo4: use different arrow styles or colors to indicate the type of call (e.g., function call, method call, class instantiation)\n    # todo5: make try/except like other conditional logic loops (maybe treat more like if loop somehow)\n    # todo6: add visual indicators to distinguish between the try, except, else, and finally sections\n    # todo7: make grouping better to keep all elements within the same box for imports and non-node_keywords_map elements within each function or method\n    # todo8: add visual separators or spacing between different types of elements (e.g., imports, non-keyword elements, control flow statements)\n    control_flow_tree = []\n    node_keywords_map = {\n        ast.FunctionDef: \"def\",\n        ast.AsyncFunctionDef: \"def\",\n        ast.ClassDef: \"class\",\n        ast.While: \"while\",\n        ast.For: \"for\",\n        ast.AsyncFor: \"for\",\n        ast.With: \"with\",\n        ast.AsyncWith: \"with\",\n        ast.Return: \"return\",\n        ast.If: \"if\",\n        ast.Try: \"try\",\n    }\n\n    for node in nodes:\n        node_type = type(node)\n        if node_type not in node_keywords_map:\n            control_flow_tree.append(ast.unparse(node))\n            continue\n        keyword = node_keywords_map[node_type]\n\n        if keyword == \"def\":\n            args_str = \", \".join([ast.unparse(arg) for arg in node.args.args])\n            key = f\"def {node.name}({args_str})\"\n            value = extract_control_flow_tree(node.body)\n            control_flow_tree.append({key: value})\n            # todo11: add support for visualizing function or method arguments and return values\n        elif keyword == \"class\":\n            key = f\"class {node.name}\"\n            value = extract_control_flow_tree(node.body)\n            control_flow_tree.append({key: value})\n        elif keyword == \"while\":\n            key = f\"while {ast.unparse(node.test)}\"\n            value = extract_control_flow_tree(node.body)\n            control_flow_tree.append({key: value})\n        elif keyword == \"for\":\n            key = f\"for {ast.unparse(node.target)} in {ast.unparse(node.iter)}\"\n            value = extract_control_flow_tree(node.body)\n            control_flow_tree.append({key: value})\n        elif keyword == \"with\":\n            key = f\"with {', '.join([ast.unparse(item) for item in node.items])}\"\n            value = extract_control_flow_tree(node.body)\n            control_flow_tree.append({key: value})\n        elif keyword == \"return\":\n            key = \"return\"\n            value = [ast.unparse(node.value)] if node.value is not None else []\n            control_flow_tree.append({key: value})\n        elif keyword == \"if\":\n            key = f\"if {ast.unparse(node.test)}\"\n            value = extract_control_flow_tree(node.body)\n            if_block = {key: value}\n            orelse = node.orelse\n            while orelse and isinstance(orelse[0], ast.If):\n                key = f\"elif {ast.unparse(orelse[0].test)}\"\n                value = extract_control_flow_tree(orelse[0].body)\n                if_block[key] = value\n                orelse = orelse[0].orelse\n            if orelse:\n                key = \"else\"\n                value = extract_control_flow_tree(orelse)\n                if_block[key] = value\n            control_flow_tree.append(if_block)\n        elif keyword == \"try\":\n            try_block = extract_control_flow_tree(node.body)\n            except_block = []\n            for handler in node.handlers:\n                h_type = ast.unparse(handler.type) if handler.type is not None else \"\"\n                h_name = (\n                    ast.unparse(handler.name)\n                    if isinstance(handler.name, ast.Name)\n                    else \"\"\n                )\n                key = f\"except {h_type} as {h_name}:\"\n                value = extract_control_flow_tree(handler.body)\n                except_block.append({key: value})\n            control_flow_dict = {\"try\": try_block, \"except\": except_block}\n            if node.orelse:\n                else_block = extract_control_flow_tree(node.orelse)\n                control_flow_dict[\"else\"] = else_block\n            if node.finalbody:\n                finally_block = extract_control_flow_tree(node.finalbody)\n                control_flow_dict[\"finally\"] = finally_block\n            control_flow_tree.append(control_flow_dict)\n        else:\n            control_flow_tree.append(ast.unparse(node))\n\n    return control_flow_tree\n\n\ndef reorganize_control_flow(code_graph, control_flow_structure):\n    \"\"\"Reorganize control flow structure to match the code graph.\"\"\"\n    # todo9: break up long strings to prevent plantuml errors\n    # todo10: add line breaks or truncation indicators to visually represent the continuation of long strings\n    targets = [edge[\"target\"] for edge in code_graph[\"edges\"]]\n    starting_points = [node for node in code_graph[\"nodes\"] if node not in targets]\n    visited = set()\n\n    def segregate_control_flow(control_flow_structure):\n        imports_globals = [element for element in control_flow_structure if isinstance(element, str)]\n        rest_structure = [element for element in control_flow_structure if isinstance(element, dict)]\n        return imports_globals, rest_structure\n\n    def dfs_order(graph, start, visited=None, order=None):\n        if visited is None:\n            visited = set()\n        if order is None:\n            order = []\n        visited.add(start)\n        for edge in graph[\"edges\"]:\n            if edge[\"source\"] == start and edge[\"target\"] not in visited:\n                dfs_order(graph, edge[\"target\"], visited, order)\n        order.append(start)  # This appends the node after all its descendants have been handled\n        return order\n\n    def map_to_control_flow(execution_order, rest_structure):\n        mapped_structure = []\n        for node in execution_order:\n            for element in rest_structure:\n                if (isinstance(element, dict) and node in next(iter(element))) or (isinstance(element, str) and node in element):\n                    if element not in mapped_structure:  # Ensure unique elements\n                        mapped_structure.append(element)\n        return mapped_structure\n\n    # Segregate imports and globals from the rest of the control flow structure\n    imports_globals, rest_structure = segregate_control_flow(control_flow_structure)\n    # Determine execution order from the code graph\n    execution_order = []\n    for start in starting_points:\n        if start not in visited:\n            temp_order = dfs_order(code_graph, start, visited, order=[])\n            execution_order.extend(temp_order)\n\n    # Reverse execution order to match logical flow\n    execution_order.reverse()\n    # Map execution order back to the control flow elements\n    organized_rest_structure = map_to_control_flow(execution_order, rest_structure)\n    # Combine imports_globals and organized_rest_structure for the final organized structure\n    organized_structure = imports_globals + organized_rest_structure\n\n    return organized_structure\n\ndef get_plantUML_element(element: dict, indentation=\"\") -> str:\n    \"\"\"Get PlantUML element from control flow structure.\"\"\"\n    plantuml_str = \"\"\n\n    if isinstance(element, dict):\n        key = next(iter(element))\n        value = element[key]\n\n        if \"def\" in key:\n            plantuml_str += (\n                f'{indentation}:{key};\\n'\n            )\n            inner_indentation = indentation + \"    \"\n            for item in value:\n                plantuml_str += get_plantUML_element(item, inner_indentation)\n\n        elif \"class\" in key or \"partition\" in key:\n            partition_name = key.split(\" \")[1]\n            plantuml_str += (\n                f'{indentation}partition \"{partition_name}\" {{\\n'\n            )\n            inner_indentation = indentation + \"    \"\n            for item in value:\n                plantuml_str += get_plantUML_element(item, inner_indentation)\n            plantuml_str += (\n                f'{indentation}}}\\n'\n            )\n\n        elif \"if\" in key or \"while\" in key or \"for\" in key:\n            condition = key.split(\" \", 1)[1]\n            plantuml_str += (\n                f'{indentation}if ({condition}) then;\\n'\n            )\n            inner_indentation = indentation + \"    \"\n            for item in value:\n                plantuml_str += get_plantUML_element(item, inner_indentation)\n            plantuml_str += (\n                f'{indentation}endif;\\n'\n            )\n\n        elif \"try\" in element:\n            plantuml_str += (\n                f'{indentation}partition \"try\" {{\\n'\n            )\n            inner_indentation = indentation + \"    \"\n            for item in element[\"try\"]:\n                plantuml_str += get_plantUML_element(item, inner_indentation)\n            plantuml_str += (\n                f'{indentation}}}\\n'\n            )\n\n            if \"except\" in element:\n                except_blocks = element[\"except\"]\n                if not isinstance(except_blocks, list):\n                    except_blocks = [except_blocks]\n                for except_block in except_blocks:\n                    except_key = next(iter(except_block))\n                    except_value = except_block[except_key]\n                    except_key = except_key.split(\" \", 1)[1]\n                    plantuml_str += (\n                        f'{indentation}partition \"{except_key}\" {{\\n'\n                    )\n                    inner_indentation = indentation + \"    \"\n                    for item in except_value:\n                        plantuml_str += get_plantUML_element(item, inner_indentation)\n                    plantuml_str += (\n                        f'{indentation}}}\\n'\n                    )\n\n            if \"else\" in element:\n                plantuml_str += (\n                    f'{indentation}else;\\n'\n                )\n                inner_indentation = indentation + \"    \"\n                for item in element[\"else\"]:\n                    plantuml_str += get_plantUML_element(item, inner_indentation)\n\n            if \"finally\" in element:\n                plantuml_str += (\n                    f'{indentation}finally;\\n'\n                )\n                inner_indentation = indentation + \"    \"\n                for item in element[\"finally\"]:\n                    plantuml_str += get_plantUML_element(item, inner_indentation)\n\n        else:\n            plantuml_str += (\n                f\"{indentation}:{key};\\n\"\n            )\n\n    elif isinstance(element, str):\n        plantuml_str += (\n            f\"{indentation}:{element};\\n\"\n        )\n\n    return plantuml_str\n\ndef get_plantUML(control_flow_structure: List[Union[str, dict]]) -> str:\n    \"\"\"Get PlantUML from control flow structure.\"\"\"\n    plantuml_str = \"@startuml\\n\"\n    plantuml_str += \"start\\n\"\n    for element in control_flow_structure:\n        plantuml_str += get_plantUML_element(element, \"\")\n    plantuml_str += \"stop\\n\"\n    plantuml_str += \"@enduml\"\n    return plantuml_str\n\n\ndef get_code_graph(file_summary: dict, file_ast: ast.AST) -> (dict, list, str):\n    \"\"\"Add code graph and control flow to file details.\"\"\"\n    try:\n        entire_code_graph = code_graph(file_summary)\n        control_flow_tree = extract_control_flow_tree(file_ast.body)\n        control_flow_structure = reorganize_control_flow(\n            entire_code_graph, control_flow_tree\n        )\n        plantUML = get_plantUML(control_flow_structure)\n    except Exception as e:\n        control_flow_structure = [str(e)]\n        plantUML = str(e)\n\n    #print('**********************')\n    #print(plantUML)\n    #print('**********************')\n\n    return entire_code_graph, control_flow_structure, plantUML\n\n```",
        "code filename": "py2dataset/get_code_graph.py"
    },
    {
        "document": "Software Documentation - Code Module Analysis Tool\n\nThis Python script consists of multiple utility functions aimed at preparing input data and instantiating a language model for generating detailed software documentation based on given context. The primary objective is to extract essential information about a Python file and create comprehensive documentation considering its dependencies, functions, classes, methods, inputs, variables, returns, docstrings, and overall processing approach. Let's elaborate on each function present within this module:\n\n1. get_start_dir(start_dir = \"\"):\n- Purpose: Returns the appropriate starting directory depending upon user input or defaults to current working directory if none provided.\n- Inputs: start_dir (optional)\n- Calls: Path().is_dir(), Path(), logging.info(), os.getcwd(), os.path.abspath()\n- Variables created/updated: start_dir\n- Return value: start_dir after setting up necessary directory paths.\n\n2. get_output_dir(output_dir = \"\"):\n- Purpose: Returns the appropriate output directory depending upon user input or defaults to current working directory if none provided and creates it if needed.\n- Inputs: output_dir (optional)\n- Calls: os.path.abspath(), os.makedirs(), logging.info()\n- Variables created/updated: output_dir\n- Return value: output_dir after setting up necessary directory paths and logs relevant information.\n\n3. get_questions(questions_pathname):\n- Purpose: Retrieves the question list from either provided or default JSON file path or returns default questions if invalid file encountered.\n- Inputs: questions_pathname (optional)\n- Calls: os.path.join(), open(), json.load(), logging.info()\n- Variables created/updated: questions_pathname, questions\n- Return value: questions list after reading JSON file or default questions if invalid file encountered.\n\n4. instantiate_model(model_config):\n- Purpose: Imports and instantiates a language model based on provided configuration parameters.\n- Inputs: model_config dictionary containing inference details\n- Calls: model_import_path splitting, attribute retrieval via getattr(), importlib.import_module(), function access using getattr() along with parameter modification\n- Variables created/updated: ModelClass, inference_function_name, model_params, llm, exception handling for errors during instantiation\n- Return value: Instantiated language model object or None on failure.\n\n5. get_model(model_config_pathname):\n- Purpose: Returns instantiated model and prompt template based on the provided configuration file path or defaults to default settings if invalid file encountered.\n- Inputs: model_config_pathname (optional)\n- Calls: os.path.join(), open(), yaml.safe_load(), logging.info(), get_default_model_config(), instantiate_model()\n- Variables created/updated: model_config, model_config_pathname\n- Return value: Updated model configuration dictionary with an instantiated language model object or default settings if invalid file encountered.\n\n6. write_questions_file(output_dir = \"\"):\n- Purpose: Writes the default questions into a JSON formatted file at specified output directory (defaults to current working directory).\n- Inputs: output_dir (optional)\n- Calls: get_default_questions(), Path().is_dir(), os.getcwd(), os.path.join(), open(), json.dump()\n- Variables created/updated: questions, output_dir\n\n7. write_model_config_file(output_dir = \"\"):\n- Purpose: Writes default model configuration into a YAML formatted file at specified output directory (defaults to current working directory).\n- Inputs: output_dir (optional)\n- Calls: get_default_model_config(), Path().is_dir(), os.getcwd(), os.path.join(), open(), yaml.dump()\n- Variables created/updated: model_config, output_dir\n\n8. get_default_questions():\n- Purpose: Returns default question list hardcoded within the script.\n- Return value: List of dictionaries containing questions with IDs, text, and types.\n\n9. get_default_model_config():\n- Purpose: Prepares standard configuration data related to Language Model and other associated properties such as model details.\n- Variables created/updated: model_config (initializes this dictionary)\n- Return value: Default model configuration dictionary.",
        "code": "```python\nimport os\nimport json\nimport logging\nimport importlib\nfrom typing import Dict, List\nfrom pathlib import Path\nimport yaml\n\n# Setting up a basic logger\nlogging.basicConfig(level=logging.INFO)\n\n# defaults if provided inputs fail\nQUESTIONS_FILE = \"py2dataset_questions.json\"\nMODEL_CONFIG_FILE = \"py2dataset_model_config.yaml\"\nOUTPUT_DIR = \"datasets\"\n\n\ndef get_default_questions() -> List[Dict]:\n    \"\"\"Return default question list\"\"\"\n    questions = [\n        {\n            \"id\": \"file_dependencies\",\n            \"text\": \"Dependencies in Python file: `{filename}`?\",\n            \"type\": \"file\",\n        },\n        {\n            \"id\": \"entire_code_graph\",\n            \"text\": \"Call code graph in Python file: `{filename}`?\",\n            \"type\": \"file\",\n        },\n        {\n            \"id\": \"file_functions\",\n            \"text\": \"Functions in Python file: `{filename}`?\",\n            \"type\": \"file\",\n        },\n        {\n            \"id\": \"file_classes\",\n            \"text\": \"Classes in Python file: `{filename}`?\",\n            \"type\": \"file\",\n        },\n        {\n            \"id\": \"function_inputs\",\n            \"text\": \"Inputs to `{function_name}` in Python file: `{filename}`?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"function_docstring\",\n            \"text\": \"Docstring of `{function_name}` in Python file: `{filename}`?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"function_calls\",\n            \"text\": \"Calls made in `{function_name}` in Python file: `{filename}`?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"function_variables\",\n            \"text\": \"Variables in `{function_name}` in Python file: `{filename}`?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"function_returns\",\n            \"text\": \"Returns from `{function_name}` in Python file: `{filename}`?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"class_methods\",\n            \"text\": \"Methods in `{class_name}` in Python file: `{filename}`?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"class_docstring\",\n            \"text\": \"Docstring of `{class_name}` in Python file: `{filename}`?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"class_attributes\",\n            \"text\": \"Attributes of `{class_name}` in Python file: `{filename}`?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"class_inheritance\",\n            \"text\": \"Inheritance of `{class_name}` in Python file: `{filename}`?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"method_inputs\",\n            \"text\": \"Inputs to `{method_name}` in Python file: `{filename}`?\",\n            \"type\": \"method\",\n        },\n        {\n            \"id\": \"method_docstring\",\n            \"text\": \"Docstring of `{method_name}` in Python file: `{filename}`?\",\n            \"type\": \"method\",\n        },\n        {\n            \"id\": \"method_calls\",\n            \"text\": \"Calls made in `{method_name}` in Python file: `{filename}`?\",\n            \"type\": \"method\",\n        },\n        {\n            \"id\": \"method_variables\",\n            \"text\": \"Variables in `{method_name}` in Python file: `{filename}`?\",\n            \"type\": \"method\",\n        },\n        {\n            \"id\": \"method_returns\",\n            \"text\": \"Returns from `{method_name}` in Python file: `{filename}`?\",\n            \"type\": \"method\",\n        },\n        {\n            \"id\": \"file_purpose\",\n            \"text\": \"I) Describe the Purpose and Processing Approach for Python file: `{filename}`; II) Define detailed Requirements, API Signatures, and Logic for all Functions and Class Methods; III) Explain the purpose of the inputs, variables, calls, and returns in the code.\",\n            \"type\": \"file\",\n        },\n    ]\n    return questions\n\n\ndef get_default_model_config() -> Dict:\n    \"\"\"Return default model config dict\"\"\"\n    model_config = {\n        \"system_prompt\": \"Lang: English. Output Format: unformatted, outline. Task: Create detailed software documentation for publication using this entire code module Context:\\n'{context}'\\n\",\n        \"instruction_prompt\": \"Analyze Context considering these objects:\\n'{code_objects}'\\n to comply with this instruction:\\n'{query}'\\n\",\n        \"prompt_template\": \"SYSTEM: {system_prompt} USER: {instruction_prompt} ASSISTANT:\",\n        \"inference_model\": {\n            \"model_import_path\": \"ctransformers.AutoModelForCausalLM\",\n            \"model_inference_function\": \"from_pretrained\",\n            \"model_params\": {\n                \"model_path\": \"jeffmeloy/WestLake-7B-v2.Q8_0.gguf\",\n                \"model_type\": \"mistral\",\n                \"local_files_only\": False,\n                ## MODEL CONFIGURATION PARAMETERS (params set for model with this HW: GPU: 4090-24GB VRAM, CPU: 5950x-64GB RAM)\n                # avx2 and gpu_layers are not compatible\n                # \"lib\": \"avx2\",\n                \"threads\": 16,\n                \"batch_size\": 512,\n                \"context_length\": 40000,\n                \"max_new_tokens\": 20000,\n                \"gpu_layers\": 100,\n                \"reset\": True,\n            },\n        },\n    }\n    return model_config\n\n\ndef get_start_dir(start_dir: str = \"\") -> str:\n    \"\"\"Returns the appropriate start directory.\"\"\"\n    if start_dir and not Path(start_dir).is_dir():\n        logging.info(f\"Setting Start Dir : {start_dir}\")\n        start_dir = os.getcwd()\n    else:\n        start_dir = os.path.abspath(start_dir)\n    return start_dir\n\n\ndef get_output_dir(output_dir: str = \"\") -> str:\n    \"\"\"Returns the appropriate output directory.\"\"\"\n    output_dir = os.path.abspath(output_dir or OUTPUT_DIR)\n    os.makedirs(output_dir, exist_ok=True)\n    logging.info(f\"Using output directory: {output_dir}\")\n    return output_dir\n\n\ndef get_questions(questions_pathname: str) -> List[Dict]:\n    \"\"\"Get questions from file or default\"\"\"\n    try:  # get questions from provided or default configuration file\n        if not questions_pathname:\n            questions_pathname = os.path.join(os.getcwd(), QUESTIONS_FILE)\n        with open(questions_pathname, \"r\") as f:\n            questions = json.load(f)\n        logging.info(f\"Using questions from file: {questions_pathname}\")\n    except (FileNotFoundError, json.decoder.JSONDecodeError):\n        logging.info(\n            f\"Questions file not valid: {questions_pathname} Using default questions\"\n        )\n        questions = get_default_questions()\n    return questions\n\n\ndef instantiate_model(model_config: Dict) -> object:\n    \"\"\"Imports and instantiates a model based on the provided configuration.\"\"\"\n    try:\n        module_name, class_name = model_config[\"model_import_path\"].rsplit(\".\", 1)\n        ModelClass = getattr(importlib.import_module(module_name), class_name)\n        model_params = model_config[\"model_params\"]\n        inference_function_name = model_config[\"model_inference_function\"]\n        if inference_function_name != \"\":\n            inference_function = getattr(ModelClass, inference_function_name)\n            llm = inference_function(model_params.pop(\"model_path\"), **model_params)\n        else:\n            llm = ModelClass(model_params.pop(\"model_path\"), **model_params)\n        return llm\n    except (ImportError, AttributeError, Exception) as e:\n        logging.info(f\"Failed to instantiate the model. Error: {e}\")\n        return None\n\n\ndef get_model(model_config_pathname: str) -> object:\n    \"\"\"Returns an instantiated model and prompt template based on the model configuration.\"\"\"\n    try:\n        if not model_config_pathname:\n            model_config_pathname = os.path.join(os.getcwd(), MODEL_CONFIG_FILE)\n        with open(model_config_pathname, \"r\") as config_file:\n            model_config = yaml.safe_load(config_file)\n        logging.info(f\"Using model config from file: {model_config_pathname}\")\n    except (FileNotFoundError, yaml.YAMLError):\n        logging.info(\n            f\"Model config file not valid: {model_config_pathname} Using default model config\"\n        )\n        model_config = get_default_model_config()\n    model_config[\"model\"] = instantiate_model(model_config[\"inference_model\"])\n\n    return model_config\n\n\ndef write_questions_file(output_dir: str = \"\") -> None:\n    \"\"\"Writes the default questions to a file in JSON format.\"\"\"\n    questions = get_default_questions()\n    output_dir = output_dir if output_dir and Path(output_dir).is_dir() else os.getcwd()\n    with open(os.path.join(output_dir, QUESTIONS_FILE), \"w\") as file:\n        json.dump(questions, file, indent=4)\n\n\ndef write_model_config_file(output_dir: str = \"\") -> None:\n    \"\"\"Writes the default model config to a file in YAML format.\"\"\"\n    model_config = get_default_model_config()\n    output_dir = output_dir if output_dir and Path(output_dir).is_dir() else os.getcwd()\n    with open(os.path.join(output_dir, MODEL_CONFIG_FILE), \"w\") as file:\n        yaml.dump(model_config, file)\n\n```",
        "code filename": "py2dataset/get_params.py"
    },
    {
        "document": "I) Description for Python File 'py2dataset\\get_python_datasets.py': This given file mainly contains a software module focusing on extracting information from Python files, structuring them into organized data format through the generation of explanations related to its constituent parts such as entire files, functions, methods, classes, etc., and producing responses using Natural Language Processing techniques when required. It utilizes various dependencies like 're', 'logging', 'yaml', and 'math'. The core functionality is encapsulated within a class named 'DatasetGenerator' that handles initialization, processing questions related to different aspects of the Python file, generating detailed responses leveraging an external language model if configured, formatting responses as required by 'py2dataset_model_config.yaml'. Notable methods are: initialize (__init__), format output formats ('format_response'), invoke language modeling capabilities ('get_response_from_llm'), generate detailed explanations ('get_detailed_response'), collect code responses ('get_code_qa'), process questions ('process_question', 'process_question_type'), retrieve string data from information dictionary ('get_info_string'), and overall dataset generation ('generate').\n\nII) Detailed Requirements, API Signatures & Logic Breakdown:\n1. get_unique_elements(input_str): This function cleans an input string to obtain a list of unique elements after removing extra characters like brackets and quotes. It generates element iterators using 'element_generator' function which yields cleaned elements when parsing the input string based on braces and commas. Finally, it joins these cleaned elements separated by comma into a single string.\n- Inputs: input_str (a string)\n- Returns: unique element strings in formatted 'str'.\n\n2. DatasetGenerator Class initialization (__init__): Creates an object of this class that serves as a Python dataset generator, consuming essential inputs and initializing various attributes for subsequent operations like llm instance availability ('self.use_llm'), instruct_list, question mapping ({'file': 'functions', 'class': 'methods' etc.), and others referenced by default settings. It accepts keyword arguments corresponding to given variables during object formation - file path (str), file details dictionary, base name (string), questions list of dicts, model configuration dictionary ('model_config'), detailed flag (bool).\n- Inputs: self, file_path (str), file_details (dict), base_name (str), questions (list[dict]), model_config (dict), detailed (bool)\n- Called By: get_python_datasets()\n\n3. format_response(): Formats the 'code_qa_response' attribute by applying YAML dumping with specific settings to create a structured JSON output, stripping unwanted quotes and newlines from it.\n- Inputs: self (DatasetGenerator instance)\n- Returns: None but modifies 'self.code_qa_response'.\n\n4. get_response_from_llm(query:str, context:str): Generates language model response to a given query considering provided context. It tries different strategies for context size optimization before sending the prompt to the language model ('self.llm') and formats the output removing special tokens while logging messages during progression. This method can produce detailed responses when necessary via 'get_detailed_response'.\n- Inputs: self, query (str), context (str)\n- Returns: Language model response as string if successful else empty string ('').\n\n5. get_detailed_response(context:str, response:str): Gathers detailed explanations for specific code objects present in instruct_list based on LLM generated responses, appending outputs to 'item' instances stored therein. It uses contextual information and query strings for this purpose.\n- Inputs: self, context (str), response (str)\n- Returns: None but modifies self attributes.\n\n6. get_code_qa(): Retrieves code responses from instruct_list after parsing given question list, preparing code responses accordingly for various Python entities like functions or classes. It creates 'responses' dictionary with code objects and types as keys to store output strings related to them.\n- Inputs: self\n- Returns: None but modifies self attributes ('self.code_qa_dict').\n\n7. process_question(question_id:str, query:str, context:str, info): Handles question processing depending upon given type (file, function, class or method). It calls corresponding methods based on the type to generate required responses.\n- Inputs: self, question_id (str), query (str), context (str), info (dict)\n- Returns: None but updates 'self.instruct_list'.\n\n8. get_info_string(info:dict, item_type:str): Extracts string data from the information dictionary for specified items and joins them with commas into a single string.\n- Inputs: info (dict), item_type (str)\n- Returns: formatted unique item string('').\n9. process_question_type(self, question_type:str, question_id:str, question_text:str): Processes questions related to files, functions, classes or methods by calling appropriate methods based on 'question_type'. It formats query strings with contextual data and invokes 'process_question' accordingly.\n- Inputs: self, question_type (str), question_id (str), question_text (str)\n- Returns: None but updates 'self.instruct_list'.\n\n10. generate(): Generates responses for all questions in the given list and returns instruct_list containing processed queries with their respective outputs.\n- Inputs: self\n- Returns: tuple[list[dict], list[dict] (DatasetGenerator instance's instruct_list).\n\nIII) Purpose of Inputs, Variables, Calls & Returns in the Code:\na. Inputs 'file_path': Python file path for processing.\n- Used In: DatasetGenerator initialization (__init__)\nb. Inputs 'file_details': Dictionary containing file information required to generate dataset responses.\n- Used In: DatasetGenerator initialization (__init__), get_response_from_llm(), process_question(), process_question_type().\nc. Inputs 'base_name': Relating file basename. Useful while forming informational strings (e.g., output filename).\n- Used In: DatasetGenerator initialization (__init__), get_python_datasets() format_response().\nd. Inputs 'questions': A list containing queries in a dict form ('questionID', {'type','id'...) expected response structures awaiting processing via 'DatasetGenerator'.\n- Used In: DatasetGenerator initialization (__init__), generate().\ne. Inputs 'model_config': Model configuration dictionary with LLM parameters.\n- Used In: DatasetGenerator initialization (__init__), get_response_from_llm(), format_response().\nf. Inputs 'detailed': Determines if elaborate responses from language models should be fetched for each code object in Python files.\n- Used In: DatasetGenerator initialization (__init__).\ng. Output Variable '_returns'_ ('instruct_list'): This generated list carries a summary of instructed actions performed (either by dataset generation, code documentation generation or LLM queries) with input-output pairs. Returned by 'generate()'.\n- Used In: get_python_datasets().\nh. Output Variable '_returns'_ ('code_qa_dict'): A dictionary containing Python file information structured as JSON format after processing all questions. Returned by 'generate()'.\n- Used In: get_python_datasets().\n\ni. element_generator Calls: Parses input string considering braces and commas to generate iterators for unique elements extraction.\nj. logging: Python's built-in module used for error reporting during processing steps.\nk. re Regular expression ('r\"\\n\\\\\\n\"') Separator of string multilines before format().\nl. yaml Object managing hierarchical data in plain text format (JSON compatible).\nm. math: Handles mathematical operations like calculating proportions or lengths as needed.\nn. llm ('self.llm'): Language model instance handling queries when detailed explanations are desired. It is configured by 'model_config'.\no. prompt_template Strings for language model prompts defined in 'DatasetGenerator's model_config'.\np. context_strategies List: Iterates over strategies to optimize LLM context size before sending queries.\nq. str(): Converts objects into string format when required.\nr. enumerate(): Generates pairs of index and values from iterables during element generator function.\ns. tokenize('prompt'): Counts LLM tokens in given prompt text for context optimization.\nt. llm Object invocation: Actual language model call with prepared prompts.\nu. line Iterator over lines in LLM responses stripping leading whitespaces.\nv. yaml.Dumper(): Safe dumping of YAML data into strings with custom settings.\nw. default_flow_style & width Float value for YAML formatting purposes during 'yaml.dump()'.\nx. indent Integer: Sets indentation level in YAML output format.\ny. line.lstrip(): Removes leading whitespaces from lines while LLM response parsing.\nz. class_info Dictionary containing classes and their methods information extracted from file details.\naa. system_prompt & instruction_prompt: Constant strings for language model prompt building ('DatasetGenerator').\nab. File path replacements (\"\\\\\" -> \"/\"): Simplifies file name paths during format_response().\nac. len(): Returns length of sequences like context size calculation in get_response_from_llm().\nad. class_.* Strings are placeholders for extracted Python code objects ('class', 'method') used in question formatting.\nae. mapping Dictionary carries queries key-value pairs like ['code Object' and output]. It expands when constructing response strings for code explanation purpose ('get_detailed_response').\naf. query Strings are placeholders for contextual information ('filename', 'Purpose') used in process_question().\nag. item Iterator over instruct_list elements during generate().\nah. str(): Converts objects into string format when required.\nai. response String variable carries output after executing functions or extracting information. It holds code explanation if found valid ('get_response_from_llm()', 'process_question').\naj. item Iterator over instruct_key and instruct_value pairs during get_detailed_response().\nak. instruction Iterator for 'instruct_list' queries based on key matching.\nal. value Holds extracted values from dicts while updating detailed explanations ('get_detailed_response').\nam. variables String joining query text in process_question() forming descriptive purposes.\nan. filtered Iterable removing empty strings during get_unique_elements().\nao. info Dictionary containing file details like 'file_info', 'classes' etc., used for information extraction ('get_info_string').\nap. key Iterator over classes or methods names in class/function iteration loops ('process_question_type').\naq. method_name String formed from class name and method identifier ('process_question_type').\nar. inputs Strings joined for code objects' input parameters ('get_info_string').\nas. methods Iterator over methods list extracted from classes information ('process_question_type')..\n\nDesigned Class Structure incorporates the full code operation life-cycle ranging from importing required modules up till generation of explanations regarding Python files and their constituents in JSON format. It leverages language models for detailed responses when necessary, ensuring a comprehensive understanding of Python code elements through question processing.",
        "code": "```python\nimport logging\nimport re\nimport math\nimport yaml\n\n\ndef get_unique_elements(input_str: str) -> str:\n    \"\"\"Clean an input string and return a string of unique elements.\"\"\"\n\n    def element_generator(input_str):\n        start, brace_level = 0, 0\n        for i, char in enumerate(input_str):\n            if char in \"{}\":\n                brace_level += 1 if char == \"{\" else -1\n            elif char == \",\" and brace_level == 0:\n                yield input_str[start:i].strip(\"'\\\" \")\n                start = i + 1\n        yield input_str[start:].strip(\"'\\\" \")\n\n    input_str = input_str.strip(\"[]'\\\"\")\n    cleaned_elements = [element for element in element_generator(input_str) if element]\n    return \", \".join(cleaned_elements)\n\n\nclass DatasetGenerator:\n    \"\"\"Generate JSON formatted dictionary outputs for a Python file.\"\"\"\n\n    def __init__(\n        self,\n        file_path: str,\n        file_details: dict,\n        base_name: str,\n        questions: list[dict],\n        model_config: dict,\n        detailed: bool,\n    ) -> None:\n        \"\"\"Initialize the DatasetGenerator class.\"\"\"\n        self.file_path = file_path\n        self.file_details = file_details\n        self.base_name = base_name\n        self.questions = questions\n        self.model_config = model_config\n        self.llm = model_config[\"model\"] if model_config else None\n        self.use_llm = bool(model_config)\n        self.detailed = detailed if self.use_llm else False\n        self.instruct_list = []\n        self.question_mapping = {\n            \"file\": \"file\",\n            \"function\": \"functions\",\n            \"class\": \"classes\",\n            \"method\": \"classes\",\n        }\n        self.code_qa_list = []\n        self.code_qa_response = \"\"\n\n    def format_response(self) -> None:\n        \"\"\"Format the response for the code_qa_dict attribute.\"\"\"\n        self.code_qa_response = (\n            re.sub(\n                r\"\\n\\s*\\n\",\n                \"\\n\",\n                yaml.dump(\n                    self.code_qa_dict,\n                    Dumper=yaml.SafeDumper,\n                    width=float(\"inf\"),\n                    sort_keys=False,\n                    default_flow_style=False,\n                    indent=2,\n                ),\n            )\n            .replace(\"''\", \"'\")\n            .strip('\"')\n            .strip(\"'\")\n            .strip()\n        )\n        self.file_details[\"file_info\"][\"code_qa_response\"] = self.code_qa_response\n\n    def get_response_from_llm(self, query: str, context: str) -> str:\n        \"\"\"Get language model response to query for given context.\"\"\"\n\n        context_strategies = [\n            lambda: str(context),\n            lambda: f\"```python\\n{self.file_details['file_info']['file_code_simplified']}\\n```\",\n            lambda: f\"```python\\n{self.get_info_string(self.file_details['file_info'], 'file_summary')}\\n```\",\n            lambda: \"\",\n        ]\n\n        max_context_length = self.model_config[\"inference_model\"][\"model_params\"][\n            \"context_length\"\n        ]\n        prompt_template = self.model_config[\"prompt_template\"].format(\n            system_prompt=self.model_config[\"system_prompt\"],\n            instruction_prompt=self.model_config[\"instruction_prompt\"],\n        )\n\n        for strategy in context_strategies:\n            context = strategy()\n            prompt = prompt_template.format(\n                context=context, query=query, code_objects=self.code_qa_response\n            )\n            context_size = len(self.llm.tokenize(prompt))\n            logging.info(f\"***Context Size: {context_size}\")\n            if context_size <= 0.70 * max_context_length:\n                break\n        else:\n            err_msg = \"Model response failed, increase py2dataset_model_config.yaml context_length\"\n            logging.error(f\"{err_msg} > {math.ceil(context_size/0.70)}\")\n            return \"\"\n\n        context = self.code_qa_list if context == \"\" else context\n        response = \"\"\n\n        try:\n            response = (\n                re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", self.llm(prompt))\n                .replace(\"<|im_end|>\", \"\")\n                .replace(\"</|im_end|>\", \"\")\n            )\n            response = \"\\n\".join(line.lstrip() for line in response.split(\"\\n\"))\n            logging.info(f\"***Overall Response: {response}\")\n        except Exception as error:\n            logging.error(f\"Failed to generate model response: {error}\")\n\n        if self.detailed:\n            self.get_detailed_response(context, response)\n\n        basename = str(self.base_name).replace(\"\\\\\", \"/\")\n        self.code_qa_dict = {basename: self.code_qa_dict}\n        self.code_qa_dict[basename] = {\n            \"Code Documentation\": [response],\n            **self.code_qa_dict[basename],\n        }\n        self.format_response()\n        self.file_details[\"file_info\"][\"purpose\"] = response\n        return response\n\n    def get_detailed_response(self, context: str, response: str) -> None:\n        \"\"\"Generate detailed responses for code objects.\"\"\"\n        for item in self.code_qa_list:\n            try:\n                instruct_key = list(item.keys())[0]\n                instruct_value = list(item.values())[0]\n                query = f\"Describe the Purpose and Significance of these {instruct_key}: [{instruct_value}] and Explain what each of these {instruct_key} does in the code.\"\n                prompt = (\n                    self.model_config[\"prompt_template\"]\n                    .format(\n                        system_prompt=self.model_config[\"system_prompt\"],\n                        instruction_prompt=self.model_config[\"instruction_prompt\"],\n                    )\n                    .format(\n                        context=f\"{context}/nCode Summary:/n{response}\",\n                        query=f\"{query}\",\n                        code_objects=f\"{instruct_value}\",\n                    )\n                )\n                item_response = (\n                    re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", self.llm(prompt))\n                    .replace(\"<|im_end|>\", \"\")\n                    .replace(\"</|im_end|>\", \"\")\n                )\n                logging.info(f\"\\n***Itemized Response: {query}\\n{item_response}\")\n                for item in self.instruct_list:\n                    if item[\"instruction\"].startswith(instruct_key):\n                        output = f\"\\n\\nPurpose:\\n{item_response}\"\n                        item[\"output\"] += output\n\n                if \"`\" in instruct_key:\n                    dict_key1, dict_key2 = (\n                        instruct_key.split(\"`\")[1],\n                        instruct_key.split()[0],\n                    )\n                else:\n                    dict_key1, dict_key2 = instruct_key, \"\"\n\n                purpose_dict = {\"Purpose\": item_response.strip()}\n                if dict_key1 not in self.code_qa_dict:\n                    self.code_qa_dict[dict_key1] = {}\n                elif not isinstance(self.code_qa_dict[dict_key1], dict):\n                    value = self.code_qa_dict[dict_key1]\n                    self.code_qa_dict[dict_key1] = {\"Value\": value}\n\n                if dict_key2:\n                    if not isinstance(self.code_qa_dict[dict_key1], dict):\n                        self.code_qa_dict[dict_key1] = {}\n                    if dict_key2 not in self.code_qa_dict[dict_key1] or not isinstance(\n                        self.code_qa_dict[dict_key1][dict_key2], dict\n                    ):\n                        value = self.code_qa_dict[dict_key1].get(dict_key2, \"\")\n                        self.code_qa_dict[dict_key1][dict_key2] = {\"Value\": value}\n                    self.code_qa_dict[dict_key1][dict_key2].update(purpose_dict)\n                else:\n                    self.code_qa_dict[dict_key1].update(purpose_dict)\n            except Exception as error:\n                logging.error(f\"Failed to generate detailed response: {error}\")\n\n    def get_code_qa(self) -> None:\n        \"\"\"Get code responses from the instruct_list and update.\"\"\"\n        excluded = {\"Call code graph\", \"Docstring\"}\n        self.code_qa_list = []\n        responses = {}\n        for item in self.instruct_list:\n            instruction = item[\"instruction\"].split(\" in Python file:\")[0]\n            output = (item[\"output\"],)\n            if not any(instruction.startswith(prefix) for prefix in excluded):\n                self.code_qa_list.append({instruction: output})\n                if \"`\" in instruction:\n                    code_object, code_type = (\n                        instruction.split(\"`\")[1],\n                        instruction.split()[0],\n                    )\n                    responses.setdefault(code_object, []).append((code_type, output))\n                else:\n                    responses.setdefault(instruction, []).append((instruction, output))\n\n        self.code_qa_dict = {}\n        for code_object, type_responses in responses.items():\n            self.code_qa_dict[code_object] = {}\n            for code_type, response in type_responses:\n                if code_object == code_type:\n                    self.code_qa_dict[code_object] = response\n                else:\n                    self.code_qa_dict.setdefault(code_object, {})[code_type] = response\n\n        if not self.use_llm:\n            basename = str(self.base_name).replace(\"\\\\\", \"/\")\n            self.code_qa_dict = {basename: self.code_qa_dict}\n\n        self.format_response()\n\n    def process_question(\n        self, question_id: str, query: str, context: str, info: dict\n    ) -> None:\n        \"\"\"Process question and add the generated response to the instruct_list.\"\"\"\n        if question_id.endswith((\"code_graph\", \"docstring\")):\n            response = info.get(question_id, {})\n        elif question_id.endswith(\"file_purpose\"):\n            self.get_code_qa()\n            response = (\n                self.get_response_from_llm(query, context) if self.use_llm else \"\"\n            )\n        else:\n            response = get_unique_elements(str(info.get(question_id, \"\")))\n\n        if response and response != \"None\":\n            response = str(response).strip()\n            self.instruct_list.append(\n                {\"instruction\": query, \"input\": context, \"output\": response}\n            )\n\n    @staticmethod\n    def get_info_string(info: dict, item_type: str) -> str:\n        \"\"\"Get string from info dictionary.\"\"\"\n        return \", \".join(\n            [item.strip() for item in str(info.get(item_type, \"\")).split(\",\") if item]\n        )\n\n    def process_question_type(\n        self, question_type: str, question_id: str, question_text: str\n    ) -> None:\n        \"\"\"Process questions related to a file, function, class, or method.\"\"\"\n        if question_type == \"file\":\n            query = question_text.format(filename=self.base_name)\n            info = self.file_details[\"file_info\"]\n            context = f\"```python\\n{self.file_details['file_info']['file_code']}\\n```\"\n            self.process_question(question_id, query, context, info)\n        elif question_type == \"method\":\n            for class_name, class_info in self.file_details[\"classes\"].items():\n                for key, method_info in class_info.items():\n                    if key.startswith(\"class_method_\"):\n                        method_name = f\"{class_name}.{key[len('class_method_'):]}\"\n                        context = f\"```python\\n{method_info['method_code']}\\n```\"\n                        mapping = {\"class_name\": class_name, \"method_name\": method_name}\n                        query = question_text.format(filename=self.base_name, **mapping)\n                        self.process_question(question_id, query, context, method_info)\n        else:  # function or class\n            for name, info in self.file_details[\n                self.question_mapping[question_type]\n            ].items():\n                context = f\"```python\\n{info[f'{question_type}_code']}\\n```\"\n                mapping = {f\"{question_type}_name\": name}\n                if question_id == f\"{question_type}_purpose\" and self.use_llm:\n                    variables = self.get_info_string(info, f\"{question_type}_variables\")\n                    inputs = self.get_info_string(info, f\"{question_type}_inputs\")\n                    combined = \", \".join(filter(None, [variables, inputs]))\n                    mapping[f\"{question_type}_variables\"] = get_unique_elements(\n                        combined\n                    )\n                    if question_type == \"class\":\n                        methods = self.get_info_string(info, \"class_methods\")\n                        mapping[\"class_methods\"] = methods\n                query = question_text.format(filename=self.base_name, **mapping)\n                self.process_question(question_id, query, context, info)\n\n    def generate(self) -> tuple[list[dict], list[dict]]:\n        \"\"\"Generate responses for all the questions and returns the instruct_list.\"\"\"\n        for question in self.questions:\n            self.process_question_type(\n                question[\"type\"], question[\"id\"], question[\"text\"]\n            )\n\n        self.instruct_list.sort(key=lambda x: len(x[\"input\"]), reverse=True)\n        return self.instruct_list\n\n\ndef get_python_datasets(\n    file_path: str,\n    file_details: dict,\n    base_name: str,\n    questions: list[dict],\n    model_config: dict,\n    detailed: bool,\n) -> tuple[list[dict], list[dict]]:\n    \"\"\"Extract information from a Python file and return it in JSON format.\"\"\"\n    return DatasetGenerator(\n        file_path, file_details, base_name, questions, model_config, detailed\n    ).generate()\n\n```",
        "code filename": "py2dataset/get_python_datasets.py"
    },
    {
        "document": "I) Purpose and Processing Approach for Python file 'py2dataset\\get_python_file_details.py':\nThis Python script aims to extract comprehensive details about a given Python source code file by analyzing its Abstract Syntax Tree (AST). It accomplishes this through various functions and a custom visitor class named CodeVisitor that traverses the AST to gather information on functions, classes, dependencies, constants, and generates a simplified version of the code without docstrings. The primary output is a structured dictionary containing all extracted data along with additional insights generated from the analysis process such as code graphs and control flow structures represented in PlantUML format.\n\nProcessing Steps:\n1. Import necessary modules like ast, logging, json, typing, get_code_graph (from an external file), and define functions remove_docstring(), get_all_calls(), CodeVisitor class, and get_python_file_details().\n2. remove_docstring() removes docstrings from the provided Python code tree by iterating through nodes and removing first expressions if they are string literals representing docstrings for functions or classes. It returns the modified AST as a string using ast.unparse().\n3. get_all_calls() performs a recursive depth-first traversal across subtree searching all occurrences of function calls found within Node instances like ast.Call, storing them in a dictionary format for further analysis.\n4. CodeVisitor is a custom class extending ast.NodeVisitor that keeps track of code details during AST traversal. It has attributes such as code (original input), tree (parsed AST), current_class (for tracking nested classes), file_info (collected data dictionary), functions, classes, and constants lists.\na. __init__ method initializes objects setting required values like self.code, self.tree for future processing and clearing class variables except current_class to a null state.\nb. visit_FunctionDef tracks function definition nodes and extracts their details into 'functions' dictionary within file_info.\nc. visit_ClassDef follows similar logic but populates 'classes' dictionary in file_info for class definitions and sets the current_class attribute for nested method tracking.\nd. visit_Assign gathers constant assignments either globally or as class attributes storing them into self.constants list.\ne. extract_details extracts relevant data about each AST node based on node type, updating respective keys within file_info dictionary like method_name/class_name/_inputs/_returns/_docstrings/_calls etc., depending upon the node type. It also populates class attributes and methods lists if applicable.\nf. analyze() traverses the entire tree using CodeVisitor's visit() method, collects dependencies by examining import statements, prepares file details into distinct dictionaries for functions ('function_defs') and classes ('class_defs'), generates a simplified code version without docstrings via remove_docstring(), creates an EntireCodeGraph using get_code_graph() function along with ControlFlowStructure and PlantUML representation. Finally, it converts file_summary dictionary to JSON format with quoted characters replaced by empty strings.\n5. get_python_file_details() receives a file path as input. It reads the Python file content using 'open' call and attempts error handling with logging warnings if needed. Parses code into an AST tree using ast.parse(). Then, it creates an instance of CodeVisitor passing required arguments and calls analyze() to generate final results stored in file_details dictionary before returning it along with additional data structures like entire_code_graph, control_flow_structure, and plant_uml from get_code_graph().\n\nII) Requirements, API Signatures, Logic for Functions & Class Methods:\na. remove_docstring(tree: Removes docstrings from provided Python code tree (AST). Input 'tree', Calls ast.walk(), isinstance(), node.body.pop(), ast.unparse(). Returns simplified AST string using ast.unparse().\nb. get_all_calls(node=[optional calls], Constructs dictionary for each call instance encountered while traversing AST containing function names with corresponding argument list for function invocations in a tree rooted at node. Input 'node', Optional parameter 'calls' to store results initially empty, Returns dictionary {func: args} mapping function names to arguments lists.\nc. CodeVisitor(__init__, Self=[instance], code=[input string], tree=[AST] Initialize object variables and prepare for traversal. No explicit return value but sets internal attributes.\ni. visit_FunctionDef(node): Updates 'functions' dictionary within file_info for function details extracted from node. Calls extract_details(). Visits nodes recursively using generic_visit().\nii. visit_ClassDef(node): Updates 'classes' dictionary within file_info for class definitions and sets current_class attribute for nested method tracking. Same as visit_FunctionDef but different key prefixes for class properties. Also calls generic_visit().\niii. visit_Assign(node): Gathers constants stored in self.constants list if assigned globally or attached to instances of ast.Attribute representing 'self'. Visits nodes recursively using generic_visit().\nd. extract_details(self, node=[AST], node_type=[str] Extracts relevant details about node based on its type (method/class), builds data into 'file_info' dictionary according to its category like input lists (_inputs/_returns/_docstrings/_calls) while extending attributes if related to class objects. Optional inputs affect the collected data format as needed. Returns an overall extracted dictionary.\ne. analyze(): Performs CodeVisitor tree traversal (visit()), creates summary dict from specific collections built in earlier processing stages. It constructs 'file_dependencies', 'function_defs', and 'class_defs' lists while handling class properties. Removes ast tree to generate JSON friendly output for file_summary replacing double quotes with empty strings using json.dumps().\nf. get_python_file_details(file_path=[input string] reads Python file content with open(), handles exceptions logging warnings if any, parses code into AST tree via ast.parse(), creates CodeVisitor instance 'visitor', analyzes data collecting EntireCodeGraph, ControlFlowStructure and PlantUML representation from get_code_graph(). Returns file_details dictionary along with additional outputs if successful else None on failure.\n\nIII) Inputs, Variables, Calls & Returns explanation across code:\na. Dependencies consist of ast, logging, json (imported), get_code_graph (from another script), typing; They enable tree parsing and manipulation, error handling, JSON serialization, data structuring, graph generation, etc.\nb. Functions have various inputs like tree, node, code, file_path, self, ast nodes or instances; Return values range from simplified AST strings to dictionaries containing comprehensive Python file details with additional derived insights.\nc. CodeVisitor class methods interact with instance attributes (self), tree, node objects, and collected data structures like functions, classes lists while calling other methods recursively. Internal variables include code, tree, current_class etc., used for tracking analysis progress. Calls span across ast modules' functionalities to extract details from AST nodes.\nd. remove_docstring() calls ast.walk(), isinstance(), node.body.pop(), ast.unparse(); Input 'tree', Returns simplified code string using ast.unparse().\ne. get_all_calls() uses ast.Call, ast.Assign, ast.Constant checks within nodes; Calls ast.unparse(), ast.iter_child_nodes(), next(), set(), list(), isinstance(). Input 'node', Optional parameter 'calls' to store results initially empty; Returns dictionary {func: args}.\nf. get_python_file_details() uses open(), file.read(), logging.warning(), ast.parse(); Sets internal attributes and creates visitor objects after file content handling, finally invoking analyze() passing generated results if successful. Calls JSON modification techniques with json to enhance final output format compatibility; Returns file_details or None for failures.\ng. In CodeVisitor.__init__, inputs self (instance), code, tree update corresponding attributes, clears non-required variables; No explicit return value but initializes internal objects for processing ahead.\nh. visit_FunctionDef() uses extract_details(), generic_visit(); Input 'node', Returns None after updating file_info dictionary entries.\ni. visit_ClassDef() calls extract_details(), generic_visit(); Input 'node', Similar to visit_FunctionDef but different key prefixes for class properties update. Returns None after processing class details in file_info dictionary.\nj. visit_Assign() invokes ast.unparse(), isinstance(), self.constants.append(), generic_visit(); Input 'node', Returns None after gathering constants assignments.\nk. extract_details() uses list(), get_all_calls(), set(), next(), call_data.keys(), len(), class_details.setdefault(), details.update(), any(); Inputs node_type, node_inputs, node_walk, details; Returns extracted dictionary 'details'.\nl. analyze() performs self.visit(), list(), isinstance(), set(), len(), class_details.items(), method_name.startswith(), self.functions.keys(), self.classes.keys(), remove_docstring(); Inputs node_walk, function_defs, class_defs, file_dependencies; Variables track progress across nested loops and functions while generating required outputs.\nm. get_python_file_details() involves open(), file.read(), logging.warning(), ast.parse(); Creates visitor object after error handling, analyze(); Returns file_details with additional structures if successful else None for errors.",
        "code": "```python\nimport ast\nimport logging\nimport json\nfrom typing import Dict, List, Union\nfrom get_code_graph import get_code_graph\n\n\ndef remove_docstring(tree: ast.AST) -> str:\n    \"\"\"Remove docstrings from the provided Python code.\"\"\"\n    for node in ast.walk(tree):\n        if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):\n            if (\n                node.body\n                and isinstance(node.body[0], ast.Expr)\n                and isinstance(node.body[0].value, ast.Str)\n            ):\n                node.body.pop(0)\n    return ast.unparse(tree)\n\n\ndef get_all_calls(node: ast.AST, calls: List[tuple] = None) -> Dict[str, List[str]]:\n    \"\"\"Recursively find all function calls in the subtree rooted at `node`.\"\"\"\n    if calls is None:\n        calls = []\n\n    if isinstance(node, ast.Call):\n        calls.append((ast.unparse(node.func), [ast.unparse(arg) for arg in node.args]))\n    elif isinstance(node, ast.ClassDef):\n        for body_item in node.body:\n            if isinstance(body_item, ast.Assign):\n                get_all_calls(body_item.value, calls)\n\n    for child in ast.iter_child_nodes(node):\n        get_all_calls(child, calls)\n\n    return {func: args for func, args in calls}\n\n\nclass CodeVisitor(ast.NodeVisitor):\n    \"\"\"Visitor class for traversing an AST and extracting details about the code.\"\"\"\n\n    def __init__(self, code: str, tree: ast.AST) -> None:\n        \"\"\"Initialize a new instance of the class.\"\"\"\n        self.code = code\n        self.tree = tree\n        self.functions: Dict[str, Dict[str, Union[str, List[str]]]] = {}\n        self.classes: Dict[str, Dict[str, Union[str, List[str]]]] = {}\n        self.file_info: Dict[str, Union[str, List[str]]] = {}\n        self.current_class: str = None\n        self.constants: List[str] = []\n\n    def visit_FunctionDef(self, node: ast.FunctionDef) -> None:\n        \"\"\"\n        Extract details about a function.\n        Args:\n            node (ast.FunctionDef): The function definition node.\n        \"\"\"\n        details = self.extract_details(\n            node, \"method\" if self.current_class else \"function\"\n        )\n        if self.current_class:\n            self.classes[self.current_class][f\"class_method_{node.name}\"] = details\n        else:\n            self.functions[node.name] = details\n        self.generic_visit(node)\n\n    def visit_ClassDef(self, node: ast.ClassDef) -> None:\n        \"\"\"Extract details about a class.\"\"\"\n        self.classes[node.name] = self.extract_details(node, \"class\")\n        self.current_class = node.name\n        self.generic_visit(node)\n        self.current_class = None\n\n    def visit_Assign(self, node: ast.Assign) -> None:\n        \"\"\"Get constants defined in the global scope or class attributes.\"\"\"\n        for target in node.targets:\n            if isinstance(target, ast.Name):\n                value = node.value\n                value_repr = (\n                    f\"'{value.value}'\"\n                    if isinstance(value, ast.Constant) and isinstance(value.value, str)\n                    else ast.unparse(value)\n                )\n                self.constants.append(f\"{target.id}={value_repr}\")\n        self.generic_visit(node)\n\n    def extract_details(\n        self, node: ast.AST, node_type: str\n    ) -> Dict[str, Union[str, List[str]]]:\n        \"\"\"Extract details about a node.\"\"\"\n        node_walk = list(ast.walk(node))\n        call_data = get_all_calls(node)\n        node_inputs = (\n            [arg.arg for arg in node.args.args]\n            if node_type in [\"function\", \"method\"]\n            else None\n        )\n        node_variables = [\n            ast.unparse(target)\n            for subnode in node_walk\n            if isinstance(subnode, ast.Assign)\n            for target in subnode.targets\n            if isinstance(target, ast.Name)\n        ]\n        if node_inputs:\n            node_variables = list(set(node_inputs + node_variables))\n\n        details = {\n            f\"{node_type}_name\": node.name,\n            f\"{node_type}_code\": ast.unparse(node),\n            f\"{node_type}_docstring\": next(\n                (\n                    n.value.s\n                    for n in node_walk\n                    if isinstance(n, ast.Expr) and isinstance(n.value, ast.Str)\n                ),\n                None,\n            ),\n            f\"{node_type}_inputs\": node_inputs,\n            f\"{node_type}_defaults\": [ast.unparse(d) for d in node.args.defaults]\n            if node_type in [\"function\", \"method\"]\n            else None,\n            f\"{node_type}_returns\": [\n                ast.unparse(subnode.value) if subnode.value is not None else \"None\"\n                for subnode in node_walk\n                if isinstance(subnode, ast.Return)\n            ],\n            f\"{node_type}_calls\": list(call_data.keys()),\n            f\"{node_type}_call_inputs\": call_data,\n            f\"{node_type}_variables\": node_variables,\n            f\"{node_type}_decorators\": [\n                ast.unparse(decorator) for decorator in node.decorator_list\n            ]\n            if node.decorator_list\n            else [],\n            f\"{node_type}_annotations\": [\n                ast.unparse(subnode.annotation)\n                for subnode in node_walk\n                if isinstance(subnode, ast.AnnAssign) and subnode.annotation is not None\n            ],\n            f\"{node_type}_properties\": [\n                ast.unparse(subnode)\n                for subnode in node_walk\n                if isinstance(subnode, ast.Attribute)\n                and isinstance(subnode.ctx, ast.Store)\n            ],\n        }\n\n        if node_type in [\"class\", \"method\"]:\n            if node_type == \"method\" and self.current_class:\n                attributes = [\n                    target.attr\n                    for subnode in node_walk\n                    if isinstance(subnode, ast.Assign)\n                    for target in subnode.targets\n                    if isinstance(target, ast.Attribute)\n                    and isinstance(target.value, ast.Name)\n                    and target.value.id == \"self\"\n                ]\n                self.classes[self.current_class].setdefault(\n                    \"class_attributes\", []\n                ).extend(attributes)\n            if node_type == \"class\":\n                details.update(\n                    {\n                        \"class_attributes\": [\n                            target.attr\n                            for subnode in node.body\n                            if isinstance(subnode, ast.Assign)\n                            for target in subnode.targets\n                            if isinstance(target, ast.Attribute)\n                        ],\n                        \"class_methods\": [\n                            subnode.name\n                            for subnode in node.body\n                            if isinstance(subnode, ast.FunctionDef)\n                        ],\n                        \"class_inheritance\": [ast.unparse(base) for base in node.bases]\n                        if node.bases\n                        else [],\n                        \"class_static_methods\": [\n                            subnode.name\n                            for subnode in node.body\n                            if isinstance(subnode, ast.FunctionDef)\n                            and any(\n                                isinstance(decorator, ast.Name)\n                                and decorator.id == \"staticmethod\"\n                                for decorator in subnode.decorator_list\n                            )\n                        ],\n                    }\n                )\n\n        return details\n\n    def analyze(self) -> None:\n        \"\"\"Traverse the AST and populate 'file_info' with details about the file.\"\"\"\n        self.visit(self.tree)\n        node_walk = list(ast.walk(self.tree))\n\n        file_dependencies = {\n            alias.name\n            for subnode in node_walk\n            if isinstance(subnode, ast.Import)\n            for alias in subnode.names\n        } | {\n            subnode.module\n            for subnode in node_walk\n            if isinstance(subnode, ast.ImportFrom)\n        }\n\n        function_defs = [\n            {\n                func_name: {\n                    \"inputs\": details[\"function_inputs\"],\n                    \"calls\": details[\"function_calls\"],\n                    \"call_inputs\": details[\"function_call_inputs\"],\n                    \"returns\": details[\"function_returns\"],\n                }\n            }\n            for func_name, details in self.functions.items()\n        ]\n\n        class_defs = [\n            {\n                class_name: {\n                    \"method_defs\": {\n                        method_name[len(\"class_method_\") :]: {\n                            \"inputs\": details[\"method_inputs\"],\n                            \"calls\": details[\"method_calls\"],\n                            \"call_inputs\": details[\"method_call_inputs\"],\n                            \"returns\": details[\"method_returns\"],\n                        }\n                        for method_name, details in class_details.items()\n                        if method_name.startswith(\"class_method_\")\n                    }\n                }\n            }\n            for class_name, class_details in self.classes.items()\n        ]\n\n        self.file_info = {\n            \"file_code\": self.code,\n            \"file_ast\": self.tree,\n            \"file_dependencies\": list(file_dependencies),\n            \"file_functions\": list(self.functions.keys()),\n            \"file_classes\": list(self.classes.keys()),\n            \"file_constants\": self.constants,\n            \"file_summary\": {\n                \"dependencies\": list(file_dependencies),\n                \"function_defs\": function_defs,\n                \"class_defs\": class_defs,\n            },\n            \"file_code_simplified\": remove_docstring(self.tree),\n        }\n\n\ndef get_python_file_details(file_path: str) -> Dict[str, Union[Dict, str]]:\n    \"\"\"Extract details from a Python file.\"\"\"\n    try:\n        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n            code = file.read()\n    except (PermissionError, IOError) as e:\n        logging.warning(f\"{e} error in file: {file_path}\")\n        return None\n\n    try:\n        tree = ast.parse(code)\n    except SyntaxError as e:\n        logging.warning(f\"{e} error in file: {file_path}\")\n        return None\n\n    visitor = CodeVisitor(code, tree)\n    visitor.analyze()\n    file_details = {\n        \"file_info\": visitor.file_info,\n        \"functions\": visitor.functions,\n        \"classes\": visitor.classes,\n    }\n\n    file_summary = file_details[\"file_info\"][\"file_summary\"]\n    file_ast = file_details[\"file_info\"][\"file_ast\"]\n    entire_code_graph, control_flow_structure, plant_uml = get_code_graph(file_summary, file_ast)\n    file_details[\"file_info\"][\"entire_code_graph\"] = entire_code_graph\n    file_details[\"file_info\"][\"control_flow_structure\"] = control_flow_structure\n    file_details[\"file_info\"][\"plant_uml\"] = plant_uml\n    file_details[\"file_info\"][\"file_summary\"] = json.dumps(file_summary).replace(\n        '\"', \"\"\n    )\n    del file_details[\"file_info\"][\"file_ast\"]\n\n    return file_details\n\n```",
        "code filename": "py2dataset/get_python_file_details.py"
    },
    {
        "document": "I) Purpose and Processing Approach for Python file `py2dataset/py2dataset.py`:\nThis Python script named 'py2dataset.py' aims to generate datasets by processing Python files within a specified directory or GitHub repository and producing question-answer pairs related to code purposes. It utilizes various functionalities such as extracting file details, handling model configurations with LLMs (Language Models), logging messages for progress tracking, and saving output data in JSON format. The user can interactively set parameters through command line arguments or provide default values. If a GitHub repository URL is given as the starting directory, it clones or updates the repository locally before processing files within it.\n\nII) Detailed Requirements, API Signatures, and Logic for all Functions & Class Methods:\n1. `process_single_python_file(params: Dict) -> None`:\n- Purpose: Processes a single Python file to generate question-answer pairs and instructions related to its functionality.\n- Input: A dictionary 'params' containing necessary details about the Python file path ('python_pathname'), model configuration ('model_config'), questions file ('questions'), relative path within output directory ('relative_path'), detailed analysis flag ('detailed').\n- Internal Calls & Operations:\n- Logging messages related to file processing status.\n- Retrieves file details using `get_python_file_details()`. If failure, logs error and returns without proceeding further.\n- Acquires instruct_data dataset via `get_python_datasets()` function call. If no data retrieved, logs error regarding it.\n- Uses `save_python_data()` to save file details and dataset. Elsewhere an error logged when generating the dataset fails.\n- Process Outcome: None type, performing necessary actions for one Python file.\n\n2. `py2dataset(start: str=\"\", output_dir: str=\"\", questions_pathname: str=\"\", model_config_pathname: str=\"\", use_llm: bool=False, quiet: bool=False, single_process: bool=False, detailed: bool=False, html: bool=False, skip_regen: bool=False) -> Dict[str, List[Dict]]:`\n- Purpose: Generates datasets by processing Python files within a specified directory or GitHub repository.\n- Inputs: Multiple arguments such as starting directory path ('start'), output directory path for saved results ('output_dir'), questions file name ('questions_pathname'), model configuration file path ('model_config_pathname'), usage of LLMs for code analysis ('use_llm'), limit logging level ('quiet'), utilizing one process ('single_process'), inclusion of detailed data analysis ('detailed'), generating HTML output ('html') or skipping existing '.instruct.json' files regeneration ('skip_regen').\n- Internal Calls & Operations:\n- Sets recursion limit higher for better AST parsing performance with `sys.setrecursionlimit()`.\n- Acquires model configuration through `get_model()` function if 'use_llm' and single process enabled. Else processes files sequentially without invoking multiple instances of `process_single_python_file()`.\n- Iterates over Python files in the given directory using `Path().rglob()`, excluding certain directories ('exclude_dirs'). For each file, sets parameters required by `process_single_python_file()` and invokes it either as a separate process or sequentially based on LLM usage.\n- Combines output JSON files with `combine_json_files()`.\n- Output: A dictionary containing generated datasets of Python files' question-answer pairs.\n\n3. `clone_github_repo(url: str) -> str:`\n- Purpose: Clones a GitHub repository or fetches latest changes and returns its local path.\n- Input: URL string for the repository.\n- Internal Calls & Operations:\n- Extracts repository name from URL with `Path().stem`.\n- Checks if repository exists locally ('repo_path.exists()') or clones it using `Repo.clone_from()` if provided URL starts with 'https://github.com/'. Otherwise, verifies directory existence ('os.path.isdir()') and uses current working directory as the base path ('os.getcwd()').\n- Logs errors encountered during repository processing with `logging`.\n- Output: Local repository path as a string if successful; empty string otherwise.\n\n4. `get_bool_from_input(input_str: str, current_value: bool) -> bool:`\n- Purpose: Returns boolean value based on user input interactively or keeping original parameter ('current_value').\n- Inputs: String user input regarding specific parameter with existing setting ('input_str') and initial Boolean ('current_value').\n- Operations & Returns: Parses 'input_str', checking whether its case matches keywords (lower()) like ['True', 'YES', or 'yes' returning True or corresponding False equivalents like 'false', 'No'.\n\n5. `main()`:\n- Purpose: Command-line entry point for processing Python files and generating datasets with interactive user input options.\n- Internal Calls & Operations:\n- Checks if help requested ('len(sys.argv) == 1 or '-h'/'--help') and prints usage instructions before exiting. Else proceeds further.\n- Converts argument string ('arg_string') to parameter key-value pairs with `params.items()`. Sets Boolean parameters based on matched prefixes like ['--start', '-X'] or invokes `get_bool_from_input()` for interactive inputs marked by '--I'.\n- If in interactive mode ('params['I']), prompts user input for each parameter, assigning new values or keeping defaults. Logs updated parameters with print().\n- Determines starting directory as cloned GitHub repository if URL given else local working directory or original path if it's a valid folder path.\n- Invokes `py2dataset()` using the finalized parameter settings ('params').\n\nIII) Purpose of inputs, variables, calls & returns in code:\n- Variables declare and hold diverse information needed by functions (e.g., output directories, model configuration paths, query datasets): 'params', 'sys', 'logging', 'start', 'questions_pathname', 'model_config_pathname', 'exclude_dirs'.\n- Calls to external libraries and custom modules advance functionality (e.g., 'Pathlib' for path manipulation, 'multiprocessing' for parallel processing): `Path`, `Repo`, `logging`, `typing`.\n- Returns facilitate program output or data retrieval (e.g., 'combine_json_files()' aggregates individual '.instruct.json' files as datasets).",
        "code": "```python\nimport os\nimport sys\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, List\nfrom multiprocessing import Process\nfrom git import Repo, GitCommandError\n\nfrom get_python_file_details import get_python_file_details\nfrom get_python_datasets import get_python_datasets\nfrom get_params import get_questions, get_model, get_output_dir, get_start_dir\nfrom save_output import combine_json_files, save_python_data\n\n\ndef process_single_python_file(params: Dict) -> None:\n    \"\"\"Processes a single Python file to generate question-answer pairs and instructions.\"\"\"\n\n    logging.info(f\"Processing file: {params['python_pathname']}\")\n    file_details = get_python_file_details(params[\"python_pathname\"])\n    if not file_details:\n        logging.error(f\"Failed to get file details for {params['python_pathname']}\")\n        return\n\n    if params[\"model_config\"] is None and params[\"use_llm\"]:\n        params[\"model_config\"] = get_model(params[\"model_config_pathname\"])\n\n    instruct_data = get_python_datasets(\n        params[\"python_pathname\"],\n        file_details,\n        params[\"relative_path\"],\n        params[\"questions\"],\n        params[\"model_config\"],\n        params[\"detailed\"],\n    )\n\n    if instruct_data:\n        save_python_data(\n            file_details, instruct_data, params[\"relative_path\"], params[\"output_dir\"]\n        )\n    else:\n        logging.error(f\"Failed getting {params['python_pathname']} dataset\")\n\n\ndef py2dataset(\n    start: str = \"\",\n    output_dir: str = \"\",\n    questions_pathname: str = \"\",\n    model_config_pathname: str = \"\",\n    use_llm: bool = False,\n    quiet: bool = False,\n    single_process: bool = False,\n    detailed: bool = False,\n    html: bool = False,\n    skip_regen: bool = False,\n) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Generates datasets by processing Python files within a specified directory.\n    Args:\n        start (str, optional): Starting directory for Python files or GitHub repository Python files. Default: current working directory.\n        output_dir (str, optional): Directory to write the output files. Default: ./dataset/.\n        questions_pathname (str, optional): Path and filename of the questions file. Default: ./py2dataset_questions.json.\n        model_config_pathname (str, optional): Path and filename of the model configuration file. Default: ./py2dataset_model_config.yaml.\n        use_llm (bool, optional): Use llm to answer code purpose question. Default: False.\n        quiet (bool, optional): Limit logging output. Default: False.\n        single_process (bool, optional): Use a single process to process Python files if --use_llm. Default: False.\n        detailed (bool, optional): Include detailed analysis. Default: False.\n        html (bool, optional): Generate HTML output. Default: False.\n        skip_regen (bool, optional): Skip regeneration of existing instruct.json files. Default: False.\n    Returns:\n        Dict[str, List[Dict]]: Generated datasets.\n    \"\"\"\n    sys.setrecursionlimit(3000)  # Set recursion limit higher for AST parsing\n    model_config = (\n        get_model(model_config_pathname) if use_llm and single_process else None\n    )\n\n    logging.getLogger().setLevel(logging.WARNING if quiet else logging.INFO)\n\n    params = {\n        \"output_dir\": get_output_dir(output_dir),\n        \"model_config_pathname\": model_config_pathname,\n        \"questions\": get_questions(questions_pathname),\n        \"use_llm\": use_llm,\n        \"model_config\": model_config,\n        \"detailed\": detailed,\n    }\n\n    exclude_dirs = [\"env\", \"venv\", \"__pycache__\", \"build\", \"dist\"]\n\n    for python_pathname in Path(start).rglob(\"*.py\"):\n        if (\n            any(dir in python_pathname.parts for dir in exclude_dirs)\n            or python_pathname.name == \"__init__.py\"\n        ):\n            continue\n\n        params[\"python_pathname\"] = str(python_pathname)\n        params[\"relative_path\"] = Path(\n            os.path.relpath(python_pathname, os.path.dirname(get_start_dir(start)))\n        )\n        instruct_pathname = Path(params[\"output_dir\"]) / params[\n            \"relative_path\"\n        ].with_suffix(\".py.instruct.json\")\n\n        if instruct_pathname.exists() and skip_regen:\n            continue\n\n        if model_config is None and use_llm:\n            # Process each Python file in a separate process to manage memory\n            process = Process(target=process_single_python_file, args=(params,))\n            process.start()\n            process.join()\n            process.close()\n        else:\n            # Process all files using a single process\n            process_single_python_file(params)\n\n    return combine_json_files(output_dir, html, params[\"questions\"])\n\n\ndef clone_github_repo(url: str) -> str:\n    \"\"\"Clone repository or fetch the latest changes and return local repository path.\"\"\"\n    try:\n        repo_name = Path(url).stem\n        repo_path = Path.cwd() / \"githubrepos\" / repo_name\n        if repo_path.exists():\n            repo = Repo(repo_path)\n            repo.remote().fetch()\n            repo.git.reset(\"--hard\", repo.heads[0].commit)\n        else:\n            Repo.clone_from(url, repo_path)\n        return str(repo_path)\n    except GitCommandError as e:\n        logging.info(f\"Error processing repository {url}: {e}\")\n        return \"\"\n\n\ndef get_bool_from_input(input_str: str, current_value: bool) -> bool:\n    \"\"\"Return boolean value based on the user input.\"\"\"\n    if input_str.lower() in [\"t\", \"true\", \"y\", \"yes\"]:\n        return True\n    elif input_str.lower() in [\"f\", \"false\", \"n\", \"no\"]:\n        return False\n    return current_value\n\n\ndef main():\n    \"\"\"\n    Command-line entry point for processing Python files and generating datasets.\n    Usage:\n        python py2dataset.py [options]\n    Options:\n        -h, --help: Show this help message and exit.\n        --start: Starting directory for Python files or GitHub repository Python files. Default: current working directory.\n        --output_dir: Directory to write the output files. Default: ./dataset/.\n        --questions_pathname: Path and filename of the questions file. Default: ./py2dataset_questions.json.\n        --model_config_pathname: Path and filename of the model configuration file. Default: ./py2dataset_model_config.yaml.\n        --use_llm: Use llm to answer code purpose question. Default: False.\n        --quiet: Limit logging output. Default: False.\n        --single_process: Use a single process to process Python files if --use_llm. Default: False.\n        --detailed: Include detailed analysis. Default: False.\n        --html: Generate HTML output. Default: False.\n        --skip_regen: Skip regeneration of existing instruct.json files. Default: False.\n        --I: Interactive mode to enter new values.\n    \"\"\"\n    if len(sys.argv) == 1 or \"-h\" in sys.argv or \"--help\" in sys.argv:\n        print(__doc__)\n        sys.exit()\n\n    params = {\n        \"start\": \".\",\n        \"output_dir\": \"./dataset/\",\n        \"questions_pathname\": \"./py2dataset_questions.json\",\n        \"model_config_pathname\": \"./py2dataset_model_config.yaml\",\n        \"use_llm\": False,\n        \"quiet\": False,\n        \"single_process\": False,\n        \"detailed\": False,\n        \"html\": False,\n        \"skip_regen\": False,\n        \"I\": False,\n    }\n\n    arg_string = \" \".join(sys.argv[1:])\n    for arg, value in params.items():\n        if f\"--{arg}\" in arg_string:\n            if isinstance(value, bool):\n                params[arg] = True\n                arg_string = arg_string.replace(f\"--{arg}\", \"\")\n            else:\n                value_segment = arg_string.split(f\"--{arg} \")[1]\n                params[arg] = value_segment.split(\" --\")[0].strip('\"')\n                arg_string = arg_string.replace(f\"--{arg} {params[arg]}\", \"\")\n\n    if params.pop(\"I\"):\n        print(\"Interactive mode, enter new values or press enter to keep.\")\n        for arg, value in params.items():\n            user_input = input(f\"{arg} [{value}]: \").strip()\n            params[arg] = (\n                get_bool_from_input(user_input, value)\n                if isinstance(value, bool)\n                else user_input or value\n            )\n            print(f\"{arg}: {params[arg]}\")\n\n    params[\"start\"] = (\n        clone_github_repo(params[\"start\"])\n        if params[\"start\"].startswith(\"https://github.com/\")\n        else params[\"start\"]\n        if os.path.isdir(params[\"start\"])\n        else os.getcwd()\n    )\n\n    py2dataset(**params)\n\n\nif __name__ == \"__main__\":\n    main()\n\n```",
        "code filename": "py2dataset/py2dataset.py"
    },
    {
        "document": "I) The given Python file `save_output.py` primarily serves as a comprehensive documentation generator and organizer for managing various data formats related to a dataset processing workflow. It reads JSON or YAML files from a specified directory, converts them into HTML format if desired, combines the content into specific target files while generating Python code graphs as images, writes instructional data in JSON format, and saves Python file details in YAML format. Its purpose can be broken down into several key functionalities provided by different methods:\n\n1. `read_file` - Loads either JSON or YAML content from a given path into a dictionary format. It identifies the file type based on its extension and calls `json.load()` or `yaml.load()`.\n2. `write_file` - Writes a provided dictionary data into JSON or YAML files by determining the file type and using appropriate methods like `json.dump()` or `yaml.Dumper`.\n3. `convert_json_to_html` - Transforms JSON files within a directory to HTML format while preserving whitespaces in values, creates HTML tables representing the dataset structure with headers corresponding to keys and values from each JSON entry.\n4. `combine_json_files` - Combines multiple JSON files into \"instruct.json,\" generates \"sharegpt.json,\" \"document_code.json,\" and \"code_details.yaml\" files after reading them, optionally converts JSON files to HTML format if instructed. It also creates a purpose question filter for identifying relevant data related to Python code generation instructions.\n5. `create_code_graph` - Generates graphs representing the code dependencies using NetworkX library and saves them as PNG images in an output directory.\n6. `save_python_data` - Saves Python file details into YAML format, instruction data as JSON files, creates \"code_details.yaml,\" generates code graphs for each file while handling exceptions if any occur during graph creation.\n\nII) API Signatures:\na. read_file(file_path: Path) -> Dict: Reads JSON or YAML content from specified path and returns dictionary data.\n- Dependencies: json, logging, yaml, pathlib\nb. write_file(data: Dict, file_path: Path) -> None: Writes dictionary data into JSON or YAML files based on extension.\n- Dependencies: json, yaml\nc. convert_json_to_html(directory: None) -> None: Converts JSON files in directory to HTML format maintaining whitespaces while creating HTML tables from their contents.\n- Dependencies: html, pathlib\nd. combine_json_files(directory: str, html: bool, questions: Dict) -> Dict[str, List[Dict]: Combines JSON files into specific target formats and generates \"document_code.json,\" \"sharegpt.json,\" \"code_details.yaml.\" Optionally converts JSON files to HTML if `html` is True.\n- Dependencies: json, logging, pathlib, yaml\ne. create_code_graph(file_details: dict, base_name: str, output_subdir: Path) -> None: Generates code dependency graphs as PNG images in an output directory using NetworkX and Matplotlib libraries.\n- Dependencies: networkx, matplotlib.pyplot, logging\nf. save_python_data(file_details: dict, instruct_list: List, relative_path: Path, output_dir: str) -> None: Stores Python file data into \"file_info.yaml,\" saves instruction data as JSON files and creates code graphs while handling exceptions.\n- Dependencies: logging, pathlib, yaml\n\nIII) Inputs, Variables, Calls & Returns Analysis:\na. read_file():\n- file_path input is a Path object representing the file location. It returns JSON or YAML content as a dictionary based on extension.\n- Uses `file_type`, `f` (file handle), `json.load()`, `yaml.load()`.\nb. write_file():\n- `data` is a dictionary to be written into file and `file_path` represents its location. It returns None after writing the data.\n- Uses `file_type`, `f` (file handle), `json.dump()`, `yaml.SafeDumper`.\nc. convert_json_to_html():\n- Directory input as string path to process JSON files conversion into HTML format. It returns None after writing HTML content in files.\n- Uses `json_file` (Path object), `preserve_spacing()`, `html_content`, `row_parts`, `html_rows`.\nd. combine_json_files():\n- Directory input as string path, boolean flag for HTML conversion named 'html', a Dict named `questions` to contain human instruction IDs and values. Returns {'instruct_list': combined data of JSON files} dictionary containing merged information.\n- Uses `logging`, `Path()`, `read_file()`, `json_file` (Path object), `purpose_question`, `code_filename`, `write_file()`.\ne. create_code_graph():\n- Takes file details dictionary and output directory path as strings, saves code graph images within the directory as PNGs. Returns None after processing graphs.\n- Uses `output_file` (Path object), `nx`, `plt`, `logging`, `G`, `edge_labels`.\nf. save_python_data():\n- file details dictionary, instruction list data for JSON saving, relative path as Path object representing the input file location, output directory string path. Returns None after processing Python file details and graphs handling exceptions if any occur.\n- Uses `Path()`, `write_file()`, `create_code_graph()`, `logging`.",
        "code": "```python\nimport json\nimport logging\nfrom html import escape\nfrom pathlib import Path\nfrom typing import Dict, List\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport yaml\n\n\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"Read a JSON or YAML file and return its contents as a dictionary.\"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == \"json\":\n            return json.load(f)\n        if file_type == \"yaml\":\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}\n\n\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"Write a dictionary to a JSON or YAML file.\"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open(\"w\", encoding=\"utf-8\") as f:\n        if file_type == \"json\":\n            json.dump(data, f, indent=4)\n        elif file_type == \"yaml\":\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(\n                data,\n                f,\n                Dumper=yaml.SafeDumper,\n                width=float(\"inf\"),\n                sort_keys=False,\n                default_flow_style=False,\n                indent=4,\n                allow_unicode=True,\n                encoding=\"utf-8\",\n            )\n\n\ndef convert_json_to_html(directory: str) -> None:\n    \"\"\"Convert JSON files within given directory to HTML format and save it.\"\"\"\n\n    def preserve_spacing(text: str, tab_width: int = 4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(\" \", \"&nbsp;\").replace(\"\\t\", \"&nbsp;\" * tab_width)\n\n    for json_file in Path(directory).rglob(\"*.json\"):\n        try:\n            dataset = read_file(json_file)\n            if not dataset:\n                continue\n        except Exception:\n            continue\n\n        html_content = \"\"\"\n        <html>\n        <head>\n            <style>\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\n                th, td {\n                    border: 1px solid black;\n                    padding: 8px;\n                    text-align: left;\n                    white-space: pre-line;\n                    vertical-align: top;\n                    word-wrap: break-word;\n                }\n            </style>\n        </head>\n        <body>\n            <table>\n                <thead>\n                    <tr>\n        \"\"\"\n        column_count = len(dataset[0].keys())\n        column_width = round(100 / column_count, 2)\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += \"\"\"\n                    </tr>\n                </thead>\n                <tbody>\n        \"\"\"\n        html_rows = []\n        for entry in dataset:\n            row_parts = [\"<tr>\"]\n            for key in entry:\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace(\"\\n\", \"<br/>\")\n                row_parts.append(f\"<td>{value}</td>\")\n            row_parts.append(\"</tr>\")\n            html_rows.append(\"\".join(row_parts))\n        html_content += \"\".join(html_rows)\n\n        html_content += \"\"\"\n                </tbody>\n            </table>\n        </body>\n        </html>\n        \"\"\"\n        html_file_path = json_file.with_suffix(\".html\")\n        try:\n            with open(html_file_path, \"w\", encoding=\"utf-8\") as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f\"Failed saving: {html_file_path}\")\n\n\ndef combine_json_files(\n    directory: str, html: bool, questions: Dict\n) -> Dict[str, List[Dict]]:\n    \"\"\"Create instruct, training, sharegpt, instruct, document_code json and code_details yaml files.\"\"\"\n    logging.info(f\"Combining JSON files in {directory}\")\n\n    # Save instruct.json file\n    combined_data, code_filename = [], []\n    skip_files = {\"instruct.json\", \"sharegpt.json\", \"document_code.json\"}\n    for json_file in Path(directory).rglob(\"*.json\"):\n        if json_file.name in skip_files:\n            continue\n        try:\n            file_data = read_file(json_file)\n            if file_data:\n                combined_data.extend(file_data)\n                cleaned_name = (\n                    json_file.relative_to(directory)\n                    .with_suffix(\"\")\n                    .as_posix()\n                    .replace(\".instruct\", \"\")\n                )\n                code_filename.append(cleaned_name)\n        except Exception as e:\n            logging.info(f\"Failed reading: {json_file}. Error: {e}\")\n    write_file(combined_data, Path(directory) / \"instruct.json\")\n\n    # Generate document_code.json file\n    logging.info(f\"Generating document_code.json file in {directory}\")\n    purpose_question = [\n        item[\"text\"] for item in questions if item[\"id\"] == \"file_purpose\"\n    ][0]\n    purpose_question = purpose_question.split(\"{filename}\")[0]\n    purpose_data = [\n        item\n        for item in combined_data\n        if item[\"instruction\"].startswith(purpose_question)\n    ]\n    document_code = [\n        {\n            \"document\": item[\"output\"],\n            \"code\": item[\"input\"],\n        }\n        for item in purpose_data\n    ]\n    for i, item in enumerate(document_code):\n        item[\"code filename\"] = code_filename[i]\n    write_file(document_code, Path(directory) / \"document_code.json\")\n\n    # save sharegpt.json file\n    logging.info(f\"Generating sharegpt.json file in {directory}\")\n    system_value = \"Use the provided documentation to output the corresponding Python code.\"\n\n    sharegpt = [\n        {\n            \"conversation\": [\n                {\n                    \"from\": \"system\",\n                    \"value\": system_value,\n                },\n                {\n                    \"from\": \"human\",\n                    \"value\": f\"Create Python code based on this documentation: {item['document']}\",\n                },\n                {\"from\": \"gpt\", \"value\": item[\"code\"]},\n            ],\n            \"nbytes\": \"0\",\n            \"source\": item[\"code filename\"],\n        }\n        for item in document_code\n    ]\n    for item in sharegpt:\n        nbytes = 0\n        for conv in item[\"conversation\"]:\n            nbytes += len(conv[\"value\"].encode(\"utf-8\"))\n        item[\"nbytes\"] = nbytes\n    write_file(sharegpt, Path(directory) / \"sharegpt.json\")\n\n    # save code_details.yaml file\n    code_details = []\n    logging.info(f\"Combining *.code_details.yaml files in {directory}\")\n    for yaml_file in Path(directory).rglob(\"*.code_details.yaml\"):\n        try:\n            with open(yaml_file, \"r\") as f:\n                file_data = f.read()\n                code_details.append(file_data)\n        except Exception as e:\n            logging.info(f\"Failed reading: {yaml_file}. Error: {e}\")\n    with open(Path(directory) / \"code_details.yaml\", \"w\") as f:\n        f.write(\"\\n\".join(code_details))\n\n    if html:\n        logging.info(\"Converting JSON files to HTML\")\n        convert_json_to_html(directory)\n\n    return {\"instruct_list\": combined_data}\n\n\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"Generate graphs from the file_details and save them as PNG images.\"\"\"\n    graph_type = \"entire_code_graph\"\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details[\"file_info\"][graph_type][\"nodes\"])\n    for edge in file_details[\"file_info\"][graph_type][\"edges\"]:\n        source, target = edge[\"source\"], edge[\"target\"]\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(\n                source,\n                target,\n                **{\n                    k: v\n                    for k, v in edge.items()\n                    if k in [\"target_inputs\", \"target_returns\"]\n                },\n            )\n\n    # draw graph\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(\n        G,\n        pos,\n        with_labels=True,\n        font_weight=\"bold\",\n        font_size=8,\n        node_shape=\"s\",\n        node_size=500,\n        width=1,\n        arrowsize=12,\n    )\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if \"target_inputs\" in edge[2] and edge[2][\"target_inputs\"]:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if \"target_returns\" in edge[2] and edge[2][\"target_returns\"]:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[(edge[0], edge[1])] = \"\\n\".join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n\n    try:  # save graph\n        output_file = output_subdir / f\"{base_name}.{graph_type}.png\"\n        plt.savefig(output_file)\n        plt.close()\n    except Exception as e:\n        logging.error(f\"Error saving graph for {base_name}: {e}\", exc_info=True)\n\n\ndef save_python_data(\n    file_details: dict, instruct_list: list, relative_path: Path, output_dir: str\n) -> None:\n    \"\"\"Save Python file details as a YAML file, the instruction data as JSON files,\n    and generate and save code graphs.\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parent\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    base_name = relative_path.name\n    write_file(instruct_list, output_subdir / f\"{base_name}.instruct.json\")\n    write_file(file_details, output_subdir / f\"{base_name}.details.yaml\")\n    output_text = file_details[\"file_info\"][\"code_qa_response\"]\n    with open(output_subdir / f\"{base_name}.code_details.yaml\", \"w\") as f:\n        f.write(output_text)\n\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except Exception as e:\n        logging.error(f\"Error creating graph for {base_name}: {e}\", exc_info=True)\n\n```",
        "code filename": "py2dataset/save_output.py"
    },
    {
        "document": "I) Purpose and Processing Approach for Python file `py2dataset\\setup.py`\n\nThe given code snippet is a part of a Setup Script written using Python language primarily focused on managing software package distribution through Setuptools library. Its main purpose is to configure metadata about the 'py2dataset' Python project for packaging and installation purposes. This script will be executed during the process of creating a distributable package using tools like `python setup.py sdist`, `python setup.py bdist_wheel`, or `pip install .`. It provides essential information such as name, version, author details, description, license type, dependencies, entry points, and more to facilitate easy installation and usage of the 'py2dataset' package by other developers or users.\n\nII) Detailed Requirements, API Signatures, and Logic for all Functions and Class Methods (None available in given context)\n- In the provided code fragment, we don't find any explicit functions or class methods being defined. Instead, it utilizes several predefined Setuptools arguments to configure package properties. These arguments act as configuration options rather than standalone functions with input-output logic. However, below are their key points in regard:\na. 'name': Takes value as 'py2dataset' identifying this particular package within installed modules catalog by Python or when specified using import command `from packaging_module_path`. It specifies what exactly gets deployed for system installation with an unambiguous identifier.\nb. 'version': Set to '0.5', indicating the current version of the 'py2dataset' software package. This helps users understand its compatibility level and assists in managing dependencies.\nc. 'author' & 'author_email': Provides authorship credentials where Jeff Meloy as contact can be reached. \nd. 'description': An essential documentation offering high-level comprehension about what this project (A tool to generate structured datasets from Python source code). \ne. 'long_description' along with `long_description_content_type`: Contains README.md file content which delivers more elaborate information regarding the package functionality and usage details. This helps users understand the package in depth before installing it. Here, Setuptools handles reading this text from the mentioned file using 'open()' function.\nf. 'url': A link to GitHub repository ('https://github.com/jeffmeloy/py2dataset') where the project source code is hosted for reference or contribution purposes.\ng. 'py_modules': Lists Python modules included in this package - ['get_code_graph', 'get_params', 'get_python_datasets', 'get_python_file_details', 'py2dataset', 'save_output']. These are likely functions or classes within the py2dataset package performing specific tasks related to dataset generation from Python source code.\nh. 'classifiers': Specifies technical metadata about the software like its programming language ('Python :: 3'), license type ('MIT License') and operating system independence ('OS Independent').\ni. 'python_requires': Mandates a minimum Python version ('>=3.8') required to run this package, ensuring compatibility with modern versions of Python interpreter.\nj. 'install_requires': A list containing dependencies required by py2dataset to function correctly ['matplotlib', 'networkx', 'ctransformers', 'PyYaml', 'GitPython']. These libraries need to be installed beforehand for successful execution of the package.\nk. 'entry_points': Maps command line script 'py2dataset' to its corresponding module function/class ('main') within the package, enabling users to invoke it directly from terminal using `py2dataset` command after installation.\nl. 'packages': Specifies the root directory of py2dataset package as '.\\\\py2dataset'. This helps Setuptools locate package files during distribution packaging.\n\nIII) Inputs, Variables, Calls & Returns Analysis (None available in given context related to functions)\n- The code fragment doesn't involve any user-defined function calls or return statements but relies on predefined Setuptools arguments for configuration purposes as discussed earlier. Consequently, inputs are limited to constant values provided during declaration while variables such as 'name', 'version', 'long_description', etc., act as holders of configured information after the script runs successfully. However, actual variable usages are handled internally by Setuptools module and aren't directly returned anywhere in this context. Instead, their impact reflects upon the final generated distribution package with associated metadata for smooth installation and utilization by end-users.",
        "code": "```python\nfrom setuptools import setup\n\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\n\nsetup(\n    name=\"py2dataset\",\n    version=\"0.5\",\n    author=\"Jeff Meloy\",\n    author_email=\"jeffmeloy@gmail.com\",\n    description=\"A tool to generate structured datasets from Python source code\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/jeffmeloy/py2dataset\",\n    py_modules=[\"get_code_graph\", \"get_params\", \"get_python_datasets\", \"get_python_file_details\", \"py2dataset\", \"save_output\"],\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires=\">=3.8\",\n    install_requires=[\"matplotlib\", \"networkx\", \"ctransformers\", \"PyYaml\", \"GitPython\", ],\n    entry_points={\"console_scripts\": [\"py2dataset = py2dataset:main\"]},\n    packages=[\"py2dataset\"],\n    package_dir={\"py2dataset\": \".\\\\\"},\n)\n\n```",
        "code filename": "py2dataset/setup.py"
    }
]